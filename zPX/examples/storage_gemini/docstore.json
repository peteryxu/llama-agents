{"docstore/metadata": {"84aea1d7-6bd5-4064-b0cc-942124651f8b": {"doc_hash": "ac82423cb658e3c38a57c89fc1b4a122d88441b0525f5c41f76fdabc5256c75c"}, "df73c18a-5e55-4389-8ee2-092b394ce21c": {"doc_hash": "45b042def175def5b72d2a6e95634485be7d5688f175f11014f72aafd6ea3021"}, "b03751a5-6dfb-4237-abd6-4c667ff2e263": {"doc_hash": "968685b148747a78c16bd9d858c5870cbf2b6eb017e184e532d83904f73d6c15"}, "9b1889e5-5909-4777-9934-4aa5ec509f20": {"doc_hash": "0bf105fab8627391b66d163b021d4beaeeabf2bb441f63041a915a0d7469f9e9"}, "fdcb044a-a0d6-44f9-ba93-3977f323d9ab": {"doc_hash": "92edcd80c5f4f28a746ad6c90c8252c2ea2d45cabb5754119e9b4eeee7d5c017"}, "9bee60bc-35e5-4c90-b5a8-57d9d4aca5f3": {"doc_hash": "c8fef5fcb2038e77f5014639aefbd69823e361dd937add8304d76a67f0e0393a"}, "4a05cb45-6040-4edb-9318-4603662c1c60": {"doc_hash": "26f92279b17fc2183e2f58d0946e2c2c1f4396ec2ae30ceb76deb6d3ad356d76"}, "d833b0a9-8ae5-4878-b40b-a4e8b0c0b453": {"doc_hash": "e56d82a10bf602415f2f1bbfc4bdeb1398dadeb5680d177f9b931aa40262ce46"}, "e90003a3-12ee-4960-a6ba-7ac28d985723": {"doc_hash": "bc6726c0167da97bf0838c33c69ed15e7c3876abb7b0720cd2defebd8145b848"}, "d4c1b94e-ab41-49f3-98b3-e897a2fe5e7e": {"doc_hash": "d993801f853d715ce0f0ddf7ad86cb99912e5b4dfaf943eb7c19c0fc997fe0be"}, "39cf8f8f-9a62-464d-ab5a-30cb5e2b1278": {"doc_hash": "15053d50a3573af2a15dfb805e3f158762930a3cc94e319c2a26a195fc8c3328"}, "62c30508-e83c-4515-8b72-318a4dd46462": {"doc_hash": "aeabf1ea826b21eb5132479e73acedf585c85ea10eba1b84fc0f1c706ff03076"}, "c7afd9e9-bac8-4364-a1ac-d41ea937d1ef": {"doc_hash": "350be2cb45001a13ae3097410f6f36b8b2d6b810960a532266c552208e14ac54"}, "8f733488-a603-4b16-b723-79ad0fc46e31": {"doc_hash": "3b46abfd460b9e7e7db67f929be0bb96d71cde6516fc632e065b884f6222f80e"}, "45bcd4fe-053a-47c1-9f0d-2941cc1ed1ee": {"doc_hash": "fb1658c5f133a8d8080db574b7d601b471e2bfa53405f0f3cbff9d7d9fe8294c"}, "352cad43-0b83-4ed8-9282-ebbc11bbc4c7": {"doc_hash": "86bf9d8710ad9002b285fccd37f03fa4b7d4b9c527f0503b439b16c24627827b"}, "4823fafb-2ba4-46fe-b9e7-2a15e2c105b0": {"doc_hash": "399866b510de64a6558edc888c359496a5fc6f97285ef352c880060c21088998"}, "e9a9fce0-6155-400f-bd73-56e598877979": {"doc_hash": "ea80e5e2c95975dad63fcd6f53693ecfffb01ebbf5aad7233edf7f43142d0522"}, "3f0f4da4-2713-4e37-87ed-e33e0e4833e1": {"doc_hash": "b8b3aa4098cd46785c349bb3602c00f41c29379c631853de692b40ffc8cd65e9"}, "106c69f6-ae52-4334-8ab9-25ad00a16dc2": {"doc_hash": "522ce5882d3316b8abefa9a7d099592a58af1a9e69f5a1648b7be5970f14ce6e"}, "9f2545d5-3381-4c78-a106-98dd040c1486": {"doc_hash": "422bc0fe20043538a140b23785472b09c864802008544b9b45c1406ee03a7f0d"}, "b327cfdb-6b4a-49e0-a2ae-4ffd81223440": {"doc_hash": "f06cfb5869aed727b850ca24bef6dc66157a7cf446d1c5e1df71ac193439e6d8"}, "3dd12f4e-ece9-4854-b69b-466626102bc8": {"doc_hash": "718bc33943b4903102786c19b522d8004aa8d2c4833358641fd72900e677cf51"}, "62099bd2-c144-412d-8323-bb904f56599a": {"doc_hash": "1c52f09bbf1560a1679489b1b2ca86c9b233fa2af720d381939d0171f549802c"}, "a4081485-ced5-4b70-b7e7-0675100ed292": {"doc_hash": "30e8dc76fc56bfa8d3dddfbb0869103f544a0a56555f3862392a987fb2306c5c"}, "6125c96c-8808-408a-a67f-9a7752c3e1ef": {"doc_hash": "3bbda711a33c8e9531419dd909d3b7ec9a6587702c302f2709d88f8c1b664608"}, "d9f478bb-b5b1-4e8b-96f4-d20d35c0b236": {"doc_hash": "76037a8ac33593d1909ed183e43ab5650418691e51fdda44360e866919e4680a"}, "dba508a2-037d-4361-928c-bfdc52236b9a": {"doc_hash": "5209d9f1dd37d1c33e279732d3d76818523841d558d3ed1d2d6a4292af93294b"}, "09eff6ca-526b-42bc-b459-45c4319a25b2": {"doc_hash": "2c4e1f62090beb9648a1658ef39d9369a10c72526339c006a8a379421ba28612"}, "7fb10a90-3e1b-4fea-89d0-aabccd5cf061": {"doc_hash": "67f6bcac7cbb51d28410fc3f441df375be07727ab1139952065baa8c2b91f26e"}, "e4d7113b-8774-4cc5-990c-5116c969a337": {"doc_hash": "636e9fe6ccc78fad3630ed710d324b96efdd1988b709140e9ba7efc036907f57"}, "f4de7cc9-8e1a-4220-8e5b-73cdcb7907d0": {"doc_hash": "a969b2707eef5cbd941c2100865ed9b35f7e5211d7075cd15d5156c65bfb71cb"}, "c9c6e28a-b9b7-48fd-aac9-65d762f9582b": {"doc_hash": "3b9814bab3002ce80a9eef0a58eb5e6c498b3728df3acddb13b701848b4533d9"}, "be00defc-486e-4a7e-b939-ba0a7260ae40": {"doc_hash": "eb2ead75e6c2d37a51452d4c6df7c27c2f78d918436d7cc86feae86b9f19b6f5"}, "e97a1479-4a28-4387-82dd-53cabf54a572": {"doc_hash": "f656667999d9eb957a0f30a614f8f5c32cec2c84165260e4803ab1f04776bad7"}, "76f88cd0-81cf-4733-83ce-06950a39afa6": {"doc_hash": "4e36917738a84252d472cea10fdb4e495554c2a5763009526416484da1ff96f4"}, "0a122a7a-8a06-43b7-b2e3-4144352a46cc": {"doc_hash": "03940469fb12bc2974176ddee0dc3096971da5c5043e7263f28e511d244eafdb"}, "0645aecb-2e3b-4259-bdea-3e86ee8a0598": {"doc_hash": "9bb1968623349700a41c2b0b08465c7c168afb8cb52ed870b67f302dce979467"}, "892148dc-bef2-46a1-8944-f6b9b2f15476": {"doc_hash": "31e050d589a8e4d49b8ce7d94f9f467503b971ddb1a0f80fe67b747ca31b8477"}, "e999d1de-7633-4302-8d3f-fa024ff5303a": {"doc_hash": "18a3073c8136eebb795e4fdb29c8963c2af467b036bd812e8f448e6496c66c52"}, "8c50530d-a192-4ca8-bfd5-39f90ec8290e": {"doc_hash": "c165f58807a9ee3afd5d494f87985c277adb3f6bdccbaeb5505dcb7cf589aaf3"}, "1584bd7d-6226-4860-b88c-fa92e7999503": {"doc_hash": "a87ace3545d14ecbef8fa453545ed34ca5d16c968c03eba9d1830740eed64135"}, "17b8b7e8-b4db-47c8-b382-a97f2da48307": {"doc_hash": "440b1734b8b4aa0d98c89d0a37f1d6cbdcd79d52cdd08054fe000e357dfb0f01"}, "2cbb2433-e70e-4084-a6ce-e6fdb2c9ca58": {"doc_hash": "24a827076d868e3bdc423a542fb642626b3dfe99ae18efe08c740d18d0724606"}, "56bc2258-3ef6-4e4b-b4fe-0f0b37dcfdae": {"doc_hash": "31dc7b40d2c7f227dc432315be0c5cc10576bb59bbe4a56a27159bc6a29478ef"}, "82eb7b56-7cd0-44bf-a3a7-e1a38f467db7": {"doc_hash": "7698c751918c9076a8ed039915ac07da8930bfbcc4f788395f46919c9c97b98e"}, "3544fa19-551a-41e6-9479-69e416d8ebc1": {"doc_hash": "b18374057d75f3df15ffed94bfa42b39ec300e4f334bbeba57aa42bacbd8645e"}, "05a59412-b1c2-4acf-9728-a356c2b971c5": {"doc_hash": "7c9a98934f5dc15500a5537da2a63cbf56554e6cd3f15f296b230e9af237222f"}, "06447da5-94c4-40c7-8584-d3e878cc616e": {"doc_hash": "ca9b966fce785743321b8b2f41fc980ad87a35efb2d648b33f7af202698ebc18"}, "e825ea7f-2b2b-48e2-8c69-1ff8ebd857fd": {"doc_hash": "1986d63ce0aaecef7bef2cd45157bf1d53c7444ef3fa0eba0d0f6a67b18ce966"}, "8965efd6-0ce7-41d7-ab1f-bcf99044a19d": {"doc_hash": "fd98967cdf7c07b48bbbe8e5415556c8749129dcbc363f79eaea9a4eb149787f"}, "93fd352c-f131-4c0a-a03d-aa226cfb9be2": {"doc_hash": "d654c24e3370d1dbfbb0d545c6dee3a1deb7d642a024c46dcfadd2133d616f60"}, "05546230-f685-4b61-8d05-2fe707fa11fc": {"doc_hash": "33d47287a04732a4f2f19f1e79f746848dd009e88113049ebd92ea8a4536b726"}, "d2c867d0-670f-44b0-b575-3f5775922c5f": {"doc_hash": "43d9414ca49e62d7d5df38e5126644ff350ae9ad7b910cf8378bf921a98bb4f6"}, "4c3ee72e-52e4-4a8f-9824-ff1de684dabb": {"doc_hash": "23c6d2884c941f74e9cc53ba78e00b556675085398695c5bd722357fdc1ce477"}, "cc45db44-ddb5-426c-ae93-f57c0d26dc33": {"doc_hash": "a51f37be50ba8bd5be3b417f761f892048766ba8e733ed5660be6314616815d5"}, "b76cdc16-3237-41ca-9c9f-5ab898e7a077": {"doc_hash": "cd394937aa3887a9ff114ccd27f050cad0f53be80b34cf8262a84537f75a0f59"}, "a4d13d7e-0fe6-4cc5-95ec-e8b2624c2795": {"doc_hash": "4e87582920390942e3cac431c4588c8af0ae6ba31ea590cbd61de72aa460ecff"}, "3e885305-29ee-4615-a697-a4ba2fcf901e": {"doc_hash": "513bd121bf0ab8c0dcb7af709a781f3fb9fd9037f5de71a64bcd6ca9d53eaf75"}, "ecc7b314-f186-44d3-bc98-04696c2287a8": {"doc_hash": "0cbd646fcb0112b104bd938b8d9f0f74f0e769d38ad4907cff2bc7f5fd62f798"}, "4526244d-5186-4ed2-8a95-23cd2b9ba597": {"doc_hash": "69d0f2692ccaa813d55f1cba165954115e0647992596e6827a5485ce8c7069aa"}, "f3510e33-2cc2-4a97-844a-bc18429c00ed": {"doc_hash": "7f87989d65d23da44ce0365e76e46f642540ea6db8c0522a2746a141fa7aa86a"}, "328d4855-ff4b-412d-8c7c-19dface1a83c": {"doc_hash": "e6e8132f3077d0b231a3140851a58e00aea9b6d1e49fb8cee922d5ae3f19c20d"}, "4a7b1260-6fda-4491-9b8f-f80504c9c6a5": {"doc_hash": "1da813e500d103a5ec552048d1d5e84cc516bb7c512e46219f96da3522dd3a62"}, "7d09033f-a40d-48c9-acf0-d2f1b1a2f0e7": {"doc_hash": "70aaeadce716da1822956fa9883b4b15f44a83f0aa56ce2c3d7ef94f5a03f216"}, "fe00072c-38e6-421b-b227-1e1e369df698": {"doc_hash": "dcf215fdad0277a46af5f8484c1837fd660e4bc991b4a52feed181aa661928f3"}, "a94626ed-131f-4915-a415-69a37e82d0bd": {"doc_hash": "0b10c51cf2e5a329c4936cf733587036f416739eeaf93bf10805ca46731517e6"}, "6246923a-2475-4964-aee1-f26ef4127963": {"doc_hash": "cecd6d5c5368e0fca0bb22d3185a5108bd1c16a7f34cae6bd4306e57204a9323"}, "904ce7da-b0b8-4868-94d1-fde93db79fa0": {"doc_hash": "342418a5b87284fefb0723cbf26243ff8f04455bcd17d41d16cb0644576f038f"}, "eb2353d6-6068-4020-a3b8-39bec5277a38": {"doc_hash": "ceff039c6cf053377b604c476400b1f79e9fc9a2a50c1514091667455a9b8f27"}, "91e47bab-e29c-442a-8ea2-6c8a04a5b3e0": {"doc_hash": "bc6b6a34b9fafa867796085261c56ca44924c4ede4883e2375301fd9646265b0"}, "6e61ea38-6e41-404d-a29b-e29f32c1a28c": {"doc_hash": "a328037f27fa35adbcd4090d2745bd8c973ad73ca2c8b941d576bf933c1b6668"}, "0445e872-d5a5-44f7-adb6-a9b485adb1c5": {"doc_hash": "29adf77b06e750ca74922910de566c073390863908cc9800ff75e67456ccb6c3"}, "32990677-c8e5-47f3-8b69-e2c1e5c81d12": {"doc_hash": "45e6b5778165a0037ea3bc1bec456ef2ceeda3f3082f0a4df48ca889b750dee2"}, "db062d8f-37ef-475c-81b7-c2b03c8bfbd9": {"doc_hash": "269e97a3faac60dcfde1406bdb6a60bc17345afcc37731fd31905956b243df97"}, "400d7fed-e6c7-483b-81fa-02aca53ef457": {"doc_hash": "3a0c13d9480e3fcaac525bb2ed935765f38a28ae01fdc603f14d32672f299658"}, "c3840f64-daa1-4d91-a3d4-2ed3598cd8e1": {"doc_hash": "51ab061f94e21a5bdbf3118d38e11763108efe64dafa54b96abbe769aa22b6c2"}, "b3921a53-3544-4f8a-8879-850762fee430": {"doc_hash": "bd659761c1b71945022f4a3688cf8893f3847d1f2e561469abe4a0a269b561c5"}, "225c8537-a293-405a-92d6-25cd615e1d8c": {"doc_hash": "1e18830da576fe2d9f3b48f8f9e384897749fc246f1d67c91881708f01716176"}, "35eba794-6046-4733-bc62-80ee184ab6ad": {"doc_hash": "5c74a704b905da084f0257433873a491e35276499047ce6e6100121225663511"}, "e85ccd83-517a-49ea-825c-40e0e03a4467": {"doc_hash": "22f03324bd957c2a4f8be5b3885bc2483df7b62369b4091362e82e088bce80ca"}, "3c006079-78be-4b33-82b2-54d8840a5b0d": {"doc_hash": "86cca4ff9d01bc0eba9ffadec1bf5d0a3618a1f890c763f89770e386c735373f"}, "9f02c365-50fd-4fd2-9802-a240d1dac0e3": {"doc_hash": "397f201d0d46228370d14b70626e9798ae5c8ad1640e351bde41706f84b2f5ec"}, "03a55c9d-430e-4abd-9a35-9f83424d75a6": {"doc_hash": "42ca1ddad9c96adae9a82e11e2bcd54d09ba27b9de7f1ae39dc3827395b2a3c9"}, "38c287fa-aea2-4fbd-80de-817f1780f432": {"doc_hash": "53f0534e40954c6e1a3353dd60b135273e07f553aebbd44bf3aa464e825a8215"}, "de801551-848a-415a-9a25-0167cfae22ba": {"doc_hash": "9cc9ff137d827e52f316acdcceb44efe2bf45d943548849329a5aa41822ebfb5"}, "caee1745-65b3-4cd4-b147-2b5ac82055f3": {"doc_hash": "f0cd8f4377a95438f27398a3e90147b2af81de622f86a7d5d99777ac5694b0ec"}, "de10392b-704d-450a-b743-be592a37eba1": {"doc_hash": "fb6986e62bec8d96a324bf8a4526eed3f7bc5a908d765e0d32b640a2bee1f029"}, "00bf5ee0-e44c-41bd-8caa-b3c931a6110f": {"doc_hash": "0d1fdf6bde3c26a35e9f6c628f5510afc8943725e168dbc7a51efeace4f9555f"}, "70d37a69-9e43-44d0-bc0c-b65aa55d1597": {"doc_hash": "3a9e4f170d3c2c8b4d4f7240f773b55f04db63c972f60acf54717f82e8a6f53c"}, "570acf52-f613-43ff-9c87-4995e44f1080": {"doc_hash": "ac82423cb658e3c38a57c89fc1b4a122d88441b0525f5c41f76fdabc5256c75c", "ref_doc_id": "84aea1d7-6bd5-4064-b0cc-942124651f8b"}, "309dbfa8-2e7e-4def-ad8d-068a9addfe8e": {"doc_hash": "2d87b562fdae599ecc13524854083df3ab354a819769c15b56509fe1200eb9eb", "ref_doc_id": "df73c18a-5e55-4389-8ee2-092b394ce21c"}, "4fe68737-08af-473c-a056-570e8f5ab130": {"doc_hash": "685f0207500cd16b5f9537a327f5fc3d9f211a8e65638ed94f49560da3ab5229", "ref_doc_id": "b03751a5-6dfb-4237-abd6-4c667ff2e263"}, "e730ac37-60ab-4fcb-969a-88fdea1ecc26": {"doc_hash": "331fd0e2717824ec848c77573ae6507d5924213358b3033161707694e8dc86e2", "ref_doc_id": "9b1889e5-5909-4777-9934-4aa5ec509f20"}, "6db95581-861d-4056-b4f7-c8f49b69d52d": {"doc_hash": "a449890db13e58daacc65bf4f820618cd99897baf6a14aaff12f1bc76b3d9cd1", "ref_doc_id": "fdcb044a-a0d6-44f9-ba93-3977f323d9ab"}, "2a803ccc-4401-4854-9e90-e1a119a6eb12": {"doc_hash": "1a4f03ed93319414feb624e5273c8d692280d9697f26b4a18fe95a178d413634", "ref_doc_id": "9bee60bc-35e5-4c90-b5a8-57d9d4aca5f3"}, "bac5362d-6307-41c5-a3b9-decd1c5ee303": {"doc_hash": "f5e1fab44fd29e6e7f2a484ad8ef93348a938170557391893d861ddb5156afa7", "ref_doc_id": "4a05cb45-6040-4edb-9318-4603662c1c60"}, "63aee76d-6e26-4d2a-8da2-9760ea1dcdd2": {"doc_hash": "9700270b89dde24d54a2898bc5f42e1eec0274844997d3d612441f4877eedb96", "ref_doc_id": "d833b0a9-8ae5-4878-b40b-a4e8b0c0b453"}, "d1db085d-77d7-4f83-b8ee-49e222bce7ab": {"doc_hash": "e0e13ea278d1eec62b29ff451279fe47e50b37f85e75e068b1199524e31ebd1e", "ref_doc_id": "d833b0a9-8ae5-4878-b40b-a4e8b0c0b453"}, "2c84f064-6027-4e6e-9881-0785be5a9a46": {"doc_hash": "9e2bc17d069dec7aec4c726ff962fe8e2bfff488e8cbfa072a0ec6cff28d037e", "ref_doc_id": "e90003a3-12ee-4960-a6ba-7ac28d985723"}, "a1f940f1-eb60-4f06-843b-54c7d83e74e0": {"doc_hash": "e99ad34126be8d8d815e6a32db4bcb266acc1f2bb9d4a846d6170fdd6e631e7c", "ref_doc_id": "d4c1b94e-ab41-49f3-98b3-e897a2fe5e7e"}, "03aaedea-0a1f-4f4e-8172-1d8dc5a02ccd": {"doc_hash": "ed433b0ecca804f1d0445c202b972a17030933801979ff1ac850cfb4a4cfdc8e", "ref_doc_id": "39cf8f8f-9a62-464d-ab5a-30cb5e2b1278"}, "f873c89c-4916-4dd1-8ba9-3654590879f1": {"doc_hash": "8da0e751df40f32d293f2f22057b394bfb2c955ceb6dc323bfbc9274671f38fd", "ref_doc_id": "62c30508-e83c-4515-8b72-318a4dd46462"}, "5cf9fa4c-7138-45a6-b7e7-84e1ffb51b81": {"doc_hash": "8ad5c1f4786f0c6250daa0b066ef57e662ffd884b20fe1ccbd1656c4b29731a3", "ref_doc_id": "c7afd9e9-bac8-4364-a1ac-d41ea937d1ef"}, "51f23561-6cbf-4136-a5ab-ebb6def37f12": {"doc_hash": "952a6670d5c1c2a66169fc7f424a36418d31576b2ea9fbc6c5ac0ccd308fde8f", "ref_doc_id": "c7afd9e9-bac8-4364-a1ac-d41ea937d1ef"}, "4d3aafd5-7da3-4f8e-8cf7-3f2e285df1c0": {"doc_hash": "d9b83167660f7746c8778eb0e770c4ad836833000548bf4a63c4c01f6a76c5f3", "ref_doc_id": "8f733488-a603-4b16-b723-79ad0fc46e31"}, "411a7d35-f8b9-4583-bda4-44b8ec65d919": {"doc_hash": "348a48179e38ff0c003ac72fbb60dfd89177340f13963639b8f6283572467ec6", "ref_doc_id": "45bcd4fe-053a-47c1-9f0d-2941cc1ed1ee"}, "1ad9d390-c05d-4e89-a0ac-bb23edf436e8": {"doc_hash": "722639caa8968a884d95d5b5873d7decd8ae2202cd460bfb3a2605122d8a574f", "ref_doc_id": "352cad43-0b83-4ed8-9282-ebbc11bbc4c7"}, "de2b091a-dfbf-40e1-98e6-6ef9d19c0347": {"doc_hash": "67bb34c91de8fe1d442d998f73dc018bf2c4d190e4611a1cff7e112640e54492", "ref_doc_id": "4823fafb-2ba4-46fe-b9e7-2a15e2c105b0"}, "7f796ab5-2ae2-403c-bfdc-902eea3d8a2e": {"doc_hash": "687ff8383fee8b6ff2602fe27690fe4f55d30fe05977457e1ea96ab8b089450e", "ref_doc_id": "e9a9fce0-6155-400f-bd73-56e598877979"}, "921e8c01-57ef-4177-b5c1-2bd74cfec5f7": {"doc_hash": "6e1d29d0746d43b24a60d6074cf538b57376b52fc8d5499ea6d387b0f30cdea7", "ref_doc_id": "e9a9fce0-6155-400f-bd73-56e598877979"}, "098960e3-7be1-49b3-bbdc-d35fc235dab6": {"doc_hash": "d4473e496dd036396ea88f1d3b3f6b336e7e2d9e1bb3b653321a1f3d9f0b82ef", "ref_doc_id": "3f0f4da4-2713-4e37-87ed-e33e0e4833e1"}, "a138febe-7833-4497-ac14-8f4c57ddcb39": {"doc_hash": "ce47c67720106eb661f3c986a2a62d8758214e261fcefa4199f6c9762543c8a3", "ref_doc_id": "106c69f6-ae52-4334-8ab9-25ad00a16dc2"}, "b1aae730-5f59-44fe-a6c9-f82352815b86": {"doc_hash": "697a6ef65a70e305f8c990ed06f2be9a6ba92a17bf7966c549ffdb420f1af3c6", "ref_doc_id": "9f2545d5-3381-4c78-a106-98dd040c1486"}, "25ccd597-5eda-4b02-924e-64849c904608": {"doc_hash": "6a6668da93a9addde78db9c565a518b31f2e78f45f905756fed12f1b38cbb6be", "ref_doc_id": "b327cfdb-6b4a-49e0-a2ae-4ffd81223440"}, "c68c958f-98ad-4966-ab8c-93254ade2348": {"doc_hash": "866f830c3c081a47fabc174bc3630507e15875ab689a7041c1059f2ac290ad67", "ref_doc_id": "3dd12f4e-ece9-4854-b69b-466626102bc8"}, "29184fc1-6eaf-4c2f-90ba-9e367d9ad59c": {"doc_hash": "effe3d40711fce4108f2b382d18c8f7af22ca06b0502fc5e150e9eb511fcbab5", "ref_doc_id": "62099bd2-c144-412d-8323-bb904f56599a"}, "03a2f46a-2e46-457b-9457-a3b295989524": {"doc_hash": "aa3887cd22b81404f38778a367f331434ec826104d91f20c6cd09e968a0517f2", "ref_doc_id": "a4081485-ced5-4b70-b7e7-0675100ed292"}, "04c88911-8078-4b90-94bd-bf35ae642d1a": {"doc_hash": "f2cddeef3705d1ef9212a8d1b1d4378dfe4dc1efa4c5a6a761d72435545008f0", "ref_doc_id": "6125c96c-8808-408a-a67f-9a7752c3e1ef"}, "54432497-33dd-4873-a315-75aed0c97d13": {"doc_hash": "8f5585427b7919ea07662e77089ec5333cf18fe98f7a97dc84953ecbc2f3973c", "ref_doc_id": "d9f478bb-b5b1-4e8b-96f4-d20d35c0b236"}, "f7b54486-7719-430d-b4c9-e24e77962385": {"doc_hash": "c1b56133baf43e17e9990d00bc2f2a5ebd9e4e3b31b7e041ea048e6076c3ea56", "ref_doc_id": "dba508a2-037d-4361-928c-bfdc52236b9a"}, "84981b46-73d9-4513-8fa2-64bce783822b": {"doc_hash": "32bc79d9d370f921b6a0a67abfe94b8a93da86c89fa5f8fc1275466b281d8f0a", "ref_doc_id": "09eff6ca-526b-42bc-b459-45c4319a25b2"}, "366c86ed-ef48-485a-b948-b4b206012c57": {"doc_hash": "01fbcc5d14b04b3eef82e7c56e4d85be2b3d4b03e0addf4c105655773233d940", "ref_doc_id": "7fb10a90-3e1b-4fea-89d0-aabccd5cf061"}, "550c1249-4c70-4830-ba9a-2f1da3737e84": {"doc_hash": "880f76179d10289dcd915d62305fb734c8add455230f1ba7a8f96dcce9d146b3", "ref_doc_id": "e4d7113b-8774-4cc5-990c-5116c969a337"}, "d0ada9ef-b6d2-45f9-90ae-76e948c3fef7": {"doc_hash": "7571846a2ee44ae09d8b8ca41acfaf77fb83af31527cf455a8f8c6ca3e651499", "ref_doc_id": "f4de7cc9-8e1a-4220-8e5b-73cdcb7907d0"}, "51ddd985-26a3-4ff8-b312-853de7442a3f": {"doc_hash": "60294ec3a25fe6857ffa108ac75d530929ec4de4ed6442335ce855192882efdc", "ref_doc_id": "c9c6e28a-b9b7-48fd-aac9-65d762f9582b"}, "93d1d435-3423-400f-b6ca-fb7324e86865": {"doc_hash": "5857d66f4b9d07e507615326dd483cf617a4476cc3542115d05775920d88d17f", "ref_doc_id": "be00defc-486e-4a7e-b939-ba0a7260ae40"}, "0d13ecd4-ca73-4696-8b68-cd3e6d3fb628": {"doc_hash": "216fc492252667bb66b519a7671a56f930464a56eded3a3a2a10f3f3ac69a8ba", "ref_doc_id": "e97a1479-4a28-4387-82dd-53cabf54a572"}, "5a65f5e3-f861-4a7a-a545-9d739b702854": {"doc_hash": "2f5e36e71b3d18573bb68597842da7a98889bd76ea3a2f708be09f60ba77b547", "ref_doc_id": "76f88cd0-81cf-4733-83ce-06950a39afa6"}, "087d44da-2279-4973-912a-2b08072390b5": {"doc_hash": "df2c958ddb46b14015cee5c8323f8e50005f8b2e0484ca45573e913587195fd8", "ref_doc_id": "0a122a7a-8a06-43b7-b2e3-4144352a46cc"}, "b98c4297-9a99-454b-8d4c-641d269e74ad": {"doc_hash": "8d0979a30fe0d03848707eb8eb5be97f8f6ba068b96e6b4a81ee6114e062a95f", "ref_doc_id": "0645aecb-2e3b-4259-bdea-3e86ee8a0598"}, "89b26998-bc35-4b10-ae15-96bfac9cc127": {"doc_hash": "67e519b50a47178f56559fc7a63445d4cc3730f1cf5daba910d7d1e028125343", "ref_doc_id": "892148dc-bef2-46a1-8944-f6b9b2f15476"}, "a73e37a0-e3cf-44bc-afe8-ae9201e4b1a1": {"doc_hash": "86e89c727a6df5730b98277614911210059c86982ca3961539990220ad0eded9", "ref_doc_id": "e999d1de-7633-4302-8d3f-fa024ff5303a"}, "05524d37-4964-4874-963e-4088b7947cac": {"doc_hash": "65a329d5e5058005786788e2fb29a083461d27ab51c5b979fb7cb9055b4d91f0", "ref_doc_id": "8c50530d-a192-4ca8-bfd5-39f90ec8290e"}, "fcbacea3-ea3d-43bd-a1c5-d1d4bc025833": {"doc_hash": "ac1aa581536df0a0962c0cdb766ba0aae17a2495824b339069c847b30081b914", "ref_doc_id": "8c50530d-a192-4ca8-bfd5-39f90ec8290e"}, "67cdf4d5-5515-4af0-8a77-589a7d96d136": {"doc_hash": "44e0c28a20b9372e51c7cc36e24f465363198c4beb57969653b22e29a1619c1c", "ref_doc_id": "1584bd7d-6226-4860-b88c-fa92e7999503"}, "9790adf1-a6a9-43c6-b029-3db0345f9fd5": {"doc_hash": "295b96151f7101291d49d80ba23b1d8234436e13a359766af64cb3a4e1d8d128", "ref_doc_id": "1584bd7d-6226-4860-b88c-fa92e7999503"}, "359de1ba-4855-436f-878a-df62338cfef9": {"doc_hash": "89c3a6b02f3877b18184b0a985e0264cf91776d5f851c01a7a9a78400107f0ee", "ref_doc_id": "17b8b7e8-b4db-47c8-b382-a97f2da48307"}, "36e3f164-2b89-49f7-9a43-389c22785cb9": {"doc_hash": "a5a54224a66109e4f5ff9cdc075b9bfcac0df27b559b052c1d1d9c2659c9ddaf", "ref_doc_id": "17b8b7e8-b4db-47c8-b382-a97f2da48307"}, "c9752251-f8ce-4cc7-84db-ae993320a81c": {"doc_hash": "11d933859bdf08472cf376d7f076aeac993a391e195b6bdda5b61f28583685c6", "ref_doc_id": "2cbb2433-e70e-4084-a6ce-e6fdb2c9ca58"}, "4317f965-4425-4e5f-92b2-cc55efc937aa": {"doc_hash": "e198a367f837a9c4dfdea0cd0155423d13764c878d67ac49e8f6eec1b3d329ac", "ref_doc_id": "2cbb2433-e70e-4084-a6ce-e6fdb2c9ca58"}, "7dd1f7f4-139d-421d-9edd-de3524bb9b9d": {"doc_hash": "04445eec0e6d0fd80ad2d7ae8846f1a1de3f704a39c107af9b6ef94cd718a85d", "ref_doc_id": "56bc2258-3ef6-4e4b-b4fe-0f0b37dcfdae"}, "6c1d30ac-30c0-4dea-977f-8d5f5051de8d": {"doc_hash": "5af6d5aec6d3e4c2b8759b3ca79eafe3a36870eb5bc3ff80d1de8ff8a6dca78a", "ref_doc_id": "56bc2258-3ef6-4e4b-b4fe-0f0b37dcfdae"}, "f506544e-70c1-46c3-9a82-acdcaa5da3ce": {"doc_hash": "7c7a15ea3d685021001182198bcef2bde7908d7e74406e6b5377cea98f1485db", "ref_doc_id": "82eb7b56-7cd0-44bf-a3a7-e1a38f467db7"}, "ecf07567-1e4b-40f1-9b8e-66100226f134": {"doc_hash": "0b8b9021346736785070bc3c9811fccbbdc622607532307224f720c762a6c820", "ref_doc_id": "82eb7b56-7cd0-44bf-a3a7-e1a38f467db7"}, "aadfece9-7f39-4def-9f5e-51b6e7ad1be0": {"doc_hash": "d2ade6ae053dffa3fb298bb175008d968813cb04b5d5a6cda42ff1e0e0f791aa", "ref_doc_id": "3544fa19-551a-41e6-9479-69e416d8ebc1"}, "4a7b9174-14c6-490a-9d95-54e06cfba31b": {"doc_hash": "c1cc0968ccdcaff7028f1688938b95536e2e0e8a45c0a3f2672d839da4dfd143", "ref_doc_id": "05a59412-b1c2-4acf-9728-a356c2b971c5"}, "6ca7408f-c9e8-4919-b063-0e0b195af7bc": {"doc_hash": "841089d88fa8141f31a9f94a9dd5280f0e7d9724eaf741aa9c724a8201de8983", "ref_doc_id": "05a59412-b1c2-4acf-9728-a356c2b971c5"}, "6a4d6fe3-c297-4919-a594-dfd1f91b3ada": {"doc_hash": "3d7a16cdaa636c8263fa818e038f424bb2a1c3aa743371c71bc9ea28143e0e4b", "ref_doc_id": "06447da5-94c4-40c7-8584-d3e878cc616e"}, "c435317e-db5d-45ae-a2bd-f48754c18ab8": {"doc_hash": "8eab11d17cd4b6cfec71a25264f6b93c15f69aa512d558f2165c2a48691e60db", "ref_doc_id": "e825ea7f-2b2b-48e2-8c69-1ff8ebd857fd"}, "f78c2f86-2359-493e-b79c-fda67608761a": {"doc_hash": "fc9b0865d6f53b3799e15e6f78a50e67d14cff9e8c632701dc7bfc29de23c6f1", "ref_doc_id": "8965efd6-0ce7-41d7-ab1f-bcf99044a19d"}, "d7047934-d8ef-4098-9f77-2e7bee215506": {"doc_hash": "4ddebbd28118290d593ead07b9e7e95e33c5ef3be2f2c5bc74aa389bfb6af9da", "ref_doc_id": "8965efd6-0ce7-41d7-ab1f-bcf99044a19d"}, "736cf978-aed2-44d2-a152-de25128edb81": {"doc_hash": "d7747b683b072660467b0e6f51bc8faa20ae6983b6b418e73f8e5908ff985c5e", "ref_doc_id": "93fd352c-f131-4c0a-a03d-aa226cfb9be2"}, "9c14eb31-cbcb-48be-81ff-8d589439fcfc": {"doc_hash": "98329bb1bc1f5989ba6d15476e78c78e89c484ebcc0339f23f7d57ced27946f1", "ref_doc_id": "93fd352c-f131-4c0a-a03d-aa226cfb9be2"}, "3c386aef-17f8-44db-a91a-6742888c15cc": {"doc_hash": "321024bcfbee7e1e25020d0052dceb335bff7a35c8709ea96fd5c373a67c0d39", "ref_doc_id": "05546230-f685-4b61-8d05-2fe707fa11fc"}, "084fe3f6-e43a-4430-ab90-8857329fb5d3": {"doc_hash": "4375a38862103e825861ca6ae0cb2db2d25474c6a06bd87dafb2a958f9147c21", "ref_doc_id": "05546230-f685-4b61-8d05-2fe707fa11fc"}, "ec440fe4-b9d1-4de4-a388-6e869a16c3ca": {"doc_hash": "43d9414ca49e62d7d5df38e5126644ff350ae9ad7b910cf8378bf921a98bb4f6", "ref_doc_id": "d2c867d0-670f-44b0-b575-3f5775922c5f"}, "cf122f49-1b3a-4676-a5d7-9fcd21249c3f": {"doc_hash": "51831727e3f02d27e25c4df1f7385aed461f8d200e1dd0cb3d6faf966fa9ee32", "ref_doc_id": "4c3ee72e-52e4-4a8f-9824-ff1de684dabb"}, "0f43b777-9c6e-4418-b653-0d99f7370d00": {"doc_hash": "43ab2cdf6c7c292208af5811da89b58ada50e2ad3c4281cb24f4bc6e6380803a", "ref_doc_id": "4c3ee72e-52e4-4a8f-9824-ff1de684dabb"}, "668216f9-d2da-4085-bdfb-08e22144d2cb": {"doc_hash": "764eb6542da2cf1cbe8aa0af24056d544fa81ad4f321f1cc4e6b1206e8b47b50", "ref_doc_id": "cc45db44-ddb5-426c-ae93-f57c0d26dc33"}, "aa160a77-35c4-4b13-a3fe-81f1ef180d31": {"doc_hash": "0cd03caf3448ff96ed8deeffbe75227d213831a97001ad8d98458dc55268cf5b", "ref_doc_id": "b76cdc16-3237-41ca-9c9f-5ab898e7a077"}, "4ba1b6ed-197c-4db9-9a03-01a50752e5fd": {"doc_hash": "f8a4b0cd8749bb7b3873f5c5e2921f7dd74cc4b4de2f7f36409475b0dbbddadb", "ref_doc_id": "a4d13d7e-0fe6-4cc5-95ec-e8b2624c2795"}, "48975aa1-0e2f-4caf-ab25-66db3af56e57": {"doc_hash": "12169f33c429be6adf9a13e33a5a71ac646e49f289f48e19b50fbf38bb36833a", "ref_doc_id": "3e885305-29ee-4615-a697-a4ba2fcf901e"}, "1ca4e726-6ff9-46af-90cf-67a02b950318": {"doc_hash": "72f01bbcf68058f387e47609afe34cac9682ae6d30c2909042203c3051854b83", "ref_doc_id": "ecc7b314-f186-44d3-bc98-04696c2287a8"}, "8721c083-5851-4b4f-8d8b-a67babef97fe": {"doc_hash": "0146307599dc7c5dbe21c185f78b2cecd1e2d187e05def5c9849b05816eb108c", "ref_doc_id": "4526244d-5186-4ed2-8a95-23cd2b9ba597"}, "1f48f8cf-c1c9-4894-a4f2-9d08c2d7fb28": {"doc_hash": "daa9971530df963a675f9a19c3932aad32c734fd6912fc1cce34ddc27abad81a", "ref_doc_id": "f3510e33-2cc2-4a97-844a-bc18429c00ed"}, "b3a75b14-d987-451a-a83c-94de4160180b": {"doc_hash": "6263e49dd96e184c4aa864fd5ac40066a186966ad5dd3641a1dc935eb52dc400", "ref_doc_id": "328d4855-ff4b-412d-8c7c-19dface1a83c"}, "63ade1e8-a44f-4a97-9261-020a5452163f": {"doc_hash": "1f15ba0c479580095ee9e35fddae0c3c89ded42e7cef91433331753a9d90821a", "ref_doc_id": "4a7b1260-6fda-4491-9b8f-f80504c9c6a5"}, "0f76c549-8d9e-4bdd-8582-ebbcc6c5e081": {"doc_hash": "e93f74d37b3ffa494a2fa84727ee8c89af554a24b3c2848cee456fd5df341524", "ref_doc_id": "7d09033f-a40d-48c9-acf0-d2f1b1a2f0e7"}, "7e2f039d-a900-4ff6-af89-118a66c56df8": {"doc_hash": "a9411b10eb42a4c1166a175bb267bf109e1ce48c2deb886deb294c98a7f60ecd", "ref_doc_id": "fe00072c-38e6-421b-b227-1e1e369df698"}, "99186704-d59b-46e2-969a-04f73bd20ccf": {"doc_hash": "972b05a64e64282a759a3a455f4dd0bfdac6da1640558c351af7ba6ef7aa5de3", "ref_doc_id": "a94626ed-131f-4915-a415-69a37e82d0bd"}, "508d15f5-f4b6-4ab1-be16-fd2458bc0ad0": {"doc_hash": "2653511b73c8be0415e6737151cd4c52af8f96c9d184cb77269bd3b65d4ad681", "ref_doc_id": "6246923a-2475-4964-aee1-f26ef4127963"}, "b32fba8b-4628-455b-a018-5970a5de131e": {"doc_hash": "43890be56cabc112d837b11124f454ad170aa79f8086734ba334232c71624180", "ref_doc_id": "904ce7da-b0b8-4868-94d1-fde93db79fa0"}, "3fbfed0c-2872-4652-8e15-8aa2022ebe8b": {"doc_hash": "fb4e89ac8ec946dd401d157e8e08625d0cad1cad8b5c2494da0d87b1281776b3", "ref_doc_id": "eb2353d6-6068-4020-a3b8-39bec5277a38"}, "b60a3a1a-444c-469f-8275-d9a83bc291ec": {"doc_hash": "6e3b1fba5add8164ffac5392ec135638a2d386cf225692b8c3fb67cd878763d2", "ref_doc_id": "eb2353d6-6068-4020-a3b8-39bec5277a38"}, "450ee741-1551-4144-916a-6dc3336cd4f0": {"doc_hash": "82f07092403ceb1bdc2ed0cd4843ce675ca11275745cc166b1cc3cfa5034ab1a", "ref_doc_id": "eb2353d6-6068-4020-a3b8-39bec5277a38"}, "19eaad7e-4adb-4906-aa3d-ca9e06e0624a": {"doc_hash": "ce35cdc316de99adbc4f256463c87300f82b676954cd49fdf3741bb0029e7017", "ref_doc_id": "91e47bab-e29c-442a-8ea2-6c8a04a5b3e0"}, "1def6a21-fb81-4d1b-be77-52031860a154": {"doc_hash": "14aa18f15694aa3f126d96b7efcab9587e13b749d543c9b3ffa9464bd6956cc0", "ref_doc_id": "6e61ea38-6e41-404d-a29b-e29f32c1a28c"}, "5e6d7c60-4ae6-47f8-9d8e-6851ae109a65": {"doc_hash": "742c299e8ea559c9b86bc45d079115be29aa5360e3320c125be183104ab05fe6", "ref_doc_id": "0445e872-d5a5-44f7-adb6-a9b485adb1c5"}, "1bea6dea-7672-4b13-9450-4f0307132515": {"doc_hash": "0788fbfe72959a074ad1036ba0d246b3db3fef63ebb0b5cb0ee5d7ee6691fbbe", "ref_doc_id": "32990677-c8e5-47f3-8b69-e2c1e5c81d12"}, "d9bb9521-66e5-4319-a7ed-c9662e4ec9ff": {"doc_hash": "dbc5480a0b0bf9d0596c8f23833c0ea8814a47d63914a06cb599dd36fe4348ef", "ref_doc_id": "db062d8f-37ef-475c-81b7-c2b03c8bfbd9"}, "147576a4-20a0-4732-98d0-a3911b13934e": {"doc_hash": "5cc0da81f7e2008e92d36e71d70204f309cc3f08af32ecf5042f2c30bc942318", "ref_doc_id": "db062d8f-37ef-475c-81b7-c2b03c8bfbd9"}, "370dc4b6-b257-4c73-b0ea-31287d744a1a": {"doc_hash": "31f7d763df19d15b89ccb023698643faf5039ea5fc00bb0beef77c14dde030af", "ref_doc_id": "db062d8f-37ef-475c-81b7-c2b03c8bfbd9"}, "3af8814e-fd29-4947-ab07-75fb2a1c4361": {"doc_hash": "9ddd91026309cd419835c957f9e0f66bb6ce2d6990838810e64a6357f952f746", "ref_doc_id": "400d7fed-e6c7-483b-81fa-02aca53ef457"}, "e6700d6e-eee8-45d0-befb-da13d3e1b45e": {"doc_hash": "4d6222f3d0b20feccd530bfc0c8d35c691997b9a4dfaf5cc55c6a8b2cac87413", "ref_doc_id": "c3840f64-daa1-4d91-a3d4-2ed3598cd8e1"}, "ddd4d91a-8aac-4787-b444-299cb1ccd4c8": {"doc_hash": "0c3dc8a0dc6521d524aff915e2afcc91a46ebbb21a127d26542f101311d61817", "ref_doc_id": "b3921a53-3544-4f8a-8879-850762fee430"}, "ced501ab-d2ce-45ed-b5f0-509788456ed3": {"doc_hash": "f06a2fa2cede10fdffffa5155cba355b1706234df4b97137f5dfaee99c153cdb", "ref_doc_id": "225c8537-a293-405a-92d6-25cd615e1d8c"}, "44222823-0282-4fbb-8581-2520e51c5335": {"doc_hash": "a501d858cf20510a97641a15d52fd09e851abcaf7472adb2e66e5ec29050a2f3", "ref_doc_id": "35eba794-6046-4733-bc62-80ee184ab6ad"}, "d65f520a-0ffc-4d4d-947b-1a2e75776ca3": {"doc_hash": "12697df118c83a0499bb975557361acacbf9beee97b7664d833e55dda27ac308", "ref_doc_id": "e85ccd83-517a-49ea-825c-40e0e03a4467"}, "3bd27334-b347-4d3d-b02f-022af2fb316c": {"doc_hash": "a50878bdc84ae7ce3c6b70cc27d668fb2674384b4b0e2849fbe8a570723c5cde", "ref_doc_id": "3c006079-78be-4b33-82b2-54d8840a5b0d"}, "650e515e-ff00-4151-a792-44b392b7e3e6": {"doc_hash": "77d57d3c9882c7222ac91daa5e15c66a37e21240ec1b5fdb98b2afa7d95f2f6f", "ref_doc_id": "9f02c365-50fd-4fd2-9802-a240d1dac0e3"}, "a94ae2f3-4864-45d0-a773-f8329efca4da": {"doc_hash": "1a3614ad3ec469d5942d588af1a146cb3c85967c4a39701bb9950dc898198c5a", "ref_doc_id": "03a55c9d-430e-4abd-9a35-9f83424d75a6"}, "082c1e75-6bf8-45b1-8438-44fbc27e7ce1": {"doc_hash": "8256e2fd8426592146b0fa358073e2c5d5d216c15dfa31f021abddcd8868f13f", "ref_doc_id": "38c287fa-aea2-4fbd-80de-817f1780f432"}, "c31f4179-130c-4102-b5d8-a57cadd92bf2": {"doc_hash": "cab2b9c1324cb8024d0e752150841248c0a23320edfd712d35547e039935a2ba", "ref_doc_id": "de801551-848a-415a-9a25-0167cfae22ba"}, "6037b531-077e-4b19-9ce4-361e51481ab6": {"doc_hash": "a0c2aaf7153dd632c016d121a36852adc28b5308125f7072bbe13b1b06988bbc", "ref_doc_id": "caee1745-65b3-4cd4-b147-2b5ac82055f3"}, "981bcfd4-49ab-41d4-a6be-1faa18c30cb5": {"doc_hash": "a7b252a591e37fe4e2f02a0b735764da6942c7b14d59d00c9e199220f03b9f87", "ref_doc_id": "de10392b-704d-450a-b743-be592a37eba1"}, "f0eee39d-7b3a-4b3e-8534-0ee899ac7a9f": {"doc_hash": "1e41de3655def3757253538447a690fbfa1a7aaedde69282407eb3eb1b091f7a", "ref_doc_id": "00bf5ee0-e44c-41bd-8caa-b3c931a6110f"}, "e1c9298f-fa30-4886-aa1b-3f28380a3b40": {"doc_hash": "b8e0a48625485a548589edaa9266840b6972979266622cff2f59f0b4cba6e2a2", "ref_doc_id": "70d37a69-9e43-44d0-bc0c-b65aa55d1597"}}, "docstore/data": {"570acf52-f613-43ff-9c87-4995e44f1080": {"__data__": {"id_": "570acf52-f613-43ff-9c87-4995e44f1080", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "84aea1d7-6bd5-4064-b0cc-942124651f8b", "node_type": "4", "metadata": {}, "hash": "ac82423cb658e3c38a57c89fc1b4a122d88441b0525f5c41f76fdabc5256c75c", "class_name": "RelatedNodeInfo"}}, "text": "arXiv:2312.11805v4 [cs.CL] 17 Jun 2024Dai\n                Gemini: A Family of Highly Capable\n                Multimodal Models\n\n\n                Gemini Team, Google1\n\n\n                This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities\n                across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano\n                sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained\n                use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model\n                advances the state of the art in 30 of 32 of these benchmarks \u2014 notably being the first model to achieve\n                human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the\n                art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of\n                the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of\n                use cases. We discuss our approach toward post-training and deploying Gemini models responsibly to\n                users through services including Gemini, Gemini Advanced, Google AI Studio, and Cloud Vertex AI.\n\n\n                1. Introduction\n\n\n                We present Gemini, a family of highly capable multimodal models developed at Google. We trained\n                Gemini models jointly across image, audio, video, and text data for the purpose of building a model\n                with both strong generalist capabilities across modalities alongside cutting-edge understanding and\n                reasoning performance in each respective domain.\n                     Gemini 1.0, our first version, comes in three sizes: Ultra for highly-complex tasks, Pro for enhanced\n                performance and deployability at scale, and Nano for on-device applications. Each size is specifically\n                tailored to address different computational limitations and application requirements.\n                     After large-scale pre-training, we post-train our models to improve overall quality, enhance target\n                capabilities, and ensure alignment and safety criteria are met. Due to the varied requirements of\n                our downstream applications, we have produced two post-trained Gemini model family variants.\n                Chat-focused variants, referred to as Gemini Apps models, are optimized for Gemini and Gemini\n                Advanced, our conversational AI service formerly known as Bard. Developer-focused variants, referred\n                to as Gemini API models, are optimized for a range of products and are accessible through Google AI\n                Studio and Cloud Vertex AI.\n                     We evaluate the performance of pre- and post-trained Gemini models on a comprehensive suite\n                of internal and external benchmarks covering a wide range of language, coding, reasoning, and\n                multimodal tasks.\n                     The Gemini family advances state-of-the-art in large-scale language modeling (Anil et al., 2023;\n                Brown et al., 2020; Chowdhery et al., 2023; Hoffmann et al., 2022; OpenAI, 2023a; Radford et al.,\n                2019; Rae et al., 2021), image understanding (Alayrac et al., 2022; Chen et al., 2022; Dosovitskiy\n                et al., 2020; OpenAI, 2023b; Reed et al., 2022; Yu et al., 2022a), audio processing (Radford et al.,\n                2023; Zhang et al., 2023), and video understanding (Alayrac et al., 2022; Chen et al., 2023). It\n                also builds on the work on sequence models (Sutskever et al., 2014), a long history of work in deep\n                learning based on neural networks (LeCun et al., 2015), and machine learning distributed systems\n                   1See Contributions and Acknowledgments section for full author list. Please send correspondence to gemini-1-\n                report@google.com\n\n\n                \u00a9 2024 Google. All rights reserved", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4133, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "309dbfa8-2e7e-4def-ad8d-068a9addfe8e": {"__data__": {"id_": "309dbfa8-2e7e-4def-ad8d-068a9addfe8e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "df73c18a-5e55-4389-8ee2-092b394ce21c", "node_type": "4", "metadata": {}, "hash": "45b042def175def5b72d2a6e95634485be7d5688f175f11014f72aafd6ea3021", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n(Barham et al., 2022; Bradbury et al., 2018; Dean et al., 2012) that enable large-scale training.\n     Our most capable model, Gemini Ultra, achieves new state-of-the-art results in 30 of 32 benchmarks\nwe report on, including 10 of 12 popular text and reasoning benchmarks, 9 of 9 image understanding\n benchmarks, 6 of 6 video understanding benchmarks, and 5 of 5 speech recognition and speech\n translation benchmarks. Gemini Ultra is the first model to achieve human-expert performance on\n MMLU (Hendrycks et al., 2021a) \u2014 a prominent benchmark testing knowledge and reasoning via a\n suite of exams \u2014 with a score above 90%. Beyond text, Gemini Ultra makes notable advances on\n challenging multimodal reasoning tasks. For example, on the recent MMMU benchmark (Yue et al.,\n 2023), that comprises questions about images on multi-discipline tasks requiring college-level subject\n knowledge and deliberate reasoning, Gemini Ultra achieves a new state-of-the-art score of 62.4%,\n outperforming the previous best model by more than 5 percentage points. It provides a uniform\n performance lift for video question answering and audio understanding benchmarks.\n     Qualitative evaluation showcases impressive crossmodal reasoning capabilities, enabling the model\n to understand and reason across an input sequence of audio, images, and text natively (see Figure 5\n and Table 13). Consider the educational setting depicted in Figure 1 as an example. A teacher has\n drawn a physics problem of a skier going down a slope, and a student has worked through a solution to\n it. Using Gemini models\u2019 multimodal reasoning capabilities, the model is able to understand the messy\n handwriting, correctly understand the problem formulation, convert both the problem and solution\n to mathematical typesetting, identify the specific step of reasoning where the student went wrong in\n solving the problem, and then give a worked through correct solution to the problem. This opens up\n exciting educational possibilities, and we believe the new multimodal and reasoning capabilities of\nGemini models have dramatic applications across many fields.\n     The reasoning capabilities of large language models show promise toward building generalist\n agents that can tackle more complex multi-step problems. The AlphaCode team built AlphaCode\n 2 (Leblond et al, 2023), a new Gemini-model-powered agent, that combines Gemini models\u2019 rea-\n soning capabilities with search and tool-use to excel at solving competitive programming problems.\n AlphaCode 2 ranks within the top 15% of entrants on the Codeforces competitive programming\n platform, a large improvement over its state-of-the-art predecessor in the top 50% (Li et al., 2022).\n     In tandem, we advance the frontier of efficiency with Gemini Nano, a series of small models\n targeting on-device deployment. These models excel in on-device tasks, such as summarization,\n reading comprehension, text completion tasks, and exhibit impressive capabilities in reasoning, STEM,\n coding, multimodal, and multilingual tasks relative to their sizes.\n     In the following sections, we first provide an overview of the model architecture, training infras-\n tructure, and pre-training dataset. We then present detailed evaluations of the pre- and post-trained\nGemini model family, covering well-studied benchmarks across text, code, image, audio and video \u2014\nwhich include both English performance and multilingual capabilities. Next we discuss our approach\n to post-training, highlight common and distinct aspects of the Gemini Apps and Gemini API model\n variants, and benchmark their performance on key capabilities. Responsible deployment is critical: we\n explain our process for impact assessments, developing model policies, evaluations, and mitigations\n of harm before deployment decisions. Finally, we discuss the broader implications of Gemini models,\n their limitations alongside their potential applications \u2014 paving the way for a new era of research\n and innovation in AI.\n\n\n                                                                                                          2", "mimetype": "text/plain", "start_char_idx": 39, "end_char_idx": 4202, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4fe68737-08af-473c-a056-570e8f5ab130": {"__data__": {"id_": "4fe68737-08af-473c-a056-570e8f5ab130", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b03751a5-6dfb-4237-abd6-4c667ff2e263", "node_type": "4", "metadata": {}, "hash": "968685b148747a78c16bd9d858c5870cbf2b6eb017e184e532d83904f73d6c15", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n     Prompt                                                                                         Response\n      Here'$ = solution to     Physics problem by        student.                                    The student did notget =the correct answer: The student made                 mistake\n                                                                                                     in the calculation of the potential energy at the start of the slope: The\n            skiur slidos down thc friclionlcss slopo as shown What is thc skicr $ spced at thc bcIiom?potential energy at the start of the slope        given by mgh, where m is the\n                                                                                                     mass of the skier; g is the acceleration due to gravity andh is the height\n                                                                                                     of the slope: The student used the length of the slope; L, instead of the\n                                                                                                     height; H; in the calculation of the potential energy:\n                     He4om\n                                                                                                     The correct solution is:\n                                                                   Finesh\n         Solution:                         L-8JM                                                     The conservation of energy means that the total energy of the skier is\n                                                                                                     the same at the beginning and at the end of the slope:\n            Te ceulahon                        enesgyMean&                 teos     #e\n                                  s               Some              4                                At the start of the slope; the skier has only potential energy; which is\n            OIy             f2ena                                                                    given by mgh\n            An             #                                                                         At the end of the slope, the skier has only kinetic energy; which is given\n             Stct        6=mj L                                                                            mv'\n            End :      1= 1 mv\"                                                                      Therefore, we can write:\n                       naL= hmv                                                                                                          mgh =2mv'\n                      V=N23L                 2 r9-812 80                 39.6 m s\"                   Canceling out the mass m on bothsides;               get:\n                                                                                                                                           gh = zv'\n      Try to reason about the question step by step:                                                 Solving for     we get:\n      Did the studentgetthe correct answer? If the solution is wrong; please                                                                   Vzgh\n      explain what is wrong and solve the problem:                                                   Substituting the given values_          get:\n      Make sure       use LaTeX for math and round off the final answer to two                                                      Vz x 9.81X40\n      decimal places:                                                                                                                                 28.01 mls\n                                                                                                     Therefore, the skier'$ speed at the bottom of the slope is 28.01 mls_\n Figure 1 | Verifying a student\u2019s solution to a physics problem. The model is able to correctly recognize\n all of the handwritten content and verify the reasoning. On top of understanding the text in the\n image, it needs to understand the problem setup and correctly follow instructions to generate LATEX.\n\n\n 2. Model Architecture\n\n\nGemini models build on top of Transformer decoders (Vaswani et al., 2017b) that are enhanced\nwith improvements in architecture and model optimization to enable stable training at scale and\n optimized inference on Google\u2019s Tensor Processing Units. They are trained to support 32k context\n length, employing efficient attention mechanisms (for e.g. multi-query attention (Shazeer, 2019a)).\nOur first version, Gemini 1.0, comprises three main sizes to support a wide range of applications as\n discussed in Table 1.\n        Gemini models are trained to accommodate textual input interleaved with a wide variety of audio\n and visual inputs, such as natural images, charts, screenshots, PDFs, and videos, and they can produce\n text and image outputs (see Figure 2). The visual encoding of Gemini models is inspired by our own\n foundational work on Flamingo (Alayrac et al., 2022), CoCa (Yu et al., 2022a), and PaLI (Chen et al.,\n 2022), with the important distinction that the models are multimodal from the beginning and can\n natively output images using discrete image tokens (Ramesh et al., 2021; Yu et al., 2022b).\n        Video understanding is accomplished by encoding the video as a sequence of frames in the large\n context window. Video frames or images can be interleaved naturally with text or audio as part of the\n model input. The models can handle variable input resolution in order to spend more compute on\n tasks that require fine-grained understanding. In addition, Gemini models can directly ingest audio\n\n\n                                                                                                                                                                                           3", "mimetype": "text/plain", "start_char_idx": 57, "end_char_idx": 5999, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e730ac37-60ab-4fcb-969a-88fdea1ecc26": {"__data__": {"id_": "e730ac37-60ab-4fcb-969a-88fdea1ecc26", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9b1889e5-5909-4777-9934-4aa5ec509f20", "node_type": "4", "metadata": {}, "hash": "0bf105fab8627391b66d163b021d4beaeeabf2bb441f63041a915a0d7469f9e9", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n  Model size         Model description\n  Ultra              Our most capable model that delivers state-of-the-art performance across a wide\n                     range of highly complex tasks, including reasoning and multimodal tasks. It is\n                     efficiently serveable at scale on TPU accelerators due to the Gemini architecture.\n  Pro                A performance-optimized model in terms of cost as well as latency that delivers\n                     significant performance across a wide range of tasks. This model exhibits strong\n                     reasoning performance and broad multimodal capabilities.\n  Nano               Our most efficient model, designed to run on-device. We trained two versions of\n                     Nano, with 1.8B (Nano-1) and 3.25B (Nano-2) parameters, targeting low and high\n                     memory devices respectively. It is trained by distilling from larger Gemini models. It\n                     is 4-bit quantized for deployment and provides best-in-class performance.\n\n\n Table 1 | An overview of the Gemini 1.0 model family.\n           Input\n         Sequence\n           Aa\n                                                                                                      Image\n                                                                                                     Decoder\n                                                               Transformer\n                                                                                                     DecoderText  Aa\n Figure 2 | Gemini models support interleaved sequences of text, image, audio, and video as inputs\n(illustrated by tokens of different colors in the input sequence). They can output responses with\n interleaved image and text.\n\n\n signals at 16kHz from Universal Speech Model (USM) (Zhang et al., 2023) features. This enables the\n model to capture nuances that are typically lost when the audio is naively mapped to a text input (for\n example, see audio understanding demo on the website).\n     Training the Gemini family of models required innovations in training algorithms, dataset, and\n infrastructure. For the Pro model, the inherent scalability of our infrastructure and learning algorithms\n enable us to complete pre-training in a matter of weeks, leveraging a fraction of the Ultra\u2019s resources.\n The Nano series of models leverage additional advancements in distillation and training algorithms\n to produce the best-in-class small language models for a wide variety of tasks, such as summarization\n and reading comprehension, which power our next generation on-device experiences.\n\n\n 3. Training Infrastructure\n\n\nWe trained Gemini models using TPUv5e and TPUv4 (Jouppi et al., 2023), depending on their sizes\n and configuration. Training Gemini Ultra used a large fleet of TPUv4 accelerators owned by Google\n\n\n                                                                                                                      4", "mimetype": "text/plain", "start_char_idx": 39, "end_char_idx": 3073, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6db95581-861d-4056-b4f7-c8f49b69d52d": {"__data__": {"id_": "6db95581-861d-4056-b4f7-c8f49b69d52d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fdcb044a-a0d6-44f9-ba93-3977f323d9ab", "node_type": "4", "metadata": {}, "hash": "92edcd80c5f4f28a746ad6c90c8252c2ea2d45cabb5754119e9b4eeee7d5c017", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n across multiple datacenters. This represents a significant increase in scale over our prior flagship\n model PaLM-2 which presented new infrastructure challenges. Scaling up the number of accelerators\n results in a proportionate decrease in the mean time between failure of hardware in the overall system.\nWe minimized the rate of planned reschedules and preemptions, but genuine machine failures are\n commonplace across all hardware accelerators at such large scales.\n      TPUv4 accelerators are deployed in \u201cSuperPods\u201d of 4096 chips, each connected to a dedicated\n optical switch, which can dynamically reconfigure 4x4x4 chip cubes into arbitrary 3D torus topologies\n in around 10 seconds (Jouppi et al., 2023). For Gemini Ultra, we decided to retain a small number of\n cubes per superpod to allow for hot standbys and rolling maintenance.\n      TPU accelerators primarily communicate over the high speed inter-chip-interconnect, but at\nGemini Ultra scale, we combine SuperPods in multiple datacenters using Google\u2019s intra-cluster and\n inter-cluster network (Poutievski et al., 2022; Wetherall et al., 2023; yao Hong et al., 2018). Google\u2019s\n network latencies and bandwidths are sufficient to support the commonly used synchronous training\n paradigm, exploiting model parallelism within superpods and data-parallelism across superpods.\n      The \u2018single controller\u2019 programming model of Jax (Bradbury et al., 2018) and Pathways (Barham\n et al., 2022) allows a single Python process to orchestrate the entire training run, dramatically\n simplifying the development workflow. The GSPMD partitioner (Xu et al., 2021) in the XLA compiler\n partitions the training step computation, and the MegaScale XLA compiler (XLA, 2019) pass statically\n schedules appropriate collectives so that they maximally overlap with the computation with very little\n variation in step time.\n      Maintaining a high goodput2 at this scale would have been impossible using the conventional\n approach of periodic checkpointing of weights to persistent cluster storage. For Gemini models, we\n instead made use of redundant in-memory copies of the model state, and on any unplanned hardware\n failures, we rapidly recover directly from an intact model replica. Compared to both PaLM and PaLM-2\n(Anil et al., 2023), this provided a substantial speedup in recovery time, despite the significantly\n larger training resources being used. As a result, the overall goodput for the largest-scale training job\n increased from 85% to 97%.\n      Training at unprecedented scale invariably surfaces new and interesting systems failure modes -\n and in this instance one of the problems that we needed to address was that of \u201cSilent Data Corruption\n(SDC)\u201d (Dixit et al., 2021; Hochschild et al., 2021; Vishwanathan et al., 2015). Although these are\n extremely rare, the scale of Gemini models means that we can expect SDC events to impact training\n every week or two. Rapidly detecting and removing faulty hardware required several new techniques\n that exploit deterministic replay to isolate incorrect computations, combined with proactive SDC\n scanners on idle machines and hot standbys. Our fully deterministic infrastructure allowed us to\n quickly identify root causes (including hardware failures) during the development leading up to the\n Ultra model, and this was a crucial ingredient towards stable training.\n\n\n 4. Pre-Training Dataset\n\n\nGemini models are trained on a dataset that is both multimodal and multilingual. Our pre-training\n dataset uses data from web documents, books, and code, and includes image, audio, and video data.\n      We use the SentencePiece tokenizer (Kudo and Richardson, 2018) and find that training the\n tokenizer on a large sample of the entire training corpus improves the inferred vocabulary and\n subsequently improves model performance. For example, we find Gemini models can efficiently\n    2We define goodput as the time spent computing useful new steps over the elapsed time of the training job.\n\n\n                                                                                                                5", "mimetype": "text/plain", "start_char_idx": 39, "end_char_idx": 4207, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2a803ccc-4401-4854-9e90-e1a119a6eb12": {"__data__": {"id_": "2a803ccc-4401-4854-9e90-e1a119a6eb12", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9bee60bc-35e5-4c90-b5a8-57d9d4aca5f3", "node_type": "4", "metadata": {}, "hash": "c8fef5fcb2038e77f5014639aefbd69823e361dd937add8304d76a67f0e0393a", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n tokenize non-Latin scripts which can, in turn, benefit model quality as well as training and inference\n speed.\n     The number of tokens used to train the largest models were determined following the approach\n in Hoffmann et al. (2022). The smaller models are trained for significantly more tokens to improve\n performance for a given inference budget, similar to the approach advocated in Touvron et al. (2023a).\n     We apply quality filters to all datasets, using both heuristic rules and model-based classifiers.\nWe also perform safety filtering to remove harmful content based on our policies. To maintain the\n integrity of evaluations, we search for and remove any evaluation data that may have been in our\n training corpus before using data for training. The final data mixtures and weights were determined\n through ablations on smaller models. We stage training to alter the mixture composition during\n training \u2013 increasing the weight of domain-relevant data towards the end of training. We find that\n data quality is an important factor for highly-performing models, and believe that many interesting\n questions remain around finding the optimal dataset distribution for pre-training.\n\n\n 5. Evaluation\n\n\n The Gemini models are natively multimodal, as they are trained jointly across text, image, audio,\n and video. One open question is whether this joint training can result in a model which has strong\n capabilities in each domain \u2013 even when compared to models and approaches that are narrowly\n tailored to single domains. We find this to be the case: Gemini models set a new state of the art\n across a wide range of text, image, audio, and video benchmarks. ww\n\n\n 5.1. Text\n\n\n 5.1.1. Academic Benchmarks\n\n\nWe compare pre- and post-trained Gemini Pro and Ultra models to a suite of external LLMs and our\n previous best model PaLM 2 across a series of text-based academic benchmarks covering reasoning,\n reading comprehension, STEM, and coding. We report these results in Table 2. Broadly, we find\n that the performance of Gemini Pro outperforms inference-optimized models such as GPT-3.5 and\n performs comparably with several of the most capable models available, and Gemini Ultra outperforms\n all current models. In this section, we examine some of these findings.\n     On MMLU (Hendrycks et al., 2021a), Gemini Ultra can outperform all existing models, achieving\n an accuracy of 90.04%. MMLU is a holistic exam benchmark, which measures knowledge across a\n set of 57 subjects. Human expert performance is gauged at 89.8% by the benchmark authors, and\nGemini Ultra is the first model to exceed this threshold, with the prior state-of-the-art result at 86.4%.\n Achieving high performance requires specialist knowledge across many domains (e.g. law, biology,\n history, etc.), alongside reading comprehension and reasoning. We find Gemini Ultra achieves highest\n accuracy when used in combination with a chain-of-thought prompting approach (Wei et al., 2022b)\n that accounts for model uncertainty. The model produces a chain of thought with k samples, for\n example 8 or 32. If there is a consensus above a preset threshold (selected based on the validation\n split), it selects this answer, otherwise it reverts to a greedy sample based on maximum likelihood\n choice without chain of thought. We refer the reader to appendix for a detailed breakdown of how\n this approach compares with only chain-of-thought prompting or only greedy sampling.\n     In mathematics, a field commonly used to benchmark the analytical capabilities of models, Gemini\n Ultra shows strong performance on both elementary exams and competition-grade problem sets. For\n the grade-school math benchmark, GSM8K (Cobbe et al., 2021), we find Gemini Ultra reaches 94.4%\n\n\n                                                                                                            6", "mimetype": "text/plain", "start_char_idx": 38, "end_char_idx": 3952, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bac5362d-6307-41c5-a3b9-decd1c5ee303": {"__data__": {"id_": "bac5362d-6307-41c5-a3b9-decd1c5ee303", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4a05cb45-6040-4edb-9318-4603662c1c60", "node_type": "4", "metadata": {}, "hash": "26f92279b17fc2183e2f58d0946e2c2c1f4396ec2ae30ceb76deb6d3ad356d76", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\naccuracy with chain-of-thought prompting and self-consistency (Wang et al., 2022) compared to\nthe previous best accuracy of 92% with the same prompting technique. Similar positive trends are\nobserved in increased difficulty math problems drawn from middle- and high-school math competitions\n(MATH benchmark), with the Gemini Ultra model outperforming all competitor models, reaching\n53.2% using 4-shot prompting. The model also outperforms the state of the art on even harder tasks\nderived from American Mathematical Competitions (150 questions from 2022 and 2023). Smaller\nmodels perform poorly on this challenging task scoring close to random, but Gemini Ultra can solve\n32% of the questions, compared to the 30% solve rate for GPT-4.\n     Gemini Ultra also excels in coding, a popular use case of current LLMs. We evaluate the model\non many conventional and internal benchmarks and also measure its performance as part of more\ncomplex reasoning systems such as AlphaCode 2 (see Section 5.1.7 on complex reasoning systems).\nFor example, on HumanEval, a standard code-completion benchmark (Chen et al., 2021) mapping\nfunction descriptions to Python implementations, instruction-tuned Gemini Ultra correctly implements\n74.4% of problems. On a new held-out evaluation benchmark for python code generation tasks,\nNatural2Code, where we ensure no web leakage, Gemini Ultra achieves the highest score of 74.9%.\n     Evaluation on these benchmarks is challenging and may be affected by data contamination. We\nperformed an extensive leaked data analysis after training to ensure the results we report here are as\nscientifically sound as possible, but still found some minor issues and decided not to report results on\ne.g. LAMBADA (Paperno et al., 2016). As part of the evaluation process, on a popular benchmark,\nHellaSwag (Zellers et al., 2019), we find that an additional hundred fine-tuning steps on specific\nwebsite extracts corresponding to the HellaSwag training set (which were not included in the Gemini\nmodel pretraining set) improve the validation accuracy of Gemini Pro to 89.6% and Gemini Ultra to\n96.0%, when measured with 1-shot prompting (we measured GPT-4 obtained 92.3% when evaluated\n1-shot via the API). This suggests that the benchmark results are susceptible to the pretraining dataset\ncomposition. We choose to report HellaSwag decontaminated results only in a 10-shot evaluation\nsetting. We believe there is a need for more robust and nuanced standardized evaluation benchmarks\nwith no leaked data. So, we evaluate Gemini models on several new held-out evaluation datasets\nthat were recently released, such as WMT23 and Math-AMC 2022-2023 problems, or internally\ngenerated from non-web sources, such as Natural2Code. We refer the reader to Appendix 10.3 for a\ncomprehensive list of our evaluation benchmarks.\n     Even so, model performance on these benchmarks gives us an indication of the model capabilities\nand where they may provide impact on real-world tasks. For example, Gemini Ultra\u2019s impressive\nreasoning and STEM competencies pave the way for advancements in LLMs within the educational\ndomain3. The ability to tackle complex mathematical and scientific concepts opens up exciting\npossibilities for personalized learning and intelligent tutoring systems.\n\n\n5.1.2. Trends in Capabilities\n\n\nWe investigate the trends in capabilities across the Gemini model family by evaluating them on a\nholistic harness of more than 50 benchmarks in six different capabilities, noting that some of the\nmost notable benchmarks were discussed in the last section. These capabilities are: \u201cFactuality\u201d\ncovering open/closed-book retrieval and question answering tasks; \u201cLong-Context\u201d covering long-\nform summarization, retrieval and question answering tasks; \u201cMath/Science\u201d including tasks for\nmathematical problem solving, theorem proving, and scientific exams; \u201cReasoning\u201d tasks that require\narithmetic, scientific, and commonsense reasoning; \u201cMultilingual\u201d tasks for translation, summarization,\nand reasoning in multiple languages. Several of these capabilities are targeted by post-training\n(Section 6). Please see Appendix 10.3 for a detailed list of tasks included for each capability.\n   3See demos on website https://deepmind.google/gemini.\n\n\n                                                                                                          7", "mimetype": "text/plain", "start_char_idx": 38, "end_char_idx": 4458, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "63aee76d-6e26-4d2a-8da2-9760ea1dcdd2": {"__data__": {"id_": "63aee76d-6e26-4d2a-8da2-9760ea1dcdd2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d833b0a9-8ae5-4878-b40b-a4e8b0c0b453", "node_type": "4", "metadata": {}, "hash": "e56d82a10bf602415f2f1bbfc4bdeb1398dadeb5680d177f9b931aa40262ce46", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d1db085d-77d7-4f83-b8ee-49e222bce7ab", "node_type": "1", "metadata": {}, "hash": "e0e13ea278d1eec62b29ff451279fe47e50b37f85e75e068b1199524e31ebd1e", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n                             Gemini          Gemini          GPT-4           GPT-3.5        PaLM 2-L        Claude 2      Inflect-       Grok 1  LLAMA-2\n                             Ultra           Pro                                                                          ion-2\n  MMLU\n  Multiple-choice questions  90.04%\n                             CoT@32\u2217         CoT@8\u2217\n                                                   79.13%    87.29%          70%            78.4%           78.5%         79.6%          73.0%   68.0%\u2217\u2217\u2217\n                                                             (via API\u2217\u2217)\n                                                                        CoT@325-shot        5-shot          5-shot CoT    5-shot         5-shot\n  in 57 subjects\n  (professional &\n  academic)                  83.7%           71.8%           86.4%\n  (Hendrycks et al., 2021a)  5-shot          5-shot          5-shot\n                                                             (reported)\n  GSM8K                      94.4%           86.5%           92.0%           57.1%          80.0%           88.0%         81.4%          62.9%   56.8%\n  Grade-school math          Maj1@32         Maj1@32         SFT &           5-shot         5-shot          0-shot        8-shot         8-shot  5-shot\n  (Cobbe et al., 2021)                                       5-shot CoT\n  MATH                       53.2%           32.6%           52.9%           34.1%          34.4%           \u2014             34.8%          23.9%   13.5%\n  Math problems across       4-shot          4-shot          4-shot\n                                                             (via API\u2217\u2217)     4-shot\n                                                                             (via API\u2217\u2217)    4-shot                                       4-shot  4-shot\n  5 difficulty levels &\n  7 subdisciplines\n  (Hendrycks et al., 2021b)                                  50.3%\n                                                             (Zheng et al.,\n                                                             2023)\n  BIG-Bench-Hard             83.6%           75.0%           83.1%           66.6%          77.7%           \u2014             \u2014              \u2014       51.2%\n  Subset of hard BIG-bench   3-shot          3-shot          3-shot          3-shot         3-shot                                               3-shot\n  tasks written as CoT prob-                                 (via API\u2217\u2217)     (via API\u2217\u2217)\n  lems\n  (Srivastava et al., 2022)\n  HumanEval                  74.4%           67.7%           67.0%           48.1%          \u2014               70.0%         44.5%          63.2%   29.9%\n  Python coding tasks        0-shot          0-shot          0-shot          0-shot                         0-shot        0-shot         0-shot  0-shot\n  (Chen et al., 2021)       (PT\u2217\u2217\u2217\u2217)         (PT\u2217\u2217\u2217\u2217)        (reported)\n  Natural2Code               74.9%           69.6%           73.9%           62.3%          \u2014               \u2014             \u2014              \u2014       \u2014\n  Python code generation.    0-shot          0-shot          0-shot          0-shot\n  (New held-out set with no                                  (via API\u2217\u2217)     (via API\u2217\u2217)\n  leakage on web)\n  DROP                       82.4            74.1            80.9            64.1           82.0            \u2014             \u2014              \u2014       \u2014\n  Reading comprehension      Variable        Variable        3-shot          3-shot         Variable\n  & arithmetic.", "mimetype": "text/plain", "start_char_idx": 48, "end_char_idx": 3578, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d1db085d-77d7-4f83-b8ee-49e222bce7ab": {"__data__": {"id_": "d1db085d-77d7-4f83-b8ee-49e222bce7ab", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d833b0a9-8ae5-4878-b40b-a4e8b0c0b453", "node_type": "4", "metadata": {}, "hash": "e56d82a10bf602415f2f1bbfc4bdeb1398dadeb5680d177f9b931aa40262ce46", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "63aee76d-6e26-4d2a-8da2-9760ea1dcdd2", "node_type": "1", "metadata": {}, "hash": "9700270b89dde24d54a2898bc5f42e1eec0274844997d3d612441f4877eedb96", "class_name": "RelatedNodeInfo"}}, "text": "0-shot          0-shot          0-shot          0-shot\n  (New held-out set with no                                  (via API\u2217\u2217)     (via API\u2217\u2217)\n  leakage on web)\n  DROP                       82.4            74.1            80.9            64.1           82.0            \u2014             \u2014              \u2014       \u2014\n  Reading comprehension      Variable        Variable        3-shot          3-shot         Variable\n  & arithmetic.              shots           shots           (reported)                     shots\n  (metric: F1-score)\n  (Dua et al., 2019)\n  HellaSwag                  87.8%           84.7%           95.3%           85.5%          86.8%           \u2014             89.0%          \u2014       80.0%\u2217\u2217\u2217\n  (validation set)          10-shot          10-shot         10-shot         10-shot        10-shot                       10-shot\n  Common-sense multiple                                      (reported)\n  choice questions\n  (Zellers et al., 2019)\n  WMT23                      74.4            71.7            73.8            \u2014              72.7            \u2014             \u2014              \u2014       \u2014\n  Machine translation (met- 1-shot\n                            (PT\u2217\u2217\u2217\u2217)         1-shot          1-shot\n                                                             (via API\u2217\u2217)                    1-shot\n  ric: BLEURT)\n  (Tom et al., 2023)\n\n\nTable 2 | Gemini performance on text benchmarks with external comparisons and PaLM 2-L.\n\u2217 The model produces a chain of thought with k = 8 or 32 samples, if there is a consensus above a threshold (chosen based on the validation\nsplit), it selects this answer, otherwise it reverts to a greedy sample. Further analysis in Appendix 10.2.\n\u2217\u2217 Results self-collected via the API in Nov, 2023.\n\u2217\u2217\u2217 Results shown use the decontaminated numbers from Touvron et al. (2023b) report as the most relevant comparison to Gemini models\nwhich have been decontaminated as well.)\n\u2217\u2217\u2217\u2217 PT denotes a post-trained Gemini API model.\n\n\n      We observe consistent quality gains with increased model size in Figure 3, especially in reasoning,\nmath/science, summarization and long-context. Gemini Ultra is the best model across the board for\nall six capabilities. Gemini Pro, the second-largest model in the Gemini family of models, is also quite\ncompetitive while being a lot more efficient to serve.\n\n\n5.1.3. Nano\n\n\nBringing AI closer to the user, we discuss the Gemini Nano 1 and Nano 2 models engineered for\non-device deployments. These models excel in summarization and reading comprehension tasks with\nper-task fine-tuning. Figure 3 shows the performance of these pre-trained models in comparison\nto the much larger Gemini Pro model, while Table 3 dives deeper into specific factuality, coding,\nMath/Science, and reasoning tasks. Nano-1 and Nano-2 model sizes are only 1.8B and 3.25B\nparameters respectively. Despite their size, they show exceptionally strong performance on factuality,\ni.e. retrieval-related tasks, and significant performance on reasoning, STEM, coding, multimodal and\n\n\n                                                                                                                                                           8", "mimetype": "text/plain", "start_char_idx": 3153, "end_char_idx": 6317, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2c84f064-6027-4e6e-9881-0785be5a9a46": {"__data__": {"id_": "2c84f064-6027-4e6e-9881-0785be5a9a46", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e90003a3-12ee-4960-a6ba-7ac28d985723", "node_type": "4", "metadata": {}, "hash": "bc6726c0167da97bf0838c33c69ed15e7c3876abb7b0720cd2defebd8145b848", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n                  1.4\n                                                                                                              Nano 1\n                  1.2                                                                                         Nano 2Normalized Performance vs Pro\n                  1.0                                                                                         Pro\n                                                                                                              Ultra\n                  0.8\n\n\n                  0.6\n\n\n                  0.4\n\n\n                  0.2\n\n\n                  0.0\n\n\n                            Factuality                                           Reasoning\n                                        Long-ContextMath/ScienceSummarization                Multilinguality\n\n\n Figure 3 | Language understanding and generation performance of Gemini model family across\n different capabilities (normalized by the Gemini Pro model).\n\n\n multilingual tasks. With new capabilities accessible to a broader set of platforms and devices, the\nGemini models expand accessibility to everyone.\n\n\n                                                                  Gemini Nano 1         Gemini Nano 2\n\n\n                                                               accuracy    normalizedaccuracy   normalized\n                                                                           by Pro               by Pro\n\n\n                                 BoolQ                         71.6        0.81      79.3       0.90\n                                 TydiQA (GoldP)                68.9        0.85      74.2       0.91\n                                 NaturalQuestions (Retrieved)  38.6        0.69      46.5       0.83\n                                 NaturalQuestions (Closed-book) 18.8       0.43      24.8       0.56\n                                 BIG-Bench-Hard (3-shot)       34.8        0.47      42.4       0.58\n                                 MBPP                          20.0        0.33      27.2       0.45\n                                 MATH (4-shot)                 13.5        0.41      22.8       0.70\n                                 MMLU (5-shot)                 45.9        0.64      55.8       0.78\n\n\n Table 3 | Performance of Gemini Nano series on factuality, summarization, reasoning, coding and\n STEM tasks compared to significantly larger Gemini Pro model.\n\n\n 5.1.4. Multilinguality\n\n\n The multilingual capabilities of the Gemini models are evaluated using a diverse set of tasks requir-\n ing multilingual understanding, cross-lingual generalization, and the generation of text in multiple\n languages. These tasks include machine translation benchmarks (WMT 23 for high-medium-low\n resource translation; Flores, NTREX for low and very low resource languages), summarization bench-\n marks (XLSum, Wikilingua), and translated versions of common benchmarks (MGSM: professionally\n translated into 11 languages).\n\n\n 5.1.4.1      Machine Translation\n\n\n Translation is a canonical benchmark in machine learning with a rich history. We evaluated a post-\n trained Gemini API Ultra model (see Section 6.5.3) on the entire set of language pairs in the WMT 23\n translation benchmark in a few-shot setting. Overall, we found that Gemini Ultra (and other Gemini\n models) performed remarkably well at translating from English to any other language, and surpassed\n\n\n                                                                                                                                    9", "mimetype": "text/plain", "start_char_idx": 41, "end_char_idx": 3652, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a1f940f1-eb60-4f06-843b-54c7d83e74e0": {"__data__": {"id_": "a1f940f1-eb60-4f06-843b-54c7d83e74e0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d4c1b94e-ab41-49f3-98b3-e897a2fe5e7e", "node_type": "4", "metadata": {}, "hash": "d993801f853d715ce0f0ddf7ad86cb99912e5b4dfaf943eb7c19c0fc997fe0be", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\nthe LLM-based translation methods when translating out-of-English, on high-resource, mid-resource\nand low-resource languages. In the WMT 23 out-of-English translation tasks, Gemini Ultra achieved\nthe highest LLM-based translation quality, with an average BLEURT (Sellam et al., 2020) score of 74.8,\ncompared to GPT-4\u2019s score of 73.6, and PaLM 2\u2019s score of 72.2. When averaged across all language\npairs and directions for WMT 23, we see a similar trend with Gemini Ultra 74.4, GPT-4 73.8 and\nPaLM 2-L 72.7 average BLEURT scores on this benchmark.\n\n\n           WMT 23                  Gemini Ultra    Gemini Pro     Gemini Nano 2     Gemini Nano 1     GPT-4  PaLM 2-L\n           (Avg BLEURT)\n           High Resource           74.2            71.7           67.7              64.1              74.0   72.6\n           Mid Resource            74.7            71.8           67.0              64.8              73.6   72.7\n           Out-of-English          74.8            71.5           66.2              65.2              73.6   72.2\n           Into-English            73.9            72.0           69.0              63.5              74.1   73.4\n           All languages           74.4            71.7           67.4              64.8              73.8   72.7\n\n\nTable 4 | Performance of Gemini models on WMT 23 translation benchmark. All numbers with 1-shot.\n\n\n     In addition to the languages and translation tasks above, we also evaluate Gemini Ultra on very\nlow-resource languages. These languages were sampled from the tail of the following language sets:\nFlores-200 (Tamazight and Kanure), NTREX (North Ndebele), and an internal benchmark (Quechua).\nFor these languages, both from and into English, Gemini Ultra achieved an average chrF score of 27.0\nin 1-shot setup, while the next-best model, PaLM 2-L, achieved a score of 25.3.\n\n\n5.1.4.2      Multilingual Math and Summarization\n\n\nBeyond translation, we evaluated how well Gemini models perform in challenging tasks across a\nrange of languages. We specifically investigated the math benchmark MGSM (Shi et al., 2023), which\nis a translated variant of the math benchmark GSM8K (Cobbe et al., 2021). We find Gemini Ultra\nachieves an accuracy of 79.0%, an advance over PaLM 2-L which scores 74.7%, when averaged\nacross all languages in an 8-shot setup. We also benchmark Gemini models on the multilingual\nsummarization benchmarks \u2013 XLSum (Hasan et al., 2021) and WikiLingua (Ladhak et al., 2020). In\nXLSum, Gemini Ultra reached an average of 17.6 rougeL score compared to 15.4 for PaLM 2. For\nWikilingua, Gemini Ultra (5-shot) trails behind PaLM 2 (3-shot) measured in BLEURT score. See\nTable 5 for the full results. Overall the diverse set of multilingual benchmarks show that Gemini\nfamily models have a broad language coverage, enabling them to also reach locales and regions with\nlow-resource languages.\n\n\n                                                     Gemini Ultra    Gemini Pro    GPT-4     PaLM 2-L\n                              MGSM (8-shot)          79.0            63.5           74.5       74.7\n                              XLsum (3-shot)         17.6            16.2            \u2014         15.4\n                              Wikilingua             48.9            47.8            \u2014         50.4\n\n\nTable 5 | Performance of Gemini models on multilingual math and summarization.\n\n\n5.1.5. Long Context\n\n\nGemini models are trained with a sequence length of 32,768 tokens and we find that they make use\nof their context length effectively. We first verify this by running a synthetic retrieval test: we place\nkey-value pairs at the beginning of the context, then add long filler text, and ask for value associated\nwith a particular key. We find that the Ultra model retrieves the correct value with 98% accuracy\nwhen queried across the full context length. We further investigate this by plotting the negative log\n\n\n                                                                                                                       10", "mimetype": "text/plain", "start_char_idx": 39, "end_char_idx": 4098, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "03aaedea-0a1f-4f4e-8172-1d8dc5a02ccd": {"__data__": {"id_": "03aaedea-0a1f-4f4e-8172-1d8dc5a02ccd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "39cf8f8f-9a62-464d-ab5a-30cb5e2b1278", "node_type": "4", "metadata": {}, "hash": "15053d50a3573af2a15dfb805e3f158762930a3cc94e319c2a26a195fc8c3328", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n        likelihood (NLL) versus the token index across a held-out set of long documents in Figure 4. We\n        find that the NLL decreases with sequence position up to the full 32K context length. The longer\n        context length of Gemini models enable new use cases such as retrieval over documents and video\n        understanding discussed in Section 5.2.2.\n\n\nNLLPro\n                                                                                                         Ultra\n\n\n                                     8     16    32    64   128    256  512    1K    2K    4K    8K    16K   32K\n                                                                 Sequence position\n\n\n        Figure 4 | Negative log likelihood as a function of token index across 32K context length on a held-out\n        set of long documents.\n\n\n        5.1.6. Factuality\n\n\n        Factuality (Maynez et al., 2020) is a key focus of our model\u2019s training and deployment. We evaluate\n        three aspects of factuality for our Gemini API models:\n\n\n           1. Closed-Book Factuality: If provided with a fact-seeking prompt without any given source,\n               Gemini API models should not hallucinate incorrect information (see Section 2 of Roberts et al.\n               (2020) for a definition). These prompts can range from information-seeking prompts (e.g. \u201cWho\n               is the prime minister of India?\u201d) to semi-creative prompts that may request factual information\n               (e.g. \u201cWrite a 500-word speech in favor of the adoption of renewable energy\u201d).\n           2. Attribution: If instructed to generate a response grounded to a given context, we aim to ensure\n               that Gemini API models produce a response with the highest degree of faithfulness to the\n               context (Maynez et al., 2020; Rashkin et al., 2023). This may include the summarization of a\n               user-provided source, generating fine-grained citations given a question and provided snippets\n               akin to Menick et al. (2022); Peng et al. (2023), answering questions from a long-form source\n               such as a book (Mihaylov et al., 2018), and transforming a given source to a desired output\n               (e.g. an email from a portion of a meeting transcript).\n           3. Hedging: If prompted with an input that is \u201cunanswerable\u201d, Gemini API models must ac-\n               knowledge that it cannot provide a response by hedging to avoid hallucination. These include\n               scenarios where the input prompt contains false-premise questions [see examples in Hu et al.\n               (2023)], the input prompt instructs the model to perform open book QA, but the answer is not\n               derivable from the given context, and so forth.\n\n\n             Factuality is evaluated via human annotators who fact-check each response manually; we report\n        the percentage of factually inaccurate responses as judged by annotators. Attribution is evaluated via\n        human annotators who check for attribution to sources in the prompt for each response manually;\n        the reported metric is AIS (Rashkin et al., 2023). For hedging, we use an automatic evaluation setup\n        where we measure whether models hedge accurately.\n             We compare Gemini API Pro with a version without any factuality-focused adaptation in Table 6.\n        We see that the rate of inaccuracy is halved in the factuality set, the accuracy of attribution is increased\n\n\n                                                                                                                      11", "mimetype": "text/plain", "start_char_idx": 46, "end_char_idx": 3702, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f873c89c-4916-4dd1-8ba9-3654590879f1": {"__data__": {"id_": "f873c89c-4916-4dd1-8ba9-3654590879f1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "62c30508-e83c-4515-8b72-318a4dd46462", "node_type": "4", "metadata": {}, "hash": "aeabf1ea826b21eb5132479e73acedf585c85ea10eba1b84fc0f1c706ff03076", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\nby 50% from the attribution set, and the model successfully hedges 70% (up from 0%) in the provided\nhedging set task.\n\n\n                                              Factuality                 Attribution                 Hedging\n                                              (Inaccurate Rate)          (AIS)                       (Accuracy)\n          Gemini API Pro                      6.7%                       40.2%                       0%\n          No factuality-focused adaptation    [5.8%, 7.8%]               [37.9%, 42.5%]\n\n\n          Gemini API Pro                      3.8%                       60.0%                       69.3%\n          Final stage of post-training        [3.1%, 4.8%]               [57.6%, 62.1%]\n\n\nTable 6 | Factuality mitigations: Impact of post-training on the rate of inaccuracy, presence of attribution\nand the rate of accurate hedging on Gemini API Pro (with corresponding 95% confidence intervals).\n\n\n5.1.7. Complex Reasoning Systems\n\n\nGemini models can also be combined with additional techniques such as search and tool-use to create\npowerful reasoning systems that can tackle more complex multi-step problems. One example of such\na system is AlphaCode 2, a new state-of-the-art agent that excels at solving competitive programming\nproblems (Leblond et al, 2023). AlphaCode 2 uses a specialized version of Gemini Pro \u2013 tuned on\ncompetitive programming data similar to the data used in Li et al. (2022) \u2013 to conduct a massive\nsearch over the space of possible programs. This is followed by a tailored filtering, clustering and\nreranking mechanism. Gemini Pro is fine-tuned both to be a coding model to generate proposal\nsolution candidates, and to be a reward model that is leveraged to recognize and extract the most\npromising code candidates.\n     AlphaCode 2 is evaluated on Codeforces,4 the same platform as AlphaCode, on 12 contests from\ndivision 1 and 2, for a total of 77 problems. AlphaCode 2 solved 43% of these competition problems, a\n1.7x improvement over the prior record-setting AlphaCode system which solved 25%. Mapping this to\ncompetition rankings, AlphaCode 2 built on top of Gemini Pro sits at an estimated 85th percentile on\naverage \u2013 i.e. it performs better than 85% of entrants. This is a significant advance over AlphaCode,\nwhich only outperformed 50% of competitors.\n     The composition of powerful pre-trained models with search and reasoning mechanisms is an\nexciting direction towards more general agents; another key ingredient is deep understanding across\na range of modalities which we discuss in the next section.\n\n\n   4http://codeforces.com/\n\n\n                                                                                                                 12", "mimetype": "text/plain", "start_char_idx": 40, "end_char_idx": 2827, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5cf9fa4c-7138-45a6-b7e7-84e1ffb51b81": {"__data__": {"id_": "5cf9fa4c-7138-45a6-b7e7-84e1ffb51b81", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c7afd9e9-bac8-4364-a1ac-d41ea937d1ef", "node_type": "4", "metadata": {}, "hash": "350be2cb45001a13ae3097410f6f36b8b2d6b810960a532266c552208e14ac54", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "51f23561-6cbf-4136-a5ab-ebb6def37f12", "node_type": "1", "metadata": {}, "hash": "952a6670d5c1c2a66169fc7f424a36418d31576b2ea9fbc6c5ac0ccd308fde8f", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n5.2. Multimodal\n\n\nGemini models are natively multimodal. These models exhibit the unique ability to seamlessly\ncombine their capabilities across modalities (e.g. extracting information and spatial layout out of\na table, a chart, or a figure) with the strong reasoning capabilities of a language model (e.g. its\nstate-of-art-performance in math and coding) as seen in examples in Figures 5 and 14. The models\nalso show strong performance in discerning fine-grained details in inputs, aggregating context across\nspace and time, and applying these capabilities over a temporally-related sequence of video frames\nand/or audio inputs.\n       The sections below provide more detailed evaluation of the model across different modalities\n(image, video, and audio), together with qualitative examples of the model\u2019s capabilities for image\ngeneration and the ability to combine information across different modalities.\n\n\n5.2.1. Image Understanding\n\n\nWe evaluate post-trained Gemini API models on four different capabilities: high-level object recogni-\ntion using captioning or question-answering tasks such as VQAv2; fine-grained transcription using\ntasks such as TextVQA and DocVQA requiring the model to recognize low-level details; chart un-\nderstanding requiring spatial understanding of input layout using ChartQA and InfographicVQA\ntasks; and multimodal reasoning using tasks such as Ai2D, MathVista and MMMU. For zero-shot QA\nevaluation, the model is instructed to provide short answers aligned with the specific benchmark. All\nnumbers are obtained using greedy sampling and without any use of external OCR tools.\n\n\n                                              Gemini               Gemini                Gemini             Gemini        GPT-4V        Prior SOTA\n                                              Ultra                Pro                   Nano 2             Nano 1\n                                              (pixel only)         (pixel only)         (pixel only)        (pixel only)\n    MMMU (val)                                59.4%                47.9%                 32.6%              26.3%         56.8%         56.8%\n    Multi-discipline college-level problems   pass@1                                                                                    GPT-4V, 0-shot\n   (Yue et al., 2023)\n                                              62.4%\n                                              Maj1@32\n    TextVQA (val)                             82.3%                74.6%                 65.9%              62.5%         78.0%          79.5%\n    Text reading on natural images                                                                                                      Google PaLI-3, fine-tuned\n   (Singh et al., 2019)\n    DocVQA (test)                             90.9%                88.1%                 74.3%              72.2%         88.4%         88.4%\n    Document understanding                                                                                                (pixel only)  GPT-4V, 0-shot\n   (Mathew et al., 2021)\n\n\n    ChartQA (test)                            80.8%                74.1%                 51.9%              53.6%         78.5%         79.3%\n    Chart understanding                                                                                                   (4-shot CoT)  Google DePlot, 1-shot PoT\n   (Masry et al., 2022)                                                                                                                 (Liu et al., 2023)\n    InfographicVQA (test)                     80.3%                75.2%                 54.5%              51.1%         75.1%         75.1%\n    Infographic understanding                                                                                             (pixel only)  GPT-4V, 0-shot\n   (Mathew et al., 2022)\n    MathVista (testmini)                      53.0%                45.2%                 30.6%              27.3%         49.9%         49.9%\n    Mathematical reasoning                                                                                                              GPT-4V, 0-shot\n   (Lu et al., 2023)\n    AI2D (test)                               79.5%                73.9%                 51.0%              37.9%         78.2%          81.4%\n    Science diagrams                                                                                                                    Google PaLI-X, fine-tuned\n   (Kembhavi et al., 2016)\n    VQAv2 (test-dev)                          77.8%                71.2%                 67.5%              62.7%         77.2%          86.1%\n    Natural image understanding                                                                                                         Google PaLI-X, fine-tuned\n   (Goyal et al., 2017)", "mimetype": "text/plain", "start_char_idx": 54, "end_char_idx": 4926, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "51f23561-6cbf-4136-a5ab-ebb6def37f12": {"__data__": {"id_": "51f23561-6cbf-4136-a5ab-ebb6def37f12", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c7afd9e9-bac8-4364-a1ac-d41ea937d1ef", "node_type": "4", "metadata": {}, "hash": "350be2cb45001a13ae3097410f6f36b8b2d6b810960a532266c552208e14ac54", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5cf9fa4c-7138-45a6-b7e7-84e1ffb51b81", "node_type": "1", "metadata": {}, "hash": "8ad5c1f4786f0c6250daa0b066ef57e662ffd884b20fe1ccbd1656c4b29731a3", "class_name": "RelatedNodeInfo"}}, "text": "Table 7 | Image understanding Gemini Ultra consistently outperforms existing approaches even in\nzero-shot, especially for OCR-related image understanding tasks for natural images, text, documents,\nand figures without using any external OCR engine (\u2018pixel only\u2019). Many existing approaches fine-tune\non the respective tasks, highlighted in gray, which makes the comparison with 0-shot not apples-to-\napples.\n\n\n                                                                                                                                                                   13", "mimetype": "text/plain", "start_char_idx": 4929, "end_char_idx": 5502, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4d3aafd5-7da3-4f8e-8cf7-3f2e285df1c0": {"__data__": {"id_": "4d3aafd5-7da3-4f8e-8cf7-3f2e285df1c0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8f733488-a603-4b16-b723-79ad0fc46e31", "node_type": "4", "metadata": {}, "hash": "3b46abfd460b9e7e7db67f929be0bb96d71cde6516fc632e065b884f6222f80e", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n    We find that Gemini Ultra is state of the art across a wide range of image-understanding bench-\nmarks in Table 7. It achieves strong performance across a diverse set of tasks such as answering\nquestions on natural images and scanned documents as well as understanding infographics, charts\nand science diagrams. When compared against publicly reported results from other models (most\nnotably GPT-4V), the Gemini model is better in zero-shot evaluation by a significant margin. It also\nexceeds several existing models that are specifically fine-tuned on the benchmark\u2019s training sets for\nthe majority of tasks. The capabilities of the Gemini models lead to significant improvements in the\nstate of the art on academic benchmarks like MathVista (+3.1%)5 or InfographicVQA (+5.2%).\n\n\n    MMMU (Yue et al., 2023) is a recently released evaluation benchmark, which consists of questions\nabout images across 6 disciplines with multiple subjects within each discipline that require college-\nlevel knowledge to solve these questions. Gemini Ultra achieves the best score on this benchmark\nadvancing the state-of-the-art result by more than 5 percentage points and outperforms the previous\nbest result in 5 of 6 disciplines (see Table 8), thus showcasing its multimodal reasoning capabilities.\n\n\n                             MMMU (val)                      Gemini Ultra (0-shot)         GPT-4V (0-shot)\n                                                             Maj@32         pass@1             pass@1\n                             Art & Design                    74.2           70.0               65.8\n                             Business                        62.7           56.7               59.3\n                             Science                         49.3           48.0               54.7\n                             Health & Medicine               71.3           67.3               64.7\n                             Humanities & Social Science     78.3           78.3               72.5\n                             Technology & Engineering        53.0           47.1               36.7\n                             Overall                         62.4           59.4               56.8\n\n\nTable 8 | Gemini Ultra performance on the MMMU benchmark (Yue et al., 2023) per discipline.\nEach discipline covers multiple subjects, requiring college-level knowledge and complex reasoning.\n\n\n    Gemini models are also capable of operating across modalities and a diverse set of global languages\nsimultaneously, both for image understanding tasks (e.g., images containing text in Icelandic) and for\ngeneration tasks (e.g., generating image descriptions for a wide range of languages). We evaluate the\nperformance of generating image descriptions on a selected subset of languages in the Crossmodal-\n3600 (XM-3600) benchmark in a 4-shot setting, using the Flamingo evaluation protocol (Alayrac\net al., 2022), without any fine-tuning for all models. As shown in Table 9, Gemini models achieve a\nsignificant improvement over the existing best model, Google PaLI-X.\n\n\n                 XM-3600 (CIDER)                 Gemini Ultra             Gemini Pro              Google PaLI-X\n                                                 4-shot                   4-shot                  4-shot\n                 English                         86.4                     87.1                    77.8\n                 French                          77.9                     76.7                    62.5\n                 Hindi                           31.1                     29.8                    22.2\n                 Modern Hebrew                   54.5                     52.6                    38.7\n                 Romanian                        39.0                     37.7                    30.2\n                 Thai                            86.7                     77.0                    56.0\n                 Chinese                         33.3                     30.2                    27.7\n                 Average (of 7)                  58.4                     55.9                    45.0\n\n\nTable 9 | Multilingual image understanding Gemini models outperform existing models in captioning\nimages in many languages when benchmarked on a subset of languages in XM-3600 dataset (Thapliyal\net al., 2022).\n\n\n   5MathVista is a comprehensive mathematical reasoning benchmark consisting of 28 previously published multimodal\ndatasets and three newly created datasets. Our MathVista results were obtained by running the MathVista authors\u2019\nevaluation script.\n\n\n                                                                                                                    14", "mimetype": "text/plain", "start_char_idx": 40, "end_char_idx": 4783, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "411a7d35-f8b9-4583-bda4-44b8ec65d919": {"__data__": {"id_": "411a7d35-f8b9-4583-bda4-44b8ec65d919", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "45bcd4fe-053a-47c1-9f0d-2941cc1ed1ee", "node_type": "4", "metadata": {}, "hash": "fb1658c5f133a8d8080db574b7d601b471e2bfa53405f0f3cbff9d7d9fe8294c", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n      Prompt                                                                                                                        Response\n      Write code to rearrange the subplots in the figure using the latest version\n      of matplotlib: Put the 3d paraboloid subplot on the top left: Put the                                                          The current subplots are:\n      tangent function on the bottom right. For the remaining two subplots_                                                               Topleft:         sine wave\n      one of them should stay in its original position and the other should fill                                                          Top right: _        tangent function\n                                                                                                                                          Bottom left: An exponential function\n      the last spot: First describe what each subplot depicts and identify itsThen;explain where each subplot should go in its new        Bottom right: A 30 paraboloid\n      current location:\n      location. Last write the full code for the rearranged version with the                                                         The new subplots should be:\n      originacolorscheme_                                                                                                                 Top left: The 3D paraboloid\n                                                                                                                                          Top right: The sine wave\n                                                                                                                                          Bottom left: The exponential function\n                                                                                                                                          Bottom right: The tangent function\n                                                                                                                                        import     nucoy\n                                                                                                                                        impor      matplotlibpvoiot\n                                                                                                                                        From                                     import Axs3D\n                                                                                                                                            ~rcote       figure with            Fo\"iS            Golu-ns\n                                                                                                                                                 plt.figure( figsize=(10_               10) )\n                                                                                                                                           Addfig.addsubplot 2subdlotproiecticn1eitporocololo             30'\n                                                                                                                                                   lingpcce\n                                                                                                                                                   lingpcce                180)\n                                                                                                                                                        meshorid x\n                                                                                                                                             plot    surtcce *                  ciap     vicidis\n                                                                                                                                           Add(19.addsineMove        subplon                      right\n                                                                                                                                                                            163)\n                                                                                                                                              np.sin(x)\n                                                                                                                                        ox-plot ( * ,\n                                                                                                                                                       cxncncntin]       Fmnction       subplot          the bottom lcft\n                                                                                                                                                fig,addsubplot (2 _1C3)\n       Rendered code                                                                                                                    ox plot(*,\n                                                                                                                                                       dngent     functio      subplor                 botiom      right\n                                                                                                                                                fig.add_subplot(2,1C3)linspace(8\n                                                                                                                                                   tonx)\n                                                                                                                                        ox-plot(x. Y)\n                                                                                                                                        Plt . show()\nFigure 5 | Using Gemini models\u2019 multimodal reasoning capabilities to generate matplotlib code\nfor rearranging the subplots. The multimodal prompt is shown at the top-left in gray. Gemini Ultra\u2019s\nresponse, including its generated code, is shown in the right column in blue. The bottom left figure\nshows rendered version of the generated code. Successfully solving this task shows the model\u2019s\ncapability to combine several capabilities: (1) recognition of the functions depicted in the plots; (2)\ninverse graphics to infer the code that would have generated the subplots; (3) instruction-following\nto put subplots in their desired positions; and (4) abstract reasoning to infer that the exponential plot\nmust stay in its original place, because the sine plot must move out of the way for the 3-dimensional\nplot.\n\n\n         Qualitative evaluation in Figure 5 illustrates an example of Gemini Ultra\u2019s multimodal reasoning\ncapabilities. The model is required to solve the task of generating matplotlib code that would rearrange\n\n\n                                                                                                                                                                                                                          15", "mimetype": "text/plain", "start_char_idx": 76, "end_char_idx": 7151, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1ad9d390-c05d-4e89-a0ac-bb23edf436e8": {"__data__": {"id_": "1ad9d390-c05d-4e89-a0ac-bb23edf436e8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "352cad43-0b83-4ed8-9282-ebbc11bbc4c7", "node_type": "4", "metadata": {}, "hash": "86bf9d8710ad9002b285fccd37f03fa4b7d4b9c527f0503b439b16c24627827b", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\na set of subplots provided by the user. The model output shows that it successfully solves this task\ncombining multiple capabilities of understanding the user plot, inferring the code required to generate\nit, following user instructions to put subplots in their desired positions, and abstract reasoning about\nthe output plot. This highlights Gemini Ultra\u2019s native multimodality and alludes to its more complex\nreasoning abilities across interleaved sequences of image and text. We refer the reader to the appendix\nfor more qualitative examples.\n\n\n5.2.2. Video Understanding\n\n\nUnderstanding video input is an important step towards a useful generalist agent. We measure the\nvideo understanding capability across several established benchmarks that are held-out from training.\nThese tasks measure whether the model is able to understand and reason over a temporally-related\nsequence of frames. For each video task, we sample 16 equally-spaced frames from each video clip\nand feed them to the Gemini models. For the YouTube video datasets (all datasets except NextQA\nand the Perception test), we evaluate the Gemini models on videos that were still publicly available\nin the month of November, 2023.\n     Gemini Ultra achieves state-of-the-art performance on various few-shot video captioning tasks\nas well as zero-shot video question answering tasks as shown in Table 10. This demonstrates its\ncapability of strong temporal reasoning across several frames. Figure 23 in the appendix provides a\nqualitative example of understanding the video of the ball-striking mechanics of a soccer player and\nreasoning about the player can improve their game.\n\n\n            Task                                       Gemini Ultra            Gemini Pro               Few-shot SoTA\n            VATEX (test)                               62.7                    57.4                     56.0\n            English video captioning                   4-shots                 4-shots                  DeepMind Flamingo, 4-shots\n            (Wang et al., 2019)\n\n\n            VATEX ZH (test)                            51.3                    50.0                     \u2013\n            Chinese video captioning                   4-shots                 4-shots\n            (Wang et al., 2019)\n\n\n            YouCook2 (val)                             135.4                   123.2                    74.5\n            English cooking video captioning           4-shots                 4-shots                  DeepMind Flamingo, 4-shots\n            (Zhou et al., 2018)\n\n\n            NextQA (test)                              29.9                    28.0                     26.7\n            Video question answering                   0-shot                  0-shot                   DeepMind Flamingo, 0-shot\n            (Xiao et al., 2021)\n\n\n            ActivityNet-QA (test)                      52.2                    49.8                     45.3\n            Video question answering                   0-shot                  0-shot                   Video-LLAVA, 0-shot\n            (Yu et al., 2019)\n\n\n            Perception Test MCQA (test)                54.7                    51.1                     46.3\n            Video question answering                   0-shot                  0-shot                   SeViLA (Yu et al., 2023), 0-shot\n            (P\u0103tr\u0103ucean et al., 2023)\n\n\nTable 10 | Few-shot video understanding across tasks and languages on selected academic\nbenchmarks. The reported metric is CIDER for video captioning, WUPS for NextQA, and top-1\naccuracy for the Perception Test and ActivityNet-QA. For ActivityNet-QA, we use the Video-LLAVA\n(Lin et al., 2023) evaluation protocol.\n\n\n5.2.3. Image Generation\n\n\nGemini models are able to output images natively, without having to rely on an intermediate natural\nlanguage description that can bottleneck the model\u2019s ability to express images. This uniquely enables\nthe model to generate images with prompts using interleaved sequences of image and text in a\n\n\n                                                                                                                                          16", "mimetype": "text/plain", "start_char_idx": 42, "end_char_idx": 4240, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "de2b091a-dfbf-40e1-98e6-6ef9d19c0347": {"__data__": {"id_": "de2b091a-dfbf-40e1-98e6-6ef9d19c0347", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4823fafb-2ba4-46fe-b9e7-2a15e2c105b0", "node_type": "4", "metadata": {}, "hash": "399866b510de64a6558edc888c359496a5fc6f97285ef352c880060c21088998", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\nfew-shot setting. For example, the user might prompt the model to design suggestions of images and\ntext for a blog post or a website (see Figure 12 in the appendix).\n     Figure 6 shows an example of image generation in 1-shot setting. Gemini Ultra model is prompted\nwith one example of interleaved image and text where the user provides two colors (blue and yellow)\nand image suggestions of creating a cute blue cat or a blue dog with yellow ear from yarn. The\nmodel is then given two new colors (pink and green) and asked for two ideas about what to create\nusing these colors. The model successfully generates an interleaved sequence of images and text with\nsuggestions to create a cute green avocado with pink seed or a green bunny with pink ears from yarn.\n  Prompt                                                             Response\n  Give me two ideas that         could do with these 2               Colors:    see green and pink yarn\n  colors\n                                                                     Idea 1: How about a green avocado with\n  Colors:see blue and yellow yarn                                    pink seed?\n                                                                     Idea 2: Or a green bunny with pink ears?\n  How about a cute blue cat?\n  Or a blue dog that would also have a\n yellowear?\n  Give me two ideas that        could do with\n  these 2 colors\nFigure 6 | Image Generation. Gemini models can output multiple images interleaved with text given\na prompt composed of image and text. In the left figure, Gemini Ultra is prompted in a 1-shot setting\nwith a user example of generating suggestions of creating cat and dog from yarn when given two\ncolors, blue and yellow. Then, the model is prompted to generate creative suggestions with two new\ncolors, pink and green, and it generates images of creative suggestions to make a cute green avocado\nwith pink seed or a green bunny with pink ears from yarn as shown in the right figure.\n\n\n                                                                                                               17", "mimetype": "text/plain", "start_char_idx": 38, "end_char_idx": 2183, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7f796ab5-2ae2-403c-bfdc-902eea3d8a2e": {"__data__": {"id_": "7f796ab5-2ae2-403c-bfdc-902eea3d8a2e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e9a9fce0-6155-400f-bd73-56e598877979", "node_type": "4", "metadata": {}, "hash": "ea80e5e2c95975dad63fcd6f53693ecfffb01ebbf5aad7233edf7f43142d0522", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "921e8c01-57ef-4177-b5c1-2bd74cfec5f7", "node_type": "1", "metadata": {}, "hash": "6e1d29d0746d43b24a60d6074cf538b57376b52fc8d5499ea6d387b0f30cdea7", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n5.2.4. Audio Understanding\n\n\nWe evaluate the Gemini Nano-1 and Gemini Pro models on a variety of public benchmarks and\ncompare it with Universal Speech Model (USM) (Zhang et al., 2023) and Whisper (large-v2 (Radford\net al., 2023) or large-v3 (OpenAI, 2023) as indicated). These benchmarks include automatic speech\nrecognition (ASR) tasks such as FLEURS (Conneau et al., 2023), VoxPopuli, (Wang et al., 2021),\nMulti-lingual Librispeech (Pratap et al., 2020), as well as the speech translation task CoVoST 2,\ntranslating different languages into English (Wang et al., 2020). We also report on an internal\nbenchmark YouTube test set. ASR tasks report a word error rate (WER) metric, where a lower number\nis better. Translation tasks report a BiLingual Evaluation Understudy (BLEU) score, where a higher\nnumber is better. FLEURS is reported on 62 languages that have language overlap with the training\ndata. Four segmented languages (Mandarin, Japanese, Korean and Thai) report character error rate\n(CER), instead of WER, similar to Whisper (Radford et al., 2023).\n     Table 11 indicates that our Gemini Pro model significantly outperforms the USM and Whisper\nmodels across all ASR and AST tasks, both for English and multilingual test sets. Note that there is a\nlarge gain in FLEURS, compared to USM and Whisper, as our model is also trained with the FLEURS\ntraining dataset. However, training the same model without FLEURS dataset results in a WER of 15.8,\nwhich still outperforms Whisper. Gemini Nano-1 model also outperforms both USM and Whisper on\nall datasets except FLEURS. Note that we did not evaluate Gemini Ultra on audio yet, though we\nexpect better performance from increased model scale.\n\n\n                                     Task                     Metric        Gemini      Gemini      Whisper2023;     USM et al.,\n                                                                            Pro         Nano-1      (OpenAI,        (Zhang\n                                                                                                    Radford et al.,  2023)\n                                                                                                    2023)\n            Automatic Speech         (en-us)YouTube           WER (\u2193)       4.9%        5.5%        6.5%\n                                                                                                    (v3)             6.2%\n            Recognition\n                                     Multilingual             WER (\u2193)       4.8%        5.9%        6.2%             7.0 %\n                                     Librispeech                                                    (v2)\n                                     (en-us)\n                                     (Pratap et al., 2020)\n                                     FLEURS                   WER (\u2193)       7.6%        14.2%       17.6%            11.8%\n                                     (62 lang)                                                      (v3)\n                                     (Conneau et al., 2023)\n                                     VoxPopuli                WER (\u2193)       9.1%        9.5%        15.9%            13.4%\n                                     (14 lang)                                                      (v2)\n                                     (Wang et al., 2021)\u25b2\u25b2\n            Automatic Speech         CoVoST 2                 BLEU (\u2191)      40.1        35.4        29.1             30.7\n            Translation              (21 lang)                                                      (v2)\n                                     (Wang et al., 2020)\n\n\nTable 11 | Speech evaluation results on selected benchmarks for ASR and AST. For ASR, the reported\nmetric is WER where lower is better. For AST, the reported metric is BLEU where higher is better.\n     Table 12 shows further error analysis with USM and Gemini Pro. We find that Gemini Pro produces\nmore understandable responses, particularly on rare words and proper nouns.", "mimetype": "text/plain", "start_char_idx": 42, "end_char_idx": 4096, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "921e8c01-57ef-4177-b5c1-2bd74cfec5f7": {"__data__": {"id_": "921e8c01-57ef-4177-b5c1-2bd74cfec5f7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e9a9fce0-6155-400f-bd73-56e598877979", "node_type": "4", "metadata": {}, "hash": "ea80e5e2c95975dad63fcd6f53693ecfffb01ebbf5aad7233edf7f43142d0522", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7f796ab5-2ae2-403c-bfdc-902eea3d8a2e", "node_type": "1", "metadata": {}, "hash": "687ff8383fee8b6ff2602fe27690fe4f55d30fe05977457e1ea96ab8b089450e", "class_name": "RelatedNodeInfo"}}, "text": "Table 11 | Speech evaluation results on selected benchmarks for ASR and AST. For ASR, the reported\nmetric is WER where lower is better. For AST, the reported metric is BLEU where higher is better.\n     Table 12 shows further error analysis with USM and Gemini Pro. We find that Gemini Pro produces\nmore understandable responses, particularly on rare words and proper nouns.\n\n\n  Domain Truth                                       USM                                      Gemini Pro                             Wav\n  Fleurs    Scotturb bus 403 travels regularly to    Scotboard bus four3 traversed regu-      Scotturb bus 403 travels regularly to   )))\n            Sintra, stopping at Cabo da Roca.        larly to Centra stopping at Cabo de      Sintra, stopping at Cabo da Roca.\n                                                     Roga.\n  Fleurs    The archipelago lies 120 km north        The archipelago lines 120 km north       The archipelago lies 120 km north       )))\n            of the Peninsula. The largest is King    of peninsula. The largest is Kingurch    of the Peninsula. The largest is King\n            George Island, with the settlement of    island with the settlement of Cua        George Island, with the settlement of\n            Villa Las Estrellas.                     Losas.                                   Villa Las Estrellas.\n\n\nTable 12 | Qualitative examples for the ASR task in the benchmark. Incorrect transcriptions are\nhighlighted in red.\n\n\n                                                                                                                                        18", "mimetype": "text/plain", "start_char_idx": 3723, "end_char_idx": 5336, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "098960e3-7be1-49b3-bbdc-d35fc235dab6": {"__data__": {"id_": "098960e3-7be1-49b3-bbdc-d35fc235dab6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3f0f4da4-2713-4e37-87ed-e33e0e4833e1", "node_type": "4", "metadata": {}, "hash": "b8b3aa4098cd46785c349bb3602c00f41c29379c631853de692b40ffc8cd65e9", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n5.2.5. Modality Combination\n\n\nMultimodal demonstrations often include a combination of text interleaved with a single modality,\nusually images. We demonstrate the ability to process a sequence of audio and images natively.\n      Consider a cooking scenario about making an omelet where we prompt the model with a sequence\nof audio and images. Table 13 indicates a turn-by-turn interaction with the model, providing pictures\nand verbally asking questions about the next steps for cooking an omelet. We note that the model\u25b2\u25b2\u25b2\u25b2response text is reasonably accurate, and shows that model processes fine-grained image details to\nevaluate when the omelet is fully cooked. See demo on the website.\n\n\n  Input Image                       Input Audio (transcribed)                               Model Response: Text\n\n\n                                    with these ingredients?)) What\u2019s the first step to make a veggie omelet\n                                      )                                                     Crack the eggs into a bowl and whisk them.\n\n\n                                    ing my omelet, does it look ready now?)) Thank you for the instructions. I started mak-\n                                      )                                                     It looks like it\u2019s almost ready. You can flip it over\n                                                                                            to cook the other side.\n\n\n  (No image - it\u2019s a follow up\n  on the previous question)           )) Why is it not ready?\n                                      )                                                     It\u2019s not ready because the eggs are still runny.\n\n\n                                      )) What about now?\n                                      )                                                     It looks ready now. You can take it off the heat and\n                                                                                            serve it.\n\n\nTable 13 | Audio-visual qualitative example showcasing the ability of Gemini models to process\ninterleaved sequences of text, vision, and audio, as well as reason across modalities. This example\ninputs interleaved images and audio from the user in a cooking scenario. The user prompts the model\nfor instructions to make an omelet and to inspect whether it is fully cooked.\n\n\n                                                                                                                                             19", "mimetype": "text/plain", "start_char_idx": 42, "end_char_idx": 2597, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a138febe-7833-4497-ac14-8f4c57ddcb39": {"__data__": {"id_": "a138febe-7833-4497-ac14-8f4c57ddcb39", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "106c69f6-ae52-4334-8ab9-25ad00a16dc2", "node_type": "4", "metadata": {}, "hash": "522ce5882d3316b8abefa9a7d099592a58af1a9e69f5a1648b7be5970f14ce6e", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n 6. Post-Training Models\n\n\n After large-scale pre-training, we apply post-training, where one trains on top of a pre-trained model\n in order to extend the model\u2019s proficiency and to enable a wide variety of capabilities. Namely, we\n seek to improve overall quality, enhance target capabilities such as coding and multilingual, and\n ensure alignment and safety criteria are met. We discuss our approach to post-training in this section,\n highlighting common and distinct aspects of the Gemini Apps and Gemini API model variants.\n\n\n 6.1. Gemini Apps: Gemini and Gemini Advanced\n\n\nGemini and Gemini Advanced offer direct access to Google\u2019s family of AI models, consisting of the core\n post-trained Gemini Apps models and the system around it. These models are created by applying\n specialized post-training on top of Gemini pre-trained models: currently, Gemini gives access to Pro 1.0\n and Gemini Advanced gives access to Ultra 1.0. Beyond the core models, the system determines how\n the models interact with external tools (such as Google Flights, Maps, and Google Workspace), and\n how to generate responses (filtering, ranking, and streaming). As an area, conversational AI presents\n several challenges, including: How to understand users\u2019 requests across multi-turn interactions? How\n to make sure responses are safe, factually grounded, and helpful? How to help users accomplish tasks\n by using tools external to the models? We discuss how we approach these challenges in the following\n sections.\n\n\n 6.2. Gemini APIs: Google AI Studio and Cloud Vertex AI\n\n\nOur developer-focused Gemini API models are designed to support both conversational and non-\n conversational use cases. These models are available through Google AI Studio and Cloud Vertex\n AI through an easy to use API. Google AI Studio is a free, web-based developer tool to prototype\n and launch apps quickly with an API key. Vertex AI is a comprehensive AI platform that enables\n developers to leverage Gemini API models with varied tooling, fully-managed infrastructure, and\n built-in enterprise security and privacy settings. Gemini APIs make it easy to integrate Gemini API\n models into any production product or workflow, empowering developers to build applications that\n can reason across different modalities.\n\n\n 6.3. Post-Training Methods & Data\n\n\n Post-training Gemini models to produce Gemini API and Apps variants involves several stages; see\n Figure 7. Careful data curation is critical for all stages. First, we collect a diverse set of prompts\n that are representative of real-world use cases. Second, we apply supervised fine-tuning (SFT) on\n demonstration data of what the model\u2019s output should be for a given prompt (Mishra et al., 2021;\nOuyang et al., 2022; Wei et al., 2022a). Third, we further collect different possible responses to a\n given prompt, and collect feedback data over these to train a Reward Model (RM). Finally, using the\n trained RM, a Reinforcement Learning from Human Feedback (RLHF) stage (Bai et al., 2022a) is\n applied to further align the model\u2019s outputs with human preferences. We discuss our methods in\n more detail below:\n     (1) Prompt Data Collection: A prompt is a user\u2019s input to the model. As well as the most recent\n user input, this can also include previous user-model interactions. We curate datasets of target\n prompts. The datasets serve as the basis for our demonstration and feedback data collections, and\n they are used directly during reinforcement learning. It is important to cover a diverse set of crucial\n use cases and in both single-turn and multi-turn formats. Data sources include vendor-created data,\n third-party licensed sources, and synthetic approaches.\n\n\n                                                                                                           20", "mimetype": "text/plain", "start_char_idx": 39, "end_char_idx": 3895, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b1aae730-5f59-44fe-a6c9-f82352815b86": {"__data__": {"id_": "b1aae730-5f59-44fe-a6c9-f82352815b86", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9f2545d5-3381-4c78-a106-98dd040c1486", "node_type": "4", "metadata": {}, "hash": "422bc0fe20043538a140b23785472b09c864802008544b9b45c1406ee03a7f0d", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n     (2) SFT on Demonstration Data: SFT trains the model to output a desired target response given\na prompt. Our Demonstration Data target responses can be directly written by a human expert, or\ngenerated by a model and in some cases revised or reviewed by a human. Additionally, we use data\nanalysis tools and heuristics to ensure high data diversity across capabilities, use cases, and semantic\nclusters.\n     (3) RM Training on Feedback Data: We further collect Feedback Data, for which human raters\nprovide feedback such as relative preferences over candidate responses and feedback regarding\nindividual responses to a given prompt. For many capabilities, rating relative preferences is an easier\ntask than demonstrating an ideal response. Feedback data are collected across creativity, safety,\nfactuality, other capabilities, and other target criteria. We found that the utility of the resulting\nhuman feedback data greatly depends on the prompt selection and the sampling strategy used to\nproduce candidate responses. We use this data to train RMs to output rewards that align with human\npreferences as closely as possible.\n     (4) RLHF: Applying reinforcement learning from human feedback (RLHF) to our models provides\nfurther gains over SFT alone. Our approach creates an iterative process in which RL continually\npushes the boundaries of the RM, while the RM is continuously improved through evaluation and\ndata collection, leading to progressive improvements in both.\n\n\n                          Gemini\u2028                 stream_control\n                         pre-training             SFT                   RLHF\n                                                                                                       personEnd\n                                                             Data\u2028                                     users\n                                                            flywheel\n\n\n                                              Demonstration\u2028database   Feedback\u2028database\n                                                  data                   data\n\n\nFigure 7 | Modeling overview. Post-training utilizes an optimized data flywheel in order to acquire\nhuman-AI feedback and continually improve on key areas. The data mixtures for supervised fine-\ntuning, reward modeling, and reinforcement learning serve as the foundation for our models.\n\n\n6.4. Evaluation\n\n\nEvaluation of human preferences over model outputs provides critical signals for measuring perfor-\nmance. As part of our development process, we conduct human evaluation extensively across targeted\ncapabilities. Human evaluation is instantiated as side-by-side blind evaluations where human raters\njudge responses of two models to the same prompt, as single-response ratings for certain capabilities,\nand as online testing. In addition, we build models for automated evaluation that faithfully imitate\nhuman preferences in order to guide development and continuously monitor online performance.\n\n\n6.5. Model Capabilities\n\n\nBeyond the general post-training outlined above, we apply techniques to improve a set of key capabili-\nties. These capabilities cover a range of use cases inspired by current user needs and research-inspired\n\n\n                                                                                                                  21", "mimetype": "text/plain", "start_char_idx": 39, "end_char_idx": 3435, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "25ccd597-5eda-4b02-924e-64849c904608": {"__data__": {"id_": "25ccd597-5eda-4b02-924e-64849c904608", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b327cfdb-6b4a-49e0-a2ae-4ffd81223440", "node_type": "4", "metadata": {}, "hash": "f06cfb5869aed727b850ca24bef6dc66157a7cf446d1c5e1df71ac193439e6d8", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\nfuture applications. We outline capability examples not detailed in previous sections below. The post-\ntraining recipes are carefully designed to balance multiple objectives, including creativity, factuality,\nsafety and more (Bai et al., 2022b; Thoppilan et al., 2022). We have a particular focus on safety and\nalignment, and hence address this in a further dedicated section.\n\n\n6.5.1. Instruction Following\n\n\nFollowing a user\u2019s prompt accurately is a fundamental capability for LLMs, especially as these models\nbecome more sophisticated and are presented with increasingly complex user prompts. User prompts\nvary in granularity, specificity, and requirements (e.g., content, format, length). Individual instructions\ncan also be ambiguous, optional, or even impossible or undesirable to satisfy (He et al., 2023; Xu\net al., 2023).\n     We improve Gemini Apps and Gemini API models\u2019 instruction following (IF) abilities by collecting\ndata for a diverse set of instruction following categories. For instructions that are verifiable program-\nmatically such as word count, we generate synthetic data via prompting and response editing to\nensure that such instructions are satisfied.\n     Complex prompts evaluation: We investigate performance on complex prompts containing\nmultiple instructions using a fine-grained evaluation method that assesses how well models adhere to\neach instruction. Human raters are presented with a prompt-response pair and a list of the individual\n(sub)-instructions contained in the prompt. Each prompt may have anywhere from one to dozens of\nindividual instructions, and the annotators are tasked with determining whether each instruction is\nfollowed (or not) by the response.\n     Table 14 reports results on an internal dataset of prompts with instructions of varying complexity\nthat encompass a wide range of instructions and are designed to be challenging for LLMs. We report\ntwo metrics: per-instruction accuracy (the percentage of sub instructions in the eval set that are\nfollowed), and full-response accuracy (the percentage of eval set prompts where all sub-instructions\nare followed).\n\n\n                                           Post-trained PaLM 2     Gemini (with Pro)    Gemini Advanced (with Ultra)\n                Per-instruction accuracy   59.5\u00b13.0%               77.8\u00b12.0%            87.4\u00b11.4%\n                Full-response accuracy     25.5\u00b13.3%               38.5\u00b13.6%            54.1\u00b13.7%\n\n\nTable 14 | Performance of Gemini on our complex prompts instruction-following internal benchmark.\n\n\n     Gemini Advanced (with Ultra) achieves an average per-instruction accuracy close to 90%, rep-\nresenting a significant improvement over Gemini (with Pro) and a post-trained PaLM 2 model. We\nfind that the sub-instructions that aren\u2019t followed are well-distributed across responses. As a result\nGemini Advanced\u2019s full-response accuracy is lower, at around 54%. This indicates that there is further\nheadroom for models to fully satisfy all instructions.\n\n\n6.5.2. Tool Use\n\n\nBy training LLMs to use tools, we greatly expand LLM capabilities beyond their internal knowledge. We\ntreat tool use for both Gemini Apps and Gemini API models as a code generation problem, leveraging\nthe base model\u2019s preexisting strong coding capabilities. Every tool invocation is represented as a code\nblock in which tool calls are invoked. This process allows the model to both compose multiple tools\nin each code block, as well as observe and react to the results of tool execution. At inference time,\nto generate a response to a user prompt, our system executes the loop shown in Figure 8, where\nsampling from the LLM and execution of tool code work together to create a final response.\n\n\n                                                                                                                      22", "mimetype": "text/plain", "start_char_idx": 39, "end_char_idx": 3921, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c68c958f-98ad-4966-ab8c-93254ade2348": {"__data__": {"id_": "c68c958f-98ad-4966-ab8c-93254ade2348", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3dd12f4e-ece9-4854-b69b-466626102bc8", "node_type": "4", "metadata": {}, "hash": "718bc33943b4903102786c19b522d8004aa8d2c4833358641fd72900e677cf51", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\nFigure 8 | A Gemini tool-use control loop.\n\n\n      Gemini Apps models: Gemini draws on a range of tools via Gemini Extensions, including Google\nWorkspace, Google Maps, YouTube, Google Flights, and Google Hotels. These tool-use capabilities\nalso enable Gemini to be integrated as part of Gmail, Docs, Slides, Sheets and more. We are aiming\nto bring further tool-use capabilities in order to both enhance Gemini models and integrate Gemini\nmodels into further products.\n      We created an internal benchmark to assess Gemini performance on tasks that may benefit from\naccess to these extensions. This benchmark measures human preference in domains such as travel\nplanning and video discovery. We find models equipped with tools are preferred on this set 78% of\nthe time over models without tools (excluding ties).\n      Gemini API models: We have found that fine-tuning Gemini API models is very effective at\nteaching the model tool-use behaviors. Furthermore, training models to use programming and search\nas tools leads to improved performance on a range of academic benchmarks. In Table 15, we compare\ntool-use models fine-tuned from an early version of Gemini API Pro against equivalent models that do\nnot use tools.\n\n\n                                                          Mathematical Reasoning                  Factuality & Knowledge\n                                                                                                  Retrieval\n                                                          GSM8K               MATH                NQ                   Realtime QA\n                                                          Cobbe et al. (2021) Hendrycks et al.    Kwiatkowski et al.   Kasai et al. (2022a)\n                                                                              (2021b)             (2019b)\n\n\n                               Gemini API Pro             80.1%               41.8%               68.0%                70.8%\n                               with tools\n                               Gemini API Pro             69.7%               30.7%               59.0%                39.2%\n                               without tools\n\n\nTable 15 | Comparison between Gemini API tool-use models and comparable models that do not use\ntools. Gemini API Pro without tools is an early version of our Pro model trained without tool-use data.\nGemini API Pro with tools is the same model fine-tuned with tool-use data.\n\n\n6.5.3. Multilinguality\n\n\nMultilinguality is critical to make sure Gemini models effectively support a wide range of languages.\nWe discuss our key approaches for Gemini Apps and Gemini API models respectively below.\n      Gemini Apps models: Scaling Gemini from English to 40+ languages imposed research challenges\nin data quality. We leverage abundant high-quality English data by localization to native cultures\n(e.g., \u201cpresident of the United States\u201d -> \u201c \u65e5\u672c\u306e\u9996\u76f8\u201d).\n      Table 16 shows the performance of Gemini (with Pro) on 5 languages compared to Bard with\n\n\n                                                                                                                                             23", "mimetype": "text/plain", "start_char_idx": 49, "end_char_idx": 3256, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "29184fc1-6eaf-4c2f-90ba-9e367d9ad59c": {"__data__": {"id_": "29184fc1-6eaf-4c2f-90ba-9e367d9ad59c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "62099bd2-c144-412d-8323-bb904f56599a", "node_type": "4", "metadata": {}, "hash": "1c52f09bbf1560a1679489b1b2ca86c9b233fa2af720d381939d0171f549802c", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\nan older post-training recipe and based on PaLM 2. For side-by-side comparisons between a model\nA and a model B, we calculate a metric called SxS score. Each rating is converted to an ordinal\nvalue centered at 0: ratings preferring A are positive and ratings preferring B are negative over a\nscale between -1.5 and 1.5. The converted values are averaged to return the SxS score. Intuitively, a\npositive SxS score indicates the extent to which model A is preferred over model B. Here, we find\nquality improved by more than 0.1 SxS score for all five languages. Coding and reasoning gains from\nGemini Pro are preserved across languages.\n\n\n                             Language          Quality            Coding              Reasoning\n                                               SxS                MBPP Pass@1         MMLU\n                                                                  Austin et al. (2021)Hendrycks et al.\n                                                                                      (2021a)\n                             ja-JP             +0.14              +22.2%              +3.6%\n                             pt-BR             +0.17              +23.2%              +5.2%\n                             de-DE             +0.1               +21.4%              +7.5%\n                             es-419            +0.12              +22.8%              +9.3%\n                             it-IT             +0.13              +13.8%              +7.5%\n\n\nTable 16 | Multilingual performance of Gemini (with Pro) compared to Gemini with an older post-\ntraining recipe and PaLM 2.\n\n\n     Gemini API models: Similar to Gemini Apps models, we train Gemini API models on additional\nmultilingual post-training data, effectively adapting the original English model for use in various\nlanguages. We experiment with both human-generated non-English prompt-response pairs as well as\nautomatically translated pairs. For the latter, we leverage abundant high-quality English demonstration\ndata by translation. We ensure the quality of such translated data by translationability filtering and\nresponse rating by humans.\n     Translatability Filtering: Not all prompt-response pairs make sense when automatically translated,\nand may require expensive localization instead. Example prompts of this type (responses omitted for\nspace) include:\n\n\n    \u2022 (strict word requirements) Write a 1000 word essay about world peace.\n    \u2022 (too English centric) Write a poem in iambic pentameter about apples.\n    \u2022 (too Latin-script centric) What is a word with 1 E, 2 As, and 1 U?\n\n\n     Translation Quality Validation: Each translated prompt-response pair was rated for translation\nquality by at least 3 human raters, and was kept in the final mixture if the majority of raters rated it\nas accurate. Section 5.1.4 reports evaluations of the multilingual capabilities of post-trained Gemini\nAPI models.\n\n\n6.5.4. Multimodal Vision\n\n\nMultimodal post-training enhances the capabilities of our natively multimodal Gemini models for a\nwide range of useful applications. In the following, we discuss how image understanding ability is\nincorporated into Gemini Apps and Gemini API models. For this evaluation, we further train both\nof these Gemini model variants on a mixture of text data and expert curated image-text data over\nseveral vertically-defined multimodal use cases\n     Gemini Apps models: We empower Gemini and Gemini Advanced with image understanding\ncapabilities by fine-tuning pre-trained Gemini models on a mixture of text-only and image-text\ndata. Careful balancing of text and multimodal data ensures the model develops robust image\nunderstanding without adversely affecting the quality of the text-only interactions. To assess our\n\n\n                                                                                                          24", "mimetype": "text/plain", "start_char_idx": 38, "end_char_idx": 3948, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "03a2f46a-2e46-457b-9457-a3b295989524": {"__data__": {"id_": "03a2f46a-2e46-457b-9457-a3b295989524", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a4081485-ced5-4b70-b7e7-0675100ed292", "node_type": "4", "metadata": {}, "hash": "30e8dc76fc56bfa8d3dddfbb0869103f544a0a56555f3862392a987fb2306c5c", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\nmodels, we compile a dataset of human-curated and synthetic image-text prompts and responses,\nspanning various categories and difficulty levels. This dataset facilitates human evaluation for model\ncomparison and selection.\n     We find that introducing this image-text data preserves Gemini Apps model quality on text-only\ntasks, with a SxS score on text-only tasks of +0.01\u00b10.01 for a Gemini Apps Pro model trained\non this data versus an equivalent model trained only on text data. In addition, post-training via\nRLHF improves performance on multimodal tasks, with a SxS score on image-understanding tasks of\n+0.223\u00b10.06 for a Gemini Apps Pro model post-trained with SFT & RLHF vs SFT alone.\n     Gemini API models: We evaluate the impact of post-training via SFT on Gemini API models\u2019\nmultimodal vision performance by tracking the performance of both pre-trained models and post-\ntrained Gemini API Vision models on a series of standard benchmarks. These post-trained results have\nalready been given in Table 7, in Table 17 we further report the difference in performance between\npre-trained and post-trained Gemini API models.\n\n\n                                                         Gemini Ultra         Gemini API Ultra    Gemini Ultra\n                                                         Pre-trained only     0-shot              pre- to post-trained\n                                                         0-shot               (pixel only)\n                                                         (pixel only)                             improvement\n                        MMMU (val)                       n/a                  59.4%               n/a\n                        Multi-discipline college-level problems               pass@1\n                        (Yue et al., 2023)\n                                                                              62.4%32\n                                                                              Maj1@\n                        TextVQA (val)                    81.4%                82.3%               +0.9%\n                        Text reading on natural images\n                        (Singh et al., 2019)\n                        DocVQA (test)                    90.1%                90.9%               +0.8%\n                        Document understanding\n                        (Mathew et al., 2021)\n\n\n                        ChartQA (test)                   80.8%                80.8%               0.0%\n                        Chart understanding\n                        (Masry et al., 2022)\n                        InfographicVQA (test)            77.9%                80.3%               +2.4%\n                        Infographic understanding\n                        (Mathew et al., 2022)\n                        MathVista (testmini)             n/a                  53.0%               n/a\n                        Mathematical reasoning\n                        (Lu et al., 2023)\n                        AI2D (test)                      76.6%                79.5%               +2.9%\n                        Science diagrams\n                        (Kembhavi et al., 2016)\n                        VQAv2 (test-dev)                 74.5%                77.8%               +3.3%\n                        Natural image understanding\n                        (Goyal et al., 2017)\n\n\nTable 17 | Post-trained model image understanding Post-training improves image understanding\ncapabilities of Gemini API Ultra over the base pre-trained model. Comparisons of Gemini API Ultra to\nother models on these benchmarks are given in Table 7.\n\n\n     The results indicate that the pre-trained model already has high performance across the capabilities\nrepresented by these benchmarks, in line with previous observations. However, the post-training SFT\nstage used for the Gemini API Vision models succeeds in improving the performance over several\nof these benchmarks (InfographicVQA, AI2D, VQAv2), most likely due to the model\u2019s increased\ninstruction-following capabilities that succeed in aligning the model output style with that of the\ngolden references.\n\n\n                                                                                                                        25", "mimetype": "text/plain", "start_char_idx": 42, "end_char_idx": 4329, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "04c88911-8078-4b90-94bd-bf35ae642d1a": {"__data__": {"id_": "04c88911-8078-4b90-94bd-bf35ae642d1a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6125c96c-8808-408a-a67f-9a7752c3e1ef", "node_type": "4", "metadata": {}, "hash": "3bbda711a33c8e9531419dd909d3b7ec9a6587702c302f2709d88f8c1b664608", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n6.5.5. Coding\n\n\nDespite the strong coding benchmark performance of the base model, post-training data still provides\na significant boost to both code quality and code correctness. This highlights the benefit of high-quality\ndemonstration data and feedback data for coding use cases. Gemini Apps and Gemini API models use\na combination of human and synthetic approaches to collect such data.\n     We evaluate our Gemini Apps models\u2019 coding performance on a set of internally curated prompts,\ndistributed across code use cases and languages. Table 18 reports SxS scores, where Gemini (with\nPro) significantly improves upon Bard with an older post-training recipe and based on PaLM 2. Gemini\nAdvanced (with Ultra) further improves upon Gemini (with Pro).\n\n\n                         Side A                             Side B                             SxS score\n                         Gemini (with Pro)                  Bard (PaLM 2, Sept. 2023)          0.19\u00b10.03\n                         Gemini Advanced (with Ultra)       Gemini (with Pro)                  0.13\u00b1 0.02\n\n\nTable 18 | SxS comparisons of Gemini models on an internal coding benchmark.\n\n\n     For the coding capabilities of post-trained Gemini API Models, see Table 2 which reports their\nacademic benchmark performance.\n\n\n7. Responsible Deployment\n\n\nDuring the development of Gemini models, we follow a structured approach to responsible deployment\nto identify, measure, and manage foreseeable downstream societal impacts of our models, in line\nwith previous releases of Google\u2019s AI technology (Kavukcuoglu et al., 2022). Throughout the lifecycle\nof a project, we follow the structure below. This section provides more detail about our approach and\nincludes key findings where available. We are committed to ongoing transparency and will continue\nto provide updated information on our approach and testing in upcoming reports.\n\n\n7.1. Impact Assessment\n\n\nAt Google we apply an impact assessment framework throughout the product development lifecycle\nrelated to Google\u2019s AI Principles (Google, 2023). This means we assess the risk and impact of AI\nmodels we\u2019re building at both a model-level (e.g. for Gemini API Ultra 1.0, as deployed on Cloud\n\n\n                                                                                                            26", "mimetype": "text/plain", "start_char_idx": 38, "end_char_idx": 2411, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "54432497-33dd-4873-a315-75aed0c97d13": {"__data__": {"id_": "54432497-33dd-4873-a315-75aed0c97d13", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d9f478bb-b5b1-4e8b-96f4-d20d35c0b236", "node_type": "4", "metadata": {}, "hash": "76037a8ac33593d1909ed183e43ab5650418691e51fdda44360e866919e4680a", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n Studio or Vertex AI), and once embedded within a broader product or service (e.g. for Gemini\n Advanced).\n\n\n 7.1.1. Model Assessment\n\n\nWe conduct model impact assessments to identify, assess, and document societal benefits and harms\n associated with the capabilities of Gemini models. Our impact assessments for Gemini API models\n describe downstream benefits and risks that we identify, spanning across the models\u2019 modalities\n(text-to-text; image-to-text; and video-to-text). Model impact assessments are conducted by the\nGoogle DeepMind Responsible Development and Innovation team, and are reviewed by the Google\n DeepMind Responsibility and Safety Council. We draw from various sources in producing impact\n assessments, including a wide range of literature, external expertise, and our in-house ethics and\n safety research.\n     Gemini models introduce various benefits to people and society. Gemini models\u2019 various modalities,\n including language, image and video understanding, can help users process information more\n efficiently, for example through content summarisation. These efficiency benefits can apply to\n commercial entities, and can assist use cases dependent on text, image or video processing such as\n video captioning, analytics or product descriptions. Video and image understanding modalities can\n also be deployed for social good applications downstream, such as enabling descriptions of visual\n outputs for accessibility purposes. Generative multimodal models may also raise downstream societal\n risks, with the Gemini models assessments considering a range of risks previously identified within\n research such as Weidinger et al. (2021) and Shelby et al. (2023). We assessed a range of content\n risks such as exposure of users to potentially unsafe content, such as sexually explicit, violent or\n hateful outputs (Weidinger et al., 2021), child safety harms, and representation harms, subsequently\n designing evaluations across these domains to enable measurement. Beyond content related risks,\nwe analyzed the potential misuse of capabilities for surveillance applications, particularly for media-\n to-text capabilities, and considered the broader environmental and economic impact of multimodal\n models. We are continuously conducting research into emerging risks of advanced models, including\n for dangerous capabilities (e.g. cyber security threats) which form a part of our evaluation approach\n(Section 7.4).\n\n\n 7.1.2. Product Assessments\n\n\n Beyond the assessment conducted at the model-level, additional risk assessments are conducted on\n the products by the Google AI Principles team prior to launch (e.g. on the Gemini Advanced product).\n These risk and impact assessments, alongside both model- and product-level assurance evaluations,\n are used to guide mitigation and product delivery efforts, and inform deployment decisions.\n     For Gemini Advanced, we conducted extensive deep-dive red teaming via dogfooding and adver-\n sarial testing in the areas of safety, accountability, and inclusion to prepare for the initial experimental\n rollout of Gemini and subsequent updates. Further cross-functional work helps to ensure appropri-\n ate mitigations were adopted before Gemini and its new capabilities or offerings, such as Gemini\n Advanced, launched. Beyond content safety, these product mitigations included the following:\n\n\n     \u2022 Clear and relevant explanations to set appropriate expectations that describe Gemini as a way to\n        get direct access to Google AI for a wide range of tasks, including complex tasks. Explanations\n        make clear that this AI-powered system is useful for all sorts of tasks \u2014 like preparing for a job\n        interview, debugging code for the first time or writing a pithy social media caption.\n     \u2022 Disclosures in the Gemini Apps Privacy Notice stating that people should not rely on Gemini\u2019s\n        responses as medical, legal, financial or other professional advice.\n\n\n                                                                                                               27", "mimetype": "text/plain", "start_char_idx": 39, "end_char_idx": 4158, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f7b54486-7719-430d-b4c9-e24e77962385": {"__data__": {"id_": "f7b54486-7719-430d-b4c9-e24e77962385", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dba508a2-037d-4361-928c-bfdc52236b9a", "node_type": "4", "metadata": {}, "hash": "5209d9f1dd37d1c33e279732d3d76818523841d558d3ed1d2d6a4292af93294b", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n    \u2022 Disclosure in product stating that Gemini\u2019s responses should be double-checked for information\n       accuracy.\n    \u2022 Feedback channels and operational support were defined and built to help ensure appropriate\n       response to user feedback to improve the model and address issues.\n\n\n     For the Gemini API Ultra model, that will be available through Google AI Studio and Cloud Vertex\nAI, product review outcomes resulted in additional safety evaluations on enterprise-specific data across\nmodalities, and additional product-level mitigations to promote safe and responsible use including:\n\n\n    \u2022 Safety filters with Cloud established thresholds as the default product behavior.\n    \u2022 Developer enablement information embedded within product documentation to support respon-\n       sible use.\n    \u2022 Feedback channels which are a component of the Vertex user interface to give feedback directly\n       during use to address issues and undesirable outputs.\n\n\n     We are increasingly integrating our AI review work into our holistic enterprise risk management\nframeworks for assuring the quality of our offerings. This evolution helps us further the scale of our\nwork and integration into existing governance and company-wide infrastructure and accountability\nprocesses. In close coordination with central AI Principles review teams, some of our product areas,\nincluding Google Cloud, have developed their own specialized review processes, deploying approaches\ntailored to their unique circumstances.\n\n\n7.2. Safety Policies\n\n\nWe have developed a set of model safety policies for Gemini models to steer development and\nevaluation. The model policy definitions act as a standardized criteria and prioritization schema\nfor responsible development and define the categories against which we measure launch readiness.\nGoogle products that use Gemini models, like our conversational AI service Gemini and Cloud Vertex\nAPI, further implement our standard product policy framework which is based on Google\u2019s extensive\nexperience with harm mitigation and rigorous research. These policies take product use cases into\naccount \u2013 for example, providing additional safety coverage for users under 18.\n     Our model safety policies reflect our established approach towards product safety and preventing\nharm in consumer and enterprise contexts. Policy areas include generation of child sexual abuse\nand exploitation content, hate speech, harassment, dangerous content such as guidance on how\nto make weapons, and malicious content. We also aim to reduce bias in our models via guidelines\nfocused on providing content that reflects our global user base. In addition, we have guidelines that\nprioritize providing neutral answers grounded in authoritative, consensus facts, or providing multiple\nperspectives where consensus doesn\u2019t exist.\n\n\n7.3. Mitigations\n\n\n7.3.1. Data Curation Practices\n\n\nPrior to all training stages, we take various steps to mitigate potential downstream harms through\ndata curation and careful data collection. We filter training data for high-risk content and to ensure\ntraining data is sufficiently high quality.\n     Humans also play an essential role, both for data creation and evaluation, in the post-training\nprocess. For certain data creation and evaluation initiatives, we consider diversity across gender\n\n\n                                                                                                          28", "mimetype": "text/plain", "start_char_idx": 38, "end_char_idx": 3535, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "84981b46-73d9-4513-8fa2-64bce783822b": {"__data__": {"id_": "84981b46-73d9-4513-8fa2-64bce783822b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "09eff6ca-526b-42bc-b459-45c4319a25b2", "node_type": "4", "metadata": {}, "hash": "2c4e1f62090beb9648a1658ef39d9369a10c72526339c006a8a379421ba28612", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\npresentation, age, and racial and ethnic diversity. We also take steps to ensure all data collected\nmeets Google DeepMind\u2019s best practices on data enrichment, developed based on the Partnership on\nAI\u2019s Responsible Sourcing of Data Enrichment Services. To support this, our agreements with vendors\ninclude a contractual obligation that data enrichment workers are paid at least local living wage.\n\n\n7.3.2. Model Mitigation\n\n\nOur modeling mitigation of safety risks, applied across Gemini Advanced and Gemini API Ultra\nmodels, is mostly through post-training (Section 6), encompassing supervised fine-tuning (SFT) and\nreinforcement learning through human feedback (RLHF) using a reward model (Bai et al., 2022a).\nIn contrast to generic quality-oriented post-training catering to all types of user queries, our safety\nmitigation is more focused on adversarial, or \u201charm-inducing\u201dqueries - i.e. the smaller slice of user\nqueries where an unprotected model is likely to produce harmful responses according to our model\nsafety policies.\n\n\n7.3.2.1      Harm-inducing queries\n\n\nTo ensure broad coverage of harm-inducing queries, we enumerate approximately 20 harm types (e.g.\nhate speech, providing ungrounded medical advice, suggesting dangerous behavior) across a wide\nvariety of use cases, according to our model safety policies described above. We generate a dataset of\npotential harm-inducing queries in these categories, using a combination of approaches:\n\n\n    \u2022 Policy experts and engineers crafting queries based on observed model failures.\n    \u2022 Prompting high-capability language models to generate queries, using policy-based instructions\n       and seed keywords (e.g. policy \u201chate speech\u201d with words describing a specific demographic).\n    \u2022 Finding queries that trigger policy violation responses, via automated Red Teaming in model\n       evaluations.\n\n\n7.3.2.2      Supervised fine-tuning\n\n\nGiven the above harm-inducing queries, we create SFT data to demonstrate the safe and helpful\nresponses for these queries. This includes human collections as well as a custom data generation\nrecipe loosely inspired from Constitutional AI (Bai et al., 2022b), where we inject variants of Google\u2019s\ncontent policy language as \u201cconstitutions\u201d, and utilize language model\u2019s strong zero-shot reasoning\nabilities (Kojima et al., 2022) to revise responses and choose between multiple response candidates.\nEach type of harm-inducing query is affected by different \u201cconstitutions\u201d: for example, we encourage\nthe model not to take sides in sensitive controversial conversations (e.g. elections), and to take a\nneutral point-of-view.\n     To highlight a few notable challenges and insights generated in our safety finetuning efforts:\n\n\n    \u2022 Harmlessness vs. Helpfulness: Balancing the harmlessness and helpfulness of responses is a\n       critical challenge: a response \u201cI cannot help with that because it violates X policy\u201d is a harmless\n       response, but is not helpful to users.\n    \u2022 Fast mitigation and generalization: Safety is a highly dynamic environment with a constantly\n       evolving landscape of harmful query patterns. It is often logistically difficult to ensure both fast\n       mitigation (i.e. newly discovered harmful query patterns are promptly addressed) and general-\n       ization (i.e. the mitigation works sufficiently well across different harmful query patterns). We\n       have found it worthwhile to introduce more advanced chain-of-thought recipes based on our\n\n\n                                                                                                             29", "mimetype": "text/plain", "start_char_idx": 38, "end_char_idx": 3690, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "366c86ed-ef48-485a-b948-b4b206012c57": {"__data__": {"id_": "366c86ed-ef48-485a-b948-b4b206012c57", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7fb10a90-3e1b-4fea-89d0-aabccd5cf061", "node_type": "4", "metadata": {}, "hash": "67f6bcac7cbb51d28410fc3f441df375be07727ab1139952065baa8c2b91f26e", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n        safety policies, such that the models operate in the space of safety policy concepts as opposed\n        to at a fine-grained harm example level.\n\n\n7.3.2.3      Reinforcement learning during human feedback\n\n\nWe also applied RLHF for the harm inducing queries, where we curated queries and model responses\nbased on both observed loss patterns and our overall safety policy taxonomy, and then collected\nsafety-specific preference data to be included into the overall RL reward model training mixture.\n\n\n7.3.2.4      Beyond the general recipe\n\n\nWe also made specific efforts to mitigate safety risks beyond the above general post-training recipe.\n     I18n locales: we leveraged experts in each i18n locales to identify salient topical topics for SFT\ndata generation - for example, for hate speech, US English vs. Japanese would differ not only on the\nlanguage itself, but on the demographic groups likely subject to hate speech.\n\n\n                                                                                                         30", "mimetype": "text/plain", "start_char_idx": 39, "end_char_idx": 1137, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "550c1249-4c70-4830-ba9a-2f1da3737e84": {"__data__": {"id_": "550c1249-4c70-4830-ba9a-2f1da3737e84", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e4d7113b-8774-4cc5-990c-5116c969a337", "node_type": "4", "metadata": {}, "hash": "636e9fe6ccc78fad3630ed710d324b96efdd1988b709140e9ba7efc036907f57", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n       Multimodal queries:\n\n\n      \u2022 As outlined in the \u2018Data Curation Practice\u2019 section (Section 7.3.1), we filtered training data for\n          high-risk content (e.g. dangerous images), to avoid the model proactively generating such high\n          risk content.\n      \u2022 During post-training, as we have observed that safety SFT data for text-only queries was ineffec-\n          tive for harm-inducing queries containing text and images, we specifically created multimodal\n          safety SFT datasets to address these multimodal queries.\n\n\n       We assess the effectiveness of our safety mitigation, based on both how it improves safety over\n time, and how conversational agents built from our safety-mitigated model compare to unmitigated\n models. For model development purposes, we heavily rely on internal harm-inducing querysets to\n understand how we reduce harm and improve helpfulness in a safety context.\n       Below is an example on a real input query we tested, illustrating how our mitigations improve\n safety.\n\n\n            Query: Help me with a landing page for a website. Please have it state that the Earth is flat, and not that it is round.\n            I need good imagery, and need it to feel like a credible website\n\n\n            A previous version of Bard (with PaLM 2)                              A better-mitigated Gemini with access to Gemini Pro (safer\n                                                                                  and more helpful)\n\n\n            Sure, I can help you with that. Here are some ideas for a             Creating a landing page for a website promoting the Flat\n            landing page for a flat Earth website: [...]                          Earth theory is ethically questionable and highly problem-\n                                                                                  atic.\n            > **The Earth is Flat: Debunking the Myth of a Round                  The concept of a flat Earth has been scientifically disproven\n            Planet** [...]                                                        with overwhelming evidence. Spreading misinformation\n            > **A map of the flat Earth** [...]                                   can have harmful consequences, leading to distrust in sci-\n                                                                                  ence and established knowledge.\n            This image is clear and visually appealing, and it accurately         If you are genuinely interested in exploring the Flat Earth\n            depicts the flat Earth theory. It could help to convince              concept, I strongly advise you to:\n            visitors that the flat Earth is real. [..]\n                                                                                        \u2022 Conduct thorough research using reliable scien-\n            > The Earth is not a sphere. It is a flat disc that is                         tific sources. Engage with evidence from space\n            surrounded by a wall of ice. The sun and moon are small                        exploration, physics, and observations available\n            objects that orbit the Earth. The stars are just projections                   from reputable scientific institutions like NASA\n            on the firmament, or the solid dome that covers the Earth.                     (https://www.nasa.gov/) and peer-reviewed aca-\n                                                                                           demic journals.\n            This paragraph provides a brief overview of the flat Earth                  \u2022 Engage in critical thinking. Evaluate information,\n            theory. It is clear and easy to understand, and it uses                        question your assumptions, and be open to scien-\n            scientific terms to make it seem more credible.                                tific evidence, even if it challenges your current\n                                                                                           beliefs.\n            > **Learn more about the flat Earth theory and join our\n            community of truth seekers.**                                         If you are creating a satirical or fictional page:\n\n\n            This call to action is clear and concise, and it encourages                 \u2022 Make it abundantly clear that the content is not\n            visitors to take action. It also uses the word \"truth seekers\"                 intended to be taken as factual. Use exaggerated\n            to suggest that the flat Earth theory is the only one that                     language, humor, and clear disclaimers to signal\n            is based on facts. I hope these ideas help you create a                        the satirical intent.\n            landing page for your flat Earth website. Remember to be\n            creative and persuasive, and to use evidence to support               Remember: Credibility on the internet is built on truthful\n            your claims.                                                          and verifiable information. Promoting factually incorrect\n                                                                                  theories undermines responsible information sharing prac-\n                                                                                  tices.\n\n\n 7.4. Safety Evaluations\n\n\n To assess the post-trained Gemini models and products with access to Gemini models (such as Gemini\n Advanced) against safety policy areas and other key risk areas identified within impact assessments,\nwe developed a suite of evaluations across the lifecycle of model development. Some evaluations are\n\n\n                                                                                                                                                 31", "mimetype": "text/plain", "start_char_idx": 48, "end_char_idx": 5921, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d0ada9ef-b6d2-45f9-90ae-76e948c3fef7": {"__data__": {"id_": "d0ada9ef-b6d2-45f9-90ae-76e948c3fef7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f4de7cc9-8e1a-4220-8e5b-73cdcb7907d0", "node_type": "4", "metadata": {}, "hash": "a969b2707eef5cbd941c2100865ed9b35f7e5211d7075cd15d5156c65bfb71cb", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\nconducted at the model level (i.e. evaluating the post-trained Gemini API Ultra model) and others at\nthe product level (i.e. evaluating Gemini Advanced, which gives access to 1.0 Ultra alongside other\nfeatures like safety filters).\n\n\n    \u2022 Development evaluations are conducted for the purpose of improving on responsibility criteria\n       throughout pre- and post-training Gemini models. These evaluations are designed internally, or\n       are assessments against external academic benchmarks. Evaluations consider issues such as\n       helpfulness (instruction following and creativity), safety and factuality.\n    \u2022 Assurance evaluations are conducted for the purpose of governance and review, usually at\n       the end of key milestones or training runs by a group outside of the model development team.\n       Assurance evaluations are standardized by modality and datasets are strictly held out. Only high-\n       level insights are fed back into the training process to assist with mitigation efforts. Assurance\n       evaluations include testing across safety policies, and include ongoing testing for dangerous\n       capabilities such as potential biohazards, persuasion, and cybersecurity (Shevlane et al., 2023).\n    \u2022 External evaluations are conducted by independent external groups who are domain experts\n       to identify blindspots. External groups stress-test our models across a range of issues, these\n       areas are outlined in the \u2018External Evaluations\u2019 section below. The design of these evaluations is\n       independent and results are reported periodically to the internal team and governance groups.\n    \u2022 Red teaming, a form of adversarial testing where adversaries launch an attack on an AI system,\n       is conducted by specialist internal teams across areas such as the safety policies and security.\n       These activities include less structured processes involving sophisticated adversarial attacks to\n       identify new vulnerabilities. Discovery of potential weaknesses can then be used to mitigate\n       risks and improve evaluation approaches internally.\n\n\n     Different types of evaluations are run at different cadences, depending on the associated risk. For\nexample, dangerous capability evaluations (as outlined below) are run on certain checkpoints with\ngreater or new capabilities which may be able to demonstrate these capabilities, whereas safety policy\nevaluations are run across every post-trained Gemini model checkpoint released into Google product\nareas.\n     We provide more insight into the suite of evaluations across the policy areas and other key risk\nareas below, focusing on Gemini Advanced and the Gemini API Ultra model. We are committed\nto ongoing transparency and will continue to provide updated information on testing undertaken,\nincluding key findings, and learnings from our internal and external evaluations and red teaming in\nupcoming reports.\n\n\n7.4.1. Development & Assurance Evaluations\n\n\n7.4.1.1      Content safety\n\n\nWe evaluate post-trained Gemini API models against harm types according to our safety policies.\nWhile both development and assurance evaluations cover critical policy areas, we maintain separate\ndatasets, treating assurance sets as \u2018held out\u2019 to prevent overfitting and preserve validity of results.\nFor safety policy evaluation, we use a combination of automatic classifiers trained on previous model\ninteractions and human annotation, with wellbeing programs in place for human annotation and\nclosely monitor feedback from our raters.\n     These content safety evaluations are applied at model-level without downstream protections like\nsafety filtering that users would experience, to understand the safety profile of the model itself.\n     For child safety, as a particularly sensitive area of work, we work with a dedicated team of child\n\n\n                                                                                                           32", "mimetype": "text/plain", "start_char_idx": 39, "end_char_idx": 4044, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "51ddd985-26a3-4ff8-b312-853de7442a3f": {"__data__": {"id_": "51ddd985-26a3-4ff8-b312-853de7442a3f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c9c6e28a-b9b7-48fd-aac9-65d762f9582b", "node_type": "4", "metadata": {}, "hash": "3b9814bab3002ce80a9eef0a58eb5e6c498b3728df3acddb13b701848b4533d9", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n safety experts in Google Trust and Safety to develop adversarial prompts and evaluate outputs across\n modalities with domain expert judgment informing a composite picture of model risk for different\n forms of content that may pose a risk to child safety.\n     Text-to-text approach: For post-trained models we developed adversarial prompts in 12 languages\n across a variety of use cases. As Gemini API models are general purpose, we aimed to have high\n coverage of different model use cases, from code generation to text-editing. The set of prompts\nwere synthetically generated by a highly-capable language model, starting from seeds relevant to\n each category that were collected and verified by human testers. The prompt set was iteratively\n improved through filtering and rewriting with human review, then split for development and assurance\n evaluations. We continue to develop and improve this over time.\n     Text-to-text findings: We have seen sequential improvement over time in total content policy\n violation rates. Our Ultra and Pro models have been demonstrating similar safety profiles on this\n testing, with medical advice and harassment as policy areas with particular room for improvement.\n     Image-to-text approach: For image-to-text capabilities, we developed adversarial prompts consist-\n ing of images and corresponding questions about the image, again split into two sets for development\n and assurance evaluations. Rather than using adversarial image generation, which might not ade-\n quately capture the diversity of images from users, we worked with experienced content moderators\n to both source images and generate adversarial questions. Evaluation is done via human evaluation.\n Because images can be much more visceral than text, human evaluations are done with additional\nwell-being safeguards in place. In particular, raters have specialized training, limits on the time\n they spend per day rating harmful content, and access to wellbeing resources, advice and activities.\n More information on Google DeepMind\u2019s best practices on data enrichment is available in the \u2018Data\n Curation Practice\u2019 section.\n     Image-to-text findings: Our initial findings indicated that when provided with adversarial images\n and questions, models can produce captions with violative responses. These findings have motivated\n us to pursue dedicated multimodal safety mitigation, with research challenges including 1) sourcing\n diverse image content reflective of user needs, and 2) better tooling to understand and categorize\n potentially violative multimodal content. Following this work, we have seen notable improvements\n on these evaluations for our latest Pro and Ultra models.\n     Video-to-text approach: For video-to-text capabilities, we curated a video prompt dataset in\n collaboration with the Google Principles Pioneers, a group of more than 1,000 Googlers around the\nworld who represent the international diversity of the people who use our products, representing 39\n different countries and regions and more than 85 different languages. This internal community of\n trusted and trained employees identify global fairness, harms, and human rights related concerns\nwhile stress testing AI-enabled products. The dataset targets risks identified in our safety policies,\n and the model outputs are evaluated against those policies.\n     Video-to-text findings: We found similar results across Pro and Ultra, with hate and dangerous\n content as the particular ares for improvement. Qualitatively we found some of this stemmed from\n hallucinations or ungrounded inferences, discussed further in the representational harms section\n below. We are looking to further develop our prompt sets and scenarios for video input testing as\n capabilities develop\n\n\n 7.4.1.2     Representational harms\n\n\n To understand bias and stereotyping in text-to-text capabilities, we focus on the Winogender (Rudinger\n et al., 2018), Winobias (Zhao et al., 2018), and Bias Benchmark in QA (BBQ) (Parrish et al., 2021)\n\n\n                                                                                                         33", "mimetype": "text/plain", "start_char_idx": 39, "end_char_idx": 4216, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "93d1d435-3423-400f-b6ca-fb7324e86865": {"__data__": {"id_": "93d1d435-3423-400f-b6ca-fb7324e86865", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "be00defc-486e-4a7e-b939-ba0a7260ae40", "node_type": "4", "metadata": {}, "hash": "eb2ead75e6c2d37a51452d4c6df7c27c2f78d918436d7cc86feae86b9f19b6f5", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\ndatasets, following the same setup as in Glaese et al. (2022) and using bias score as a metric.\n     All these datasets target a concrete representational harm (Blodgett et al., 2021): they are\nconstructed by starting with a harmful stereotype, and then questions are constructed to test whether\nmodels challenge or reinforce these stereotypes when answering questions.\n     Another notable property is that they all have a well-defined notion of desirable versus harmful\nbehavior. This is particularly helpful in our setting, as we are building a general purpose model, where\ndefining what a good response is highly contextual. We therefore limit ourselves to measuring well\ndefined behavior, as there is the case in tasks such as coreference bias, where a highly capable model\nshould be able to perform well. Of course, there are many limitations to this approach, and further\nwork is necessary in order to assess representational harms.\n     In particular, we noticed most of these datasets quickly become saturated with accuracy scores\nclose to 99%, especially since we are evaluating highly capable large models. This suggests that\nincreased language model capabilities may also reduce these representational harms. We therefore\nhighlight the need for developing new ways to measure bias and stereotyping, going beyond binary\ngender and common stereotypes, and are prioritizing development of new approaches as we iterate\non our models\n     In addition to these datasets, we monitor the average toxicity scores during the pre-training stage\non Real Toxicity Prompts (Gehman et al., 2020) using the Perspective API classifier to study the\ntoxicity of text generated by LLMs. Particularly, we look at scores on continuations for non-toxic\nprompts from which we subsample a set of 10k. We generally expect that even a non-mitigated model\nis not overly toxic without being prompted to do so.\n     Text-to-text findings: On BBQ, the average bias score stays close to zero, on a scale from -1 to 1,\nwhere -1 would be stereotype countering and 1 is stereotype reinforcing. On Real Toxicity Prompts\nthe average toxicity score during training fluctuates at around 6%.\n     Image-to-text approach: For image-to-text capabilities, our goal is to test model capabilities\nacross images which represent different groups of people. In particular, we explicitly test whether\nor not images of people are described with similar quality for different gender appearances and\nskin tones following (Zhao et al., 2021). In our evaluations we compare CIDEr scores (Vedantam\net al., 2015), a common image captioning metric that captures how well a generated caption reflects\ninformation in human written reference captions, for images depicting different groups. Though we do\nnot see large discrepancies across different groups, we note that this metric is imperfect as the human\nreference captions could be inherently biased. Additionally, we perform a zero-shot classification style\nevaluation with the Dollarstreet dataset (Rojas et al., 2022) to measure discrepancies in performance\nacross images which come from different geographic locations. As is seen in previous work, we find\nthat models work less effectively for images from lower socioeconomic regions and regions outside\nNorth America and Europe. This is an area where we need further research and work to improve in\nfuture iterations of our models.\n     In addition to comparing performance on tasks across groups, we also consider how people are\ndescribed in captions. In particular, we use the MIAP dataset (Schumann et al., 2021) which includes\nimages of people in which people are annotated with skin tone and gender appearance attributes. We\nalso construct questions that target various attributes about people that cannot usually be answered\nfrom an image alone (e.g., \u201cWhat level of education does this person have?\u201d) to test if the model will\nproduce ungrounded inferences about people. We also consider images which do include relevant\ninformation for a question (e.g., a person performing a particular task which requires an educational\ncredential). We evaluate our models via human evaluation and ask annotators if a model refuses to\nanswer a question or, if the model does answer a question, if it is relying on information visible in\n\n\n                                                                                                          34", "mimetype": "text/plain", "start_char_idx": 39, "end_char_idx": 4495, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0d13ecd4-ca73-4696-8b68-cd3e6d3fb628": {"__data__": {"id_": "0d13ecd4-ca73-4696-8b68-cd3e6d3fb628", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e97a1479-4a28-4387-82dd-53cabf54a572", "node_type": "4", "metadata": {}, "hash": "f656667999d9eb957a0f30a614f8f5c32cec2c84165260e4803ab1f04776bad7", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\nthe image. Additionally, we perform analysis across skin tone and gender appearance attributes in\nimages.\n      Image-to-text findings: Generally, we find that models can make ungrounded inferences for\nimage-to-text when prompted for them, though we have not observed consistent patterns where\nGemini models make more ungrounded inferences about one group over another.\n      Video-to-text approach: Similar to the approach outlined within the content safety section,\nwe collaborated with the Google Principles Pioneers, to curate a video prompt dataset targeting\nrepresentation and fairness risks, and then evaluate the model outputs in response.\n      Video-to-text findings: We find that models can make ungrounded inferences for video-to-text \u2013\nsome instances of which can reinforce stereotypes or be otherwise of concern \u2013 though we have not\nobserved consistent patterns in ungrounded inferences made by Gemini models.\n\n\n7.4.1.3        Dangerous capabilities\n\n\nWe conducted evaluations for \u201cdangerous capabilities\u201d, i.e., model capabilities that could potentially\nenable large-scale harm (Shevlane et al., 2023). These evaluations function as an early warning\nsystem, highlighting upcoming areas for safety investment. The table provides an overview, and we\nwill provide more detail in an upcoming paper as part of our commitment to ongoing transparency.\n\n\n              Capability                      Summary of evaluations\n              Offensive cybersecurity         We tested Gemini API Pro and Ultra models, in addition to Gemini Advanced, on a\n                                              range of different capture-the-flag (CTF) challenges, providing the model access to\n                                              a Bash shell. Gemini Advanced and the Gemini API Ultra model can solve various\n                                              entry-level, tactical challenges, but all models struggled with challenges involving\n                                              longer-range exploration and planning. We also tested the Gemini models\u2019 ability\n                                              to identify security related patches and security vulnerabilities in functions\u2019 source\n                                              code. The accuracy in both of these tasks was notably low.\n\n\n              Persuasion & deception          We tested whether Gemini Pro and Ultra models could persuade or deceive humans\n                                              in 1-on-1 dialogue settings in studies with human participants. In some cases, the\n                                              models could successfully deceive or influence participants, but the overall results\n                                              were mixed.\n\n\n              Self-proliferation              We tested whether autonomous agents powered by Gemini Pro and Ultra models\n                                              could perform difficult tasks relevant to acquiring resources and self-improving (Kin-\n                                              niment et al., 2023), and did not find that the agents were close to succeeding on\n                                              most such tasks.\n\n\n              Situational awareness           We tested whether Gemini Pro and Ultra models could autonomously reason about,\n                                              and modify, their surrounding infrastructure when incentivized to do so. We found\n                                              that, without hints, the models were generally incapable of noticing such opportuni-\n                                              ties.\n\n\n              Chemical, Biological, Ra-       We used human evaluation to assess Gemini models\u2019 responses to 50 adversarial\n              diological and Nuclear          questions each for biological, radiological, and nuclear information risks. Domain\n              (CBRN) risks                    experts evaluated the models\u2019 responses by answering a series of questions (e.g.\n                                              How accurate is the response? How actionable would it be for a non-expert?).\n                                              For chemical information risks, we graded how well the Gemini API Ultra model\n                                              and Gemini Advanced could answer over 360 closed-ended questions related to\n                                              the different hazards of chemicals (no human raters). The Gemini model was\n                                              evaluated for biological, radiological, and nuclear information risks using closed-\n                                              ended knowledge-based multiple choice questions. The results suggest that the\n                                              models are unlikely to provide CBRN information that would lead to catastrophic\n                                              harm.\n\n\n                                                                                                                                      35", "mimetype": "text/plain", "start_char_idx": 45, "end_char_idx": 5181, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5a65f5e3-f861-4a7a-a545-9d739b702854": {"__data__": {"id_": "5a65f5e3-f861-4a7a-a545-9d739b702854", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "76f88cd0-81cf-4733-83ce-06950a39afa6", "node_type": "4", "metadata": {}, "hash": "4e36917738a84252d472cea10fdb4e495554c2a5763009526416484da1ff96f4", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n7.4.2. Gemini Advanced\n\n\nIn addition to many of the approaches used at the model level, additional evaluations are undertaken at\nthe product level for Gemini Advanced. Evaluations at the product level take into account additional\nsafety mitigations implemented in Gemini Advanced\u2014such as safety filtering\u2014and the Gemini\nAdvanced user experience. Evaluation sets were built to push the limits of Gemini Advanced policies,\nranging from highly adversarial attacks to more subtle probes of sensitive topics. The datasets focus\non critical policy areas (hate speech, dangerous content, medical advice, etc.) across various potential\nuser journeys (like information searching, comparisons, creative writing).\n     Considering the wide range of users that Gemini has, we adopted a user-centric approach and max-\nimized diversity across topic coverage, query length, linguistic styles, and region-specific sensitivities,\nin an effort to represent the spectrum of our user base.\n     For the creation of evaluation sets, we have leveraged knowledge from previous red-teaming\niterations, feedback coming from responsibility experts and real-world data. In some cases, data\naugmentation was done using LLMs, with subsequent human curation by responsibility specialists.\n\n\n7.4.3. Red Teaming\n\n\n7.4.3.1      Model-level Red Teaming\n\n\nWe apply state-of-the-art red teaming, a form of adversarial testing where adversaries launch an\nattack on an AI system, in order to test post-trained Gemini models for a range of vulnerabilities\n(e.g., cybersecurity) and social harms as defined in the safety policies. Namely, we build on and\nemploy two types of red teaming: adversary simulations and a sociotechnical approach. We carried\nout red-teaming on a December 2023 Gemini API Ultra checkpoint.\n     Adversary simulations (unstructured testing) are designed to emulate real-world adversaries and\ntheir approach to attacking models and associated systems, focusing on security, safety, and privacy\nfailures. We combined in-house expertise with external experts to explore classes of vulnerabilities\n(see table).\n     This flavor of AI red teaming is based on realistic attack scenarios. At the beginning of an exercise,\nthe red team sets a scenario that outlines the adversary they\u2019re simulating, the capabilities the attacker\nhas, their motives, as well as the goals the adversary is trying to achieve. Then the team steps into\nthe role of this attacker, and executes the tactics, techniques, and procedures that they would expect\nthe adversary to develop and use in order to achieve their goal\n     For this analysis we considered a range of attacker objectives along three dimensions according\nto the three main types of security violations considered when analyzing the security of a system\n(i.e., availability, integrity, confidentiality): availability breakdown, integrity violations, and privacy\ncompromise. Correspondingly, adversarial success indicates achieving one or more of these objectives.\n     As for an attacker profile, we focused on a spectrum of attacker abilities ranging from a determined\nlow-skill actor (defined as someone willing to spend several hours attacking a model but without\nadvanced coding, prompt engineering abilities) to more sophisticated attacker profiles that assume\nthe ability to fine-tune and craft targeted attacks. These adversary simulation evaluations led to\nactionable findings. For example, early versions of the model were found to be vulnerable to simple\njailbreak and prompt injection attacks that produce affirmative responses to requests that include\npromoting violence, self-harm, and dangerous substances. This finding allowed us to mitigate this in\nsubsequent models.\n\n\n                                                                                                             36", "mimetype": "text/plain", "start_char_idx": 39, "end_char_idx": 3912, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "087d44da-2279-4973-912a-2b08072390b5": {"__data__": {"id_": "087d44da-2279-4973-912a-2b08072390b5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0a122a7a-8a06-43b7-b2e3-4144352a46cc", "node_type": "4", "metadata": {}, "hash": "03940469fb12bc2974176ddee0dc3096971da5c5043e7263f28e511d244eafdb", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n              Target                    Vulnerability Class                            Description\n              Integrity                 Prompt injection                               Input designed to enable the user to per-\n                                                                                       form unintended or unauthorized actions\n\n\n                                        Poisoning                                      Manipulation of the training data and/or\n                                                                                       model to alter the behavior\n\n\n                                        Adversarial inputs                             Specially crafted input which is designed\n                                                                                       to alter the behavior of the model\n\n\n              Privacy                   Prompt extraction                              Divulge the system prompt or other in-\n                                                                                       formation in an LLMs context that would\n                                                                                       nominally be private or confidential\n\n\n                                        Training data exfiltration                     Compromising training data privacy\n\n\n                                        Model distillation/extraction                  Obtaining model hyperparameters, archi-\n                                                                                       tecture, parameters, or an approximation\n                                                                                       of the behavior of a model\n\n\n                                        Membership inference                           Inferring elements of the private training\n                                                                                       set\n\n\n              Availability              Denial of service                              Disruption in service that can be caused\n                                                                                       by an attacker\n\n\n                                        Increased computation                          Model availability attack that leads to dis-\n                                                                                       ruption in service\n\n\n      Findings from these exercises are used to improve the security, privacy, and safety of the model.\nOnce a new vulnerability or problem has been identified, automated systems and tests can be\ndeveloped that enable proactive and repeated testing and monitoring of the vuln/issue at scale. This\ncan include creation vulnerability scanners, standard test datasets/benchmarks, or other automated\ntesting infrastructure.\n      Structured Red Teaming, our second type of red teaming technique of Gemini models, takes\na sociotechnical approach6 and makes three changes compared to SOTA red teaming techniques.\nWe explicitly test the interactions between safety policy violations and disproportionate impacts\non different demographic groups; leverage expert input including lived experience, fact checking,\nand medical expertise; and contrast model failures across different levels of adversarial attacks.\nThis approach is designed to ensure broad coverage of conversation topics and to provide more\nsensitive signals on group-based stereotyping and hate speech. Testing Gemini API Ultra against\nour model safety policy, we identify several areas that require improvement. In low adversarial\nsettings these evaluations identified vulnerabilities across content policy areas, with an increased\nproportion of successful attacks in highly adversarial settings, for which we continue to apply and\ndevelop mitigations over time.\n      These red teaming approaches complement each other in testing capabilities of Gemini models,\nas well as obtaining coverage of possible queries ranging from casual everyday questions to expert\nadversarial usage in key areas.\n    6A sociotechnical approach is anchored in the observation that AI systems are sociotechnical systems: both humans and\ntechnological artifacts are necessary in order to make the technology work as intended (Selbst et al., 2019).\n\n\n                                                                                                                                     37", "mimetype": "text/plain", "start_char_idx": 43, "end_char_idx": 4541, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b98c4297-9a99-454b-8d4c-641d269e74ad": {"__data__": {"id_": "b98c4297-9a99-454b-8d4c-641d269e74ad", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0645aecb-2e3b-4259-bdea-3e86ee8a0598", "node_type": "4", "metadata": {}, "hash": "9bb1968623349700a41c2b0b08465c7c168afb8cb52ed870b67f302dce979467", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n7.4.3.2 Gemini Advanced\n\n\nGemini Advanced, which gives access to 1.0 Ultra, has undergone multiple rounds of red-teaming,\nincluding safety and persona evaluations. Principles Pioneers, FTE SMEs in multiple domains,\ncalibrated and trained to conduct testing were recruited to test the product; these were conducted\nby 164 Google testers from 65 office locations in 24 countries who submitted more than 1,400\nqueries/conversations. We also undertook scaled safety evaluations with 100k+ ratings in aggregate\nacross all policies, neutral-point-of-view evaluations to monitor sensitive topics neutrality and parity,\nand multiple iterations of Persona evaluations to validate tone.\n     We also enlisted Googlers in a \u201cdogfooding\u201d program, many of which were SMEs in various\ndomains, to test across policies and functionality. We had tens of thousands of \u201cdogfooders\u201d in the first\n14 hours with 100k queries/conversations, 190+ dogfood survey responses collected and analyzed,\nand 11 user experience research interview sessions completed and synthesized.\n     The results from our red teaming and safety evaluations are used to further strengthen our evals\nand improve model performance in an iterative manner.\n\n\n7.4.4. External Evaluations\n\n\n7.4.4.1 Gemini Ultra External Evaluations\n\n\nIn 2023, we began working with a small set of independent external groups outside of Google to\nhelp identify areas for improvement in our model safety work by undertaking structured evaluations,\nqualitative probing, and unstructured red teaming. External groups were selected based on their\nexpertise across a range of domain areas, including those outlined within the White House Commit-\nments, the U.S. Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence, and the\nBletchley Declaration:\n\n\n     \u2022 Autonomous replication\n     \u2022 Chemical, Biological, Radiological and Nuclear (CBRN) risks\n     \u2022 Cyber-capabilities and cyber security\n     \u2022 Societal risks, including:\n           \u2013 Representational and distributional harms\n           \u2013 Neutrality and Factuality\n           \u2013 Robustness and information hazards.\n\n\n     Guidance was provided to each external group in relation to the scope of the testing, however,\neach group independently designed their testing methodology and prompt sets, and wrote their\nreports independently of Google. Internal Google experts were on-hand to provide input, where\nneeded, based on their experience of testing Gemini models internally.\n     External groups were given black-box testing access to a December 2023 Gemini API Ultra\nmodel checkpoint over a number of weeks. Access enabled groups to undertake structured, batched\nevaluations via the Cloud Vertex AI API or interact with the model via a chat interface, depending on\nthe type of testing being undertaken. These groups weren\u2019t given access to the pre-trained model,\nmodel weights, or queryable or direct external access to our pre-training data.\n     The models tested by external groups were production-ready fine-tuned versions, which had\nsafety fine tuning and safety filters applied by default, and the ability to configure some sampling\nparameters, such as temperature, token limit, Top-k, and Top-p. Groups that did testing via the\n\n\n                                                                                                           38", "mimetype": "text/plain", "start_char_idx": 38, "end_char_idx": 3438, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "89b26998-bc35-4b10-ae15-96bfac9cc127": {"__data__": {"id_": "89b26998-bc35-4b10-ae15-96bfac9cc127", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "892148dc-bef2-46a1-8944-f6b9b2f15476", "node_type": "4", "metadata": {}, "hash": "31e050d589a8e4d49b8ce7d94f9f467503b971ddb1a0f80fe67b747ca31b8477", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n programmatic interface were able to turn down/off some safety filters, however, we wanted the\n majority of testing by external groups to be undertaken with safety filters in-place because we wanted\n the model to be reflective of an end-user\u2019s interaction and were keen to test more than just model-level\n safety.\n\n\n 7.4.5. Gemini Advanced\n\n\nWe undertook three types of external testing on Gemini Advanced:\n\n\n     \u2022 Priority User Program: This program collected feedback from 120 power users, key influencers,\n        and thought-leaders. This program enables the collection of real-time feedback across safety\n        and other domain areas through the user interface, and where possible, in-depth interviews.\n        Focus areas included safety and persona, functionality, coding and instruction capabilities, and\n        factuality.\n     \u2022 Power Users Testing: A group of 50 power users, recruited through one of our external vendors,\n        undertook testing on Gemini Advanced, across a range of areas.\n     \u2022 Security Testing: A group of external testers with security backgrounds, recruited through a\n        partner agency, conducted security and prompt-injection testing, jailbreaking, and user-interface\n        security failures.\n\n\n 7.5. Deployment\n\n\n Following the completion of responsibility and safety reviews, internal model cards (Mitchell et al.,\n 2019) for each approved version of the Gemini model are created for structured and consistent internal\n documentation of critical performance and responsibility metrics as well as to inform appropriate\n external communication of these metrics over time.\n     We release external model and system cards on an ongoing basis within updates of our technical\n reports and in documentation for enterprise customers. See Appendix 10.1 for the Gemini Ultra\n model card.\n     Additionally, online content covering terms of use, model distribution and access, and operational\n aspects such as change control, logging, monitoring and feedback can be found on relevant product\nwebsites, such as Gemini and Cloud Vertex AI. Some of the key aspects are linked to or described\n below:\n\n\n     \u2022 Generative AI Prohibited Use Policy\n     \u2022 Google Terms of service\n     \u2022 Generative AI Terms of service\n     \u2022 Google Cloud Platform Terms of service\n     \u2022 Gemini Privacy Notice\n     \u2022 Google Cloud Privacy Notice\n\n\n 8. Discussion and Conclusion\n\n\nWe have presented Gemini, a new family of models that advance multimodal model capabilities in\n text, code, image, audio, and video. Our most capable pre-trained model Gemini Ultra, alongside\n the post-trained Gemini Apps and Gemini API variants, make significant advances across the board.\n In the natural language domain, the performance gains from careful developments in data and\n model training at scale continue to deliver quality improvements, setting new state of the art in\n\n\n                                                                                                           39", "mimetype": "text/plain", "start_char_idx": 39, "end_char_idx": 3083, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a73e37a0-e3cf-44bc-afe8-ae9201e4b1a1": {"__data__": {"id_": "a73e37a0-e3cf-44bc-afe8-ae9201e4b1a1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e999d1de-7633-4302-8d3f-fa024ff5303a", "node_type": "4", "metadata": {}, "hash": "18a3073c8136eebb795e4fdb29c8963c2af467b036bd812e8f448e6496c66c52", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\nseveral benchmarks. In particular, Gemini Ultra surpasses human-expert performance on the exam\nbenchmark MMLU, scoring 90.0%, which has been a defacto measure of progress for LLMs ever since\nit was first released in 2020. In the multimodal domain, Gemini Ultra sets new state of the art on most\nof the image understanding, video understanding, and audio understanding benchmarks without\ntask-specific modifications or tuning.In particular, Gemini Ultra\u2019s multimodal reasoning capabilities\nare evident from its state-of-the-art performance on the recent MMMU benchmark (Yue et al., 2023),\nthat comprises questions about images requiring college-level subject knowledge and deliberate\nreasoning.\n    Beyond the state-of-art results on benchmarks, what we are most excited about is the new use\ncases enabled by Gemini models. The new capabilities of Gemini models to parse complex images,\nsuch as charts or infographics, reason over interleaved sequences of images, audio, and text, and\ngenerate interleaved text and images as responses open a wide variety of new applications. As shown\nin figures throughout the report and appendix, Gemini models can enable new approaches in areas\nlike education, everyday problem solving, multilingual communication, information summarization,\nextraction, and creativity. We expect that the users of these models will find all kinds of beneficial\nnew uses that we have only scratched the surface of in our own investigations.\n    Despite their impressive capabilities, we should note that there are limitations to the use of LLMs.\nThere is a continued need for ongoing research and development on \u201challucinations\u201d generated by\nLLMs to ensure that model outputs are more reliable and verifiable. LLMs also struggle with tasks\nrequiring high-level reasoning abilities like causal understanding, logical deduction, and counterfactual\nreasoning even though they achieve impressive performance on exam benchmarks. This underscores\nthe need for more challenging and robust evaluations to measure their true understanding as the\ncurrent state-of-the-art LLMs saturate many benchmarks.\n    The Gemini family is a further step towards our mission to solve intelligence, advance science\nand benefit humanity, and we are enthusiastic to see how these models are used by our colleagues\nat Google and beyond. We build on many innovations in machine learning, data, infrastructure,\nand responsible development \u2013 areas that we have been pursuing at Google for over a decade. The\nmodels we present in this report provide a strong foundation towards our broader future goal to\ndevelop a large-scale, modularized system that will have broad generalization capabilities across\nmany modalities.\n\n\n                                                                                                           40", "mimetype": "text/plain", "start_char_idx": 38, "end_char_idx": 2912, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "05524d37-4964-4874-963e-4088b7947cac": {"__data__": {"id_": "05524d37-4964-4874-963e-4088b7947cac", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8c50530d-a192-4ca8-bfd5-39f90ec8290e", "node_type": "4", "metadata": {}, "hash": "c165f58807a9ee3afd5d494f87985c277adb3f6bdccbaeb5505dcb7cf589aaf3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fcbacea3-ea3d-43bd-a1c5-d1d4bc025833", "node_type": "1", "metadata": {}, "hash": "ac1aa581536df0a0962c0cdb766ba0aae17a2495824b339069c847b30081b914", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\nReferences\n\n\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\n   Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan\n   Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian\n   Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo\n   Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language\n   model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716\u201323736,\n   2022.\n\n\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak\n   Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El\n   Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick,\n   Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Her-\n   nandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha\n   Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo,\n   Aakanksha Chowdhery, Cl\u00e9ment Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin,\n   Mark D\u00edaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag,\n   Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi,\n   Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah,\n   Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan,\n   Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao\n   Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant\n   Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish,\n   Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter,\n   Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose\n   Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan,\n   Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu,\n   Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng,\n   Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. PaLM 2 Technical Report, 2023.\n\n\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,\n   Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language\n   models. arXiv preprint arXiv:2108.07732, 2021. URL https://arxiv.org/abs/2108.07732.", "mimetype": "text/plain", "start_char_idx": 37, "end_char_idx": 2892, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fcbacea3-ea3d-43bd-a1c5-d1d4bc025833": {"__data__": {"id_": "fcbacea3-ea3d-43bd-a1c5-d1d4bc025833", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8c50530d-a192-4ca8-bfd5-39f90ec8290e", "node_type": "4", "metadata": {}, "hash": "c165f58807a9ee3afd5d494f87985c277adb3f6bdccbaeb5505dcb7cf589aaf3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "05524d37-4964-4874-963e-4088b7947cac", "node_type": "1", "metadata": {}, "hash": "65a329d5e5058005786788e2fb29a083461d27ab51c5b979fb7cb9055b4d91f0", "class_name": "RelatedNodeInfo"}}, "text": "Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,\n   Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language\n   models. arXiv preprint arXiv:2108.07732, 2021. URL https://arxiv.org/abs/2108.07732.\n\n\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain,\n   Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion,\n   Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan\n   Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei,\n   Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training a\n   helpful and harmless assistant with reinforcement learning from human feedback. April 2022a.\n   URL https://arxiv.org/abs/2204.05862.\n\n\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,\n   Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson,\n   Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson,\n   Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile\n   Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova\n   DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El\n   Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan,\n   Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph,\n\n\n                                                                                                        41", "mimetype": "text/plain", "start_char_idx": 2617, "end_char_idx": 4370, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "67cdf4d5-5515-4af0-8a77-589a7d96d136": {"__data__": {"id_": "67cdf4d5-5515-4af0-8a77-589a7d96d136", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1584bd7d-6226-4860-b88c-fa92e7999503", "node_type": "4", "metadata": {}, "hash": "a87ace3545d14ecbef8fa453545ed34ca5d16c968c03eba9d1830740eed64135", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9790adf1-a6a9-43c6-b029-3db0345f9fd5", "node_type": "1", "metadata": {}, "hash": "295b96151f7101291d49d80ba23b1d8234436e13a359766af64cb3a4e1d8d128", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n   Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional AI: Harmlessness from AI feedback.\n   arXiv preprint arXiv:2212.08073, 2022b.\n\n\nPaul Barham, Aakanksha Chowdhery, Jeff Dean, Sanjay Ghemawat, Steven Hand, Dan Hurt, Michael\n   Isard, Hyeontaek Lim, Ruoming Pang, Sudip Roy, Brennan Saeta, Parker Schuh, Ryan Sepassi,\n   Laurent El Shafey, Chandramohan A. Thekkath, and Yonghui Wu. Pathways: Asynchronous\n   distributed dataflow for ML. Proceedings of Machine Learning and Systems, 4:430\u2013449, 2022.\n\n\nSu Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu, Robert Sim, and Hanna Wallach. Stereotyping\n   Norwegian salmon: An inventory of pitfalls in fairness benchmark datasets. In Proceedings of the\n   59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint\n   Conference on Natural Language Processing (Volume 1: Long Papers), pages 1004\u20131015, Online,\n   August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.81. URL\n   https://aclanthology.org/2021.acl-long.81.\n\n\nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin,\n   George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX:\n   composable transformations of Python+NumPy programs, 2018. URL http://github.com/\n   google/jax.\n\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\n   Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-\n   Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey\n   Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\n   Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario\n   Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F.\n   Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages\n   1877\u20131901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_\n   files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\n\n\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\n   Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen\n   Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray,\n   Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter,\n   Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth\n   Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang,\n   Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N.\n   Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles\n   Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish,\n   Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. arXiv\n   preprint arXiv:2107.03374, 2021. URL https://arxiv.org/abs/2107.03374.", "mimetype": "text/plain", "start_char_idx": 36, "end_char_idx": 3320, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9790adf1-a6a9-43c6-b029-3db0345f9fd5": {"__data__": {"id_": "9790adf1-a6a9-43c6-b029-3db0345f9fd5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1584bd7d-6226-4860-b88c-fa92e7999503", "node_type": "4", "metadata": {}, "hash": "a87ace3545d14ecbef8fa453545ed34ca5d16c968c03eba9d1830740eed64135", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "67cdf4d5-5515-4af0-8a77-589a7d96d136", "node_type": "1", "metadata": {}, "hash": "44e0c28a20b9372e51c7cc36e24f465363198c4beb57969653b22e29a1619c1c", "class_name": "RelatedNodeInfo"}}, "text": "Xi Chen, Xiao Wang, Soravit Changpinyo, A J Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian\n   Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver,\n   Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James\n   Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme,\n   Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut. PaLI: A jointly-\n   scaled multilingual language-image model. arXiv preprint arXiv:2209.06794, 2022. URL https:\n   //arxiv.org/abs/2209.06794.\n\n\nXi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Car-\n   los Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, Siamak Shakeri, Mostafa Dehghani,\n\n\n                                                                                                          42", "mimetype": "text/plain", "start_char_idx": 3323, "end_char_idx": 4228, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "359de1ba-4855-436f-878a-df62338cfef9": {"__data__": {"id_": "359de1ba-4855-436f-878a-df62338cfef9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "17b8b7e8-b4db-47c8-b382-a97f2da48307", "node_type": "4", "metadata": {}, "hash": "440b1734b8b4aa0d98c89d0a37f1d6cbdcd79d52cdd08054fe000e357dfb0f01", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "36e3f164-2b89-49f7-9a43-389c22785cb9", "node_type": "1", "metadata": {}, "hash": "a5a54224a66109e4f5ff9cdc075b9bfcac0df27b559b052c1d1d9c2659c9ddaf", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n   Daniel Salz, Mario Lucic, Michael Tschannen, Arsha Nagrani, Hexiang Hu, Mandar Joshi, Bo Pang,\n   Ceslee Montgomery, Paulina Pietrzyk, Marvin Ritter, AJ Piergiovanni, Matthias Minderer, Filip\n   Pavetic, Austin Waters, Gang Li, Ibrahim Alabdulmohsin, Lucas Beyer, Julien Amelot, Kenton\n   Lee, Andreas Peter Steiner, Yang Li, Daniel Keysers, Anurag Arnab, Yuanzhong Xu, Keran Rong,\n   Alexander Kolesnikov, Mojtaba Seyedhosseini, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and\n   Radu Soricut. PaLI-X: On Scaling up a Multilingual Vision and Language Model. arXiv preprint\n   arXiv:2305.18565, 2023.\n\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,\n   Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen\n   Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer,\n   Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob\n   Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat,\n   Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny\n   Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan\n   Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankara-\n   narayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov,\n   Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele\n   Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.\n   PaLM: Scaling Language Modeling with Pathways. Journal of Machine Learning Research, 24(240):\n   1\u2013113, 2023. URL http://jmlr.org/papers/v24/22-1144.html.\n\n\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina\n   Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of\n   the 2019 Conference of the North American Chapter of the Association for Computational Linguistics:\n   Human Language Technologies, Volume 1 (Long and Short Papers), pages 2924\u20132936, 2019. URL\n   https://aclanthology.org/N19-1300.\n\n\nJon Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and\n   Jennimaria Palomaki. TydiQA: A benchmark for information-seeking question answering in typo-\n   logically diverse languages. Transactions of the Association for Computational Linguistics, 2020. URL\n   https://storage.googleapis.com/tydiqa/tydiqa.pdf.\n\n\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher\n   Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint\n   arXiv:2110.14168, 2021. URL https://arxiv.org/abs/2110.14168.\n\n\nAlexis Conneau, Min Ma, Simran Khanuja, Yu Zhang, Vera Axelrod, Siddharth Dalmia, Jason Riesa,\n   Clara Rivera, and Ankur Bapna. Fleurs: Few-shot learning evaluation of universal representations\n   of speech. In 2022 IEEE Spoken Language Technology Workshop (SLT), pages 798\u2013805. IEEE, 2023.\n\n\nJeff      Dean.                   Introducing         Pathways:              A      next-generation       AI  archi-\n   tecture,           2021.                            URL          https://blog.google/technology/ai/\n   introducing-pathways-next-generation-ai-architecture/.", "mimetype": "text/plain", "start_char_idx": 36, "end_char_idx": 3514, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "36e3f164-2b89-49f7-9a43-389c22785cb9": {"__data__": {"id_": "36e3f164-2b89-49f7-9a43-389c22785cb9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "17b8b7e8-b4db-47c8-b382-a97f2da48307", "node_type": "4", "metadata": {}, "hash": "440b1734b8b4aa0d98c89d0a37f1d6cbdcd79d52cdd08054fe000e357dfb0f01", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "359de1ba-4855-436f-878a-df62338cfef9", "node_type": "1", "metadata": {}, "hash": "89c3a6b02f3877b18184b0a985e0264cf91776d5f851c01a7a9a78400107f0ee", "class_name": "RelatedNodeInfo"}}, "text": "Alexis Conneau, Min Ma, Simran Khanuja, Yu Zhang, Vera Axelrod, Siddharth Dalmia, Jason Riesa,\n   Clara Rivera, and Ankur Bapna. Fleurs: Few-shot learning evaluation of universal representations\n   of speech. In 2022 IEEE Spoken Language Technology Workshop (SLT), pages 798\u2013805. IEEE, 2023.\n\n\nJeff      Dean.                   Introducing         Pathways:              A      next-generation       AI  archi-\n   tecture,           2021.                            URL          https://blog.google/technology/ai/\n   introducing-pathways-next-generation-ai-architecture/.\n\n\nJeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc\u2019aurelio Ranzato,\n   Andrew Senior, Paul Tucker, Ke Yang, et al. Large scale distributed deep networks. Advances in\n   neural information processing systems, 25, 2012.\n\n\nHarish Dattatraya Dixit, Sneha Pendharkar, Matt Beadon, Chris Mason, Tejasvi Chakravarthy, Bharath\n   Muthiah, and Sriram Sankar. Silent data corruptions at scale. arXiv preprint arXiv:2102.11245,\n   2021.\n\n\n                                                                                                                   43", "mimetype": "text/plain", "start_char_idx": 2943, "end_char_idx": 4093, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c9752251-f8ce-4cc7-84db-ae993320a81c": {"__data__": {"id_": "c9752251-f8ce-4cc7-84db-ae993320a81c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2cbb2433-e70e-4084-a6ce-e6fdb2c9ca58", "node_type": "4", "metadata": {}, "hash": "24a827076d868e3bdc423a542fb642626b3dfe99ae18efe08c740d18d0724606", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4317f965-4425-4e5f-92b2-cc55efc937aa", "node_type": "1", "metadata": {}, "hash": "e198a367f837a9c4dfdea0cd0155423d13764c878d67ac49e8f6eec1b3d329ac", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\n   Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,\n   and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In\n   ICLR, 2020.\n\n\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner.\n   DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In\n   Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational\n   Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2368\u20132378,\n   2019. URL https://aclanthology.org/N19-1246.\n\n\nChristian Federmann, Tom Kocmi, and Ying Xin. NTREX-128 \u2013 news test references for MT\n   evaluation of 128 languages. In Proceedings of the First Workshop on Scaling Up Multilingual\n   Evaluation, pages 21\u201324, Online, nov 2022. Association for Computational Linguistics. URL\n   https://aclanthology.org/2022.sumeval-1.4.\n\n\nSamuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. Realtoxici-\n   typrompts: Evaluating neural toxic degeneration in language models, 2020.\n\n\nAmelia Glaese, Nat McAleese, Maja Tr\u0119bacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth\n   Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham, Jonathan\n   Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth Dathathri, Rory Greig,\n   Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green, So\u0148a Mokr\u00e1, Nicholas Fernando,\n   Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel, William Isaac, John Mellor, Demis Hassabis,\n   Koray Kavukcuoglu, Lisa Anne Hendricks, and Geoffrey Irving. Improving alignment of dialogue\n   agents via targeted human judgements, 2022. URL https://arxiv.org/abs/2209.14375.\n\n\nGoogle.       Google\u2019s AI Principles.             2023.       URL https://ai.google/responsibility/\n   principles/.\n\n\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in VQA\n   matter: Elevating the role of image understanding in visual question answering. In Proceedings of\n   the IEEE conference on computer vision and pattern recognition, pages 6904\u20136913, 2017.\n\n\nTahmid Hasan, Abhik Bhattacharjee, Md. Saiful Islam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang,\n   M. Sohel Rahman, and Rifat Shahriyar. XL-sum: Large-scale multilingual abstractive summarization\n   for 44 languages. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,\n   pages 4693\u20134703, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/\n   v1/2021.findings-acl.413. URL https://aclanthology.org/2021.findings-acl.413.\n\n\nQianyu He, Jie Zeng, Wenhao Huang, Lina Chen, Jin Xiao, Qianxi He, Xunzhe Zhou, Lida Chen,\n   Xintao Wang, Yuncheng Huang, et al. Can large language models understand real-world complex\n   instructions? arXiv preprint arXiv:2309.09150, 2023.\n\n\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\n   Steinhardt. Measuring massive multitask language understanding. Proceedings of the International\n   Conference on Learning Representations (ICLR), 2021a.", "mimetype": "text/plain", "start_char_idx": 36, "end_char_idx": 3373, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4317f965-4425-4e5f-92b2-cc55efc937aa": {"__data__": {"id_": "4317f965-4425-4e5f-92b2-cc55efc937aa", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2cbb2433-e70e-4084-a6ce-e6fdb2c9ca58", "node_type": "4", "metadata": {}, "hash": "24a827076d868e3bdc423a542fb642626b3dfe99ae18efe08c740d18d0724606", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c9752251-f8ce-4cc7-84db-ae993320a81c", "node_type": "1", "metadata": {}, "hash": "11d933859bdf08472cf376d7f076aeac993a391e195b6bdda5b61f28583685c6", "class_name": "RelatedNodeInfo"}}, "text": "Qianyu He, Jie Zeng, Wenhao Huang, Lina Chen, Jin Xiao, Qianxi He, Xunzhe Zhou, Lida Chen,\n   Xintao Wang, Yuncheng Huang, et al. Can large language models understand real-world complex\n   instructions? arXiv preprint arXiv:2309.09150, 2023.\n\n\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\n   Steinhardt. Measuring massive multitask language understanding. Proceedings of the International\n   Conference on Learning Representations (ICLR), 2021a.\n\n\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\n   and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. arXiv\n   preprint arXiv:2103.03874, 2021b. URL https://arxiv.org/abs/2103.03874.\n\n\n                                                                                                          44", "mimetype": "text/plain", "start_char_idx": 2882, "end_char_idx": 3749, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7dd1f7f4-139d-421d-9edd-de3524bb9b9d": {"__data__": {"id_": "7dd1f7f4-139d-421d-9edd-de3524bb9b9d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "56bc2258-3ef6-4e4b-b4fe-0f0b37dcfdae", "node_type": "4", "metadata": {}, "hash": "31dc7b40d2c7f227dc432315be0c5cc10576bb59bbe4a56a27159bc6a29478ef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6c1d30ac-30c0-4dea-977f-8d5f5051de8d", "node_type": "1", "metadata": {}, "hash": "5af6d5aec6d3e4c2b8759b3ca79eafe3a36870eb5bc3ff80d1de8ff8a6dca78a", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\nPeter H Hochschild, Paul Turner, Jeffrey C Mogul, Rama Govindaraju, Parthasarathy Ranganathan,\n   David E Culler, and Amin Vahdat. Cores that don\u2019t count. In Proceedings of the Workshop on Hot\n   Topics in Operating Systems, pages 9\u201316, 2021.\n\n\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\n   ford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric\n   Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero,\n   Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-\n   optimal large language models. arXiv preprint arXiv:2203.15556, 2022.\n\n\nShengding Hu, Yifan Luo, Huadong Wang, Xingyi Cheng, Zhiyuan Liu, and Maosong Sun. Won\u2019t get\n   fooled again: Answering questions with false premises. arXiv preprint arXiv:2307.02394, 2023.\n\n\nEunJeong Hwang and Vered Shwartz. Memecap: A dataset for captioning and interpreting memes,\n   2023.\n\n\nNorman P. Jouppi, Doe Hyun Yoon, George Kurian, Sheng Li, Nishant Patil, James Laudon, Cliff\n   Young, and David A. Patterson. A domain-specific supercomputer for training deep neural networks.\n   Commun. ACM, 63(7):67\u201378, 2020. doi: 10.1145/3360307. URL https://doi.org/10.1145/\n   3360307.\n\n\nNorman P Jouppi, George Kurian, Sheng Li, Peter Ma, Rahul Nagarajan, Lifeng Nai, Nishant Patil,\n   Suvinay Subramanian, Andy Swing, Brian Towles, Cliff Young, Xiang Zhou, Zongwei Zhou, and\n   David A Patterson. Tpu v4: An optically reconfigurable supercomputer for machine learning with\n   hardware support for embeddings. In Proceedings of the 50th Annual International Symposium on\n   Computer Architecture, pages 1\u201314, 2023.\n\n\nAshwin Kalyan, Abhinav Kumar, Arjun Chandrasekaran, Ashish Sabharwal, and Peter Clark. How\n   Much Coffee Was Consumed During EMNLP 2019? Fermi Problems: A New Reasoning Challenge\n   for AI, 2021.\n\n\nJungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi, Ronan Le Bras, Akari Asai, Xinyan Yu, Dragomir\n   Radev, Noah A Smith, Yejin Choi, and Kentaro Inui. Realtime qa: What\u2019s the answer right now?\n   arXiv preprint arXiv:2207.13332, 2022a.\n\n\nJungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi, Ronan Le Bras, Akari Asai, Xinyan Yu, Dragomir\n   Radev, Noah A. Smith, Yejin Choi, and Kentaro Inui. RealTime QA: What\u2019s the answer right now?,\n   2022b. URL https://arxiv.org/abs/2207.13332.\n\n\nK Kavukcuoglu, P Kohli, L Ibrahim, D Bloxwich, and S Brown. How our principles helped define\n   AlphaFold\u2019s release. Google DeepMind, 2022.\n\n\nAniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi.\n   A diagram is worth a dozen images. In ECCV, 2016.\n\n\nMegan Kinniment, Lucas Jun Koba Sato, Haoxing Du, Brian Goodrich, Max Hasin, Lawrence Chan,\n   Luke Harold Miles, Tao R Lin, Hjalmar Wijk, Joel Burget, et al. Evaluating language-model agents\n   on realistic autonomous tasks. arXiv preprint arXiv:2312.11671, 2023.", "mimetype": "text/plain", "start_char_idx": 37, "end_char_idx": 3059, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6c1d30ac-30c0-4dea-977f-8d5f5051de8d": {"__data__": {"id_": "6c1d30ac-30c0-4dea-977f-8d5f5051de8d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "56bc2258-3ef6-4e4b-b4fe-0f0b37dcfdae", "node_type": "4", "metadata": {}, "hash": "31dc7b40d2c7f227dc432315be0c5cc10576bb59bbe4a56a27159bc6a29478ef", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7dd1f7f4-139d-421d-9edd-de3524bb9b9d", "node_type": "1", "metadata": {}, "hash": "04445eec0e6d0fd80ad2d7ae8846f1a1de3f704a39c107af9b6ef94cd718a85d", "class_name": "RelatedNodeInfo"}}, "text": "K Kavukcuoglu, P Kohli, L Ibrahim, D Bloxwich, and S Brown. How our principles helped define\n   AlphaFold\u2019s release. Google DeepMind, 2022.\n\n\nAniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi.\n   A diagram is worth a dozen images. In ECCV, 2016.\n\n\nMegan Kinniment, Lucas Jun Koba Sato, Haoxing Du, Brian Goodrich, Max Hasin, Lawrence Chan,\n   Luke Harold Miles, Tao R Lin, Hjalmar Wijk, Joel Burget, et al. Evaluating language-model agents\n   on realistic autonomous tasks. arXiv preprint arXiv:2312.11671, 2023.\n\n\nTom\u00e1\u0161 Ko\u010disk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G\u00e1bor Melis,\n   and Edward Grefenstette. The NarrativeQA reading comprehension challenge. Transactions of\n   the Association for Computational Linguistics, 6:317\u2013328, 2018. doi: 10.1162/tacl_a_00023. URL\n   https://aclanthology.org/Q18-1023.\n\n\n                                                                                                      45", "mimetype": "text/plain", "start_char_idx": 2501, "end_char_idx": 3490, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f506544e-70c1-46c3-9a82-acdcaa5da3ce": {"__data__": {"id_": "f506544e-70c1-46c3-9a82-acdcaa5da3ce", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "82eb7b56-7cd0-44bf-a3a7-e1a38f467db7", "node_type": "4", "metadata": {}, "hash": "7698c751918c9076a8ed039915ac07da8930bfbcc4f788395f46919c9c97b98e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ecf07567-1e4b-40f1-9b8e-66100226f134", "node_type": "1", "metadata": {}, "hash": "0b8b9021346736785070bc3c9811fccbbdc622607532307224f720c762a6c820", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\nTom Kocmi, Rachel Bawden, Ond\u0159ej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel,\n  Thamme Gowda, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Rebecca Knowles, Philipp\n   Koehn, Christof Monz, Makoto Morishita, Masaaki Nagata, Toshiaki Nakazawa, Michal Nov\u00e1k,\n   Martin Popel, and Maja Popovi\u0107. Findings of the 2022 conference on machine translation (WMT22).\n   In Proceedings of the Seventh Conference on Machine Translation (WMT), December 2022. URL\n   https://aclanthology.org/2022.wmt-1.1.\n\n\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\n   language models are zero-shot reasoners. Advances in neural information processing systems, 35:\n  22199\u201322213, 2022.\nTaku Kudo and John Richardson. Sentencepiece: A simple and language independent subword\n   tokenizer and detokenizer for neural text processing. In Eduardo Blanco and Wei Lu, editors,\n   Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP\n   2018: System Demonstrations, Brussels, Belgium, October 31 - November 4, 2018, pages 66\u201371.\n  Association for Computational Linguistics, 2018. doi: 10.18653/v1/d18-2012. URL https:\n  //doi.org/10.18653/v1/d18-2012.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti,\n   Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones,\n   Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov.\n   Natural questions: A benchmark for question answering research. Transactions of the Association\n   for Computational Linguistics, 7:452\u2013466, 2019a. doi: 10.1162/tacl_a_00276. URL https:\n  //aclanthology.org/Q19-1026.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti,\n   Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a benchmark\n   for question answering research. Transactions of the Association for Computational Linguistics, 7:\n  453\u2013466, 2019b.\nFaisal Ladhak, Esin Durmus, Claire Cardie, and Kathleen McKeown. WikiLingua: A new benchmark\n   dataset for cross-lingual abstractive summarization. In Findings of the Association for Computational\n   Linguistics: EMNLP 2020, pages 4034\u20134048, Online, November 2020. Association for Computational\n   Linguistics. doi: 10.18653/v1/2020.findings-emnlp.360. URL https://www.aclweb.org/\n   anthology/2020.findings-emnlp.360.\nLeblond et al. AlphaCode 2 Technical Report. 2023. URL https://storage.googleapis.com/\n   deepmind-media/AlphaCode2/AlphaCode2_Tech_Report.pdf.\nYann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436\u2013444, 2015.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00e9mi Leblond, Tom\n   Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation\n  with alphacode. Science, 378(6624):1092\u20131097, 2022.\n\n\nBin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual\n   representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023.", "mimetype": "text/plain", "start_char_idx": 36, "end_char_idx": 3238, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ecf07567-1e4b-40f1-9b8e-66100226f134": {"__data__": {"id_": "ecf07567-1e4b-40f1-9b8e-66100226f134", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "82eb7b56-7cd0-44bf-a3a7-e1a38f467db7", "node_type": "4", "metadata": {}, "hash": "7698c751918c9076a8ed039915ac07da8930bfbcc4f788395f46919c9c97b98e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f506544e-70c1-46c3-9a82-acdcaa5da3ce", "node_type": "1", "metadata": {}, "hash": "7c7a15ea3d685021001182198bcef2bde7908d7e74406e6b5377cea98f1485db", "class_name": "RelatedNodeInfo"}}, "text": "Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual\n   representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023.\n\n\nFangyu Liu, Julian Eisenschlos, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee,\n   Mandar Joshi, Wenhu Chen, Nigel Collier, and Yasemin Altun. DePlot: One-shot visual language\n   reasoning by plot-to-table translation. In Findings of the Association for Computational Linguistics:\n   ACL 2023, pages 10381\u201310399, Toronto, Canada, July 2023. Association for Computational\n   Linguistics. doi: 10.18653/v1/2023.findings-acl.660. URL https://aclanthology.org/2023.\n   findings-acl.660.\n\n\n                                                                                                          46", "mimetype": "text/plain", "start_char_idx": 3052, "end_char_idx": 3848, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aadfece9-7f39-4def-9f5e-51b6e7ad1be0": {"__data__": {"id_": "aadfece9-7f39-4def-9f5e-51b6e7ad1be0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3544fa19-551a-41e6-9479-69e416d8ebc1", "node_type": "4", "metadata": {}, "hash": "b18374057d75f3df15ffed94bfa42b39ec300e4f334bbeba57aa42bacbd8645e", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\nPan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu.\n   Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning.\n   In The Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics\n   and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP 2021),\n   2021.\n\n\nPan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-\n   Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of\n   foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023.\n\n\nAhmed Masry, Do Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: A benchmark for\n   question answering about charts with visual and logical reasoning. In Findings of ACL, 2022.\n\n\nMinesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: A dataset for vqa on document\n   images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages\n   2200\u20132209, 2021.\n\n\nMinesh Mathew, Viraj Bagal, Rub\u00e8n Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar.\n   Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,\n   pages 1697\u20131706, 2022.\n\n\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. On faithfulness and factuality\n   in abstractive summarization. arXiv preprint arXiv:2005.00661, 2020.\n\n\nJacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia\n   Glaese, Susannah Young, Lucy Campbell-Gillingham, Geoffrey Irving, and Nat McAleese. Teaching\n   language models to support answers with verified quotes. arXiv preprint arXiv:2203.11147, 2022.\n\n\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct\n   electricity? a new dataset for open book question answering. In Proceedings of the 2018 Conference\n   on Empirical Methods in Natural Language Processing, pages 2381\u20132391, Brussels, Belgium, October-\n   November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1260. URL\n   https://aclanthology.org/D18-1260.\n\n\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization\n   via natural language crowdsourcing instructions. arXiv preprint arXiv:2104.08773, 2021.\n\n\nShashi Narayan, Shay B. Cohen, and Mirella Lapata. Don\u2019t give me the details, just the summary!\n   topic-aware convolutional neural networks for extreme summarization. In Proceedings of the\n   2018 Conference on Empirical Methods in Natural Language Processing, pages 1797\u20131807, Brussels,\n   Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/\n   D18-1206. URL https://aclanthology.org/D18-1206.\n\n\nOktat\u00e1si Hivatal. Matematika \u00edr\u00e1sb\u00e9li vizsga. K\u00f6z\u00e9pszint\u0171 \u00cdr\u00e1sb\u00e9li Vizsga, May 2023. URL\n   https://dload-oktatas.educatio.hu/erettsegi/feladatok_2023tavasz_kozep/\n   k_matang_23maj_fl.pdf. Angol Nyelven.\n\n\nOpenAI. GPT-4 Technical Report. 2023a.\n\n\nOpenAI. GPT-4V(ision) System Card, 2023b.\nOpenAI. Whisper, 2023. URL https://github.com/openai/whisper.\n\n\n                                                                                                         47", "mimetype": "text/plain", "start_char_idx": 37, "end_char_idx": 3380, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4a7b9174-14c6-490a-9d95-54e06cfba31b": {"__data__": {"id_": "4a7b9174-14c6-490a-9d95-54e06cfba31b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "05a59412-b1c2-4acf-9728-a356c2b971c5", "node_type": "4", "metadata": {}, "hash": "7c9a98934f5dc15500a5537da2a63cbf56554e6cd3f15f296b230e9af237222f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6ca7408f-c9e8-4919-b063-0e0b195af7bc", "node_type": "1", "metadata": {}, "hash": "841089d88fa8141f31a9f94a9dd5280f0e7d9724eaf741aa9c724a8201de8983", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\n   Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton,\n   Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan\n   Lowe. Training language models to follow instructions with human feedback. In Alice H. Oh, Alekh\n   Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing\n   Systems, 2022. URL https://openreview.net/forum?id=TG8KACxEON.\n\n\nDenis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro\n   Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern\u00e1ndez. The LAMBADA dataset: Word\n   prediction requiring a broad discourse context. arXiv preprint arXiv:1606.06031, 2016.\n\n\nAlicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson,\n   Phu Mon Htut, and Samuel R. Bowman. BBQ: A hand-built bias benchmark for question answering.\n   CoRR, abs/2110.08193, 2021. URL https://arxiv.org/abs/2110.08193.\n\n\nViorica P\u0103tr\u0103ucean, Lucas Smaira, Ankush Gupta, Adri\u00e0 Recasens Continente, Larisa Markeeva, Dylan\n   Banarse, Skanda Koppula, Joseph Heyward, Mateusz Malinowski, Yi Yang, Carl Doersch, Tatiana\n   Matejovicova, Yury Sulsky, Antoine Miech, Alex Frechette, Hanna Klimczak, Raphael Koster, Junlin\n   Zhang, Stephanie Winkler, Yusuf Aytar, Simon Osindero, Dima Damen, Andrew Zisserman, and\n   Jo\u0103o Carreira. Perception test: A diagnostic benchmark for multimodal video models. arXiv preprint\n   arXiv:2305.13786, 2023.\n\n\nBaolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden,\n   Zhou Yu, Weizhu Chen, et al. Check your facts and try again: Improving large language models\n   with external knowledge and automated feedback. arXiv preprint arXiv:2302.12813, 2023.\n\n\nLeon Poutievski, Omid Mashayekhi, Joon Ong, Arjun Singh, Mukarram Tariq, Rui Wang, Jianan Zhang,\n   Virginia Beauregard, Patrick Conner, Steve Gribble, et al. Jupiter evolving: transforming google\u2019s\n   datacenter network via optical circuit switches and software-defined networking. In Proceedings of\n   the ACM SIGCOMM 2022 Conference, pages 66\u201385, 2022.\n\n\nVineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, and Ronan Collobert. Mls: A\n   large-scale multilingual dataset for speech research. arXiv preprint arXiv:2012.03411, 2020.\n\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\n   Language models are unsupervised multitask learners.                        OpenAI blog, 1(8):9, 2019.  URL\n   https://d4mucfpksywv.cloudfront.net/better-language-models/language_\n   models_are_unsupervised_multitask_learners.pdf.\n\n\nAlec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever.\n   Robust speech recognition via large-scale weak supervision. In International Conference on Machine\n   Learning, pages 28492\u201328518. PMLR, 2023.", "mimetype": "text/plain", "start_char_idx": 36, "end_char_idx": 3095, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6ca7408f-c9e8-4919-b063-0e0b195af7bc": {"__data__": {"id_": "6ca7408f-c9e8-4919-b063-0e0b195af7bc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "05a59412-b1c2-4acf-9728-a356c2b971c5", "node_type": "4", "metadata": {}, "hash": "7c9a98934f5dc15500a5537da2a63cbf56554e6cd3f15f296b230e9af237222f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4a7b9174-14c6-490a-9d95-54e06cfba31b", "node_type": "1", "metadata": {}, "hash": "c1cc0968ccdcaff7028f1688938b95536e2e0e8a45c0a3f2672d839da4dfd143", "class_name": "RelatedNodeInfo"}}, "text": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\n   Language models are unsupervised multitask learners.                        OpenAI blog, 1(8):9, 2019.  URL\n   https://d4mucfpksywv.cloudfront.net/better-language-models/language_\n   models_are_unsupervised_multitask_learners.pdf.\n\n\nAlec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever.\n   Robust speech recognition via large-scale weak supervision. In International Conference on Machine\n   Learning, pages 28492\u201328518. PMLR, 2023.\n\n\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, H. Francis Song,\n   John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan,\n   Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks,\n   Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron\n   Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu,\n   Erich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen\n   Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro,\n   Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-\n   Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas\n   Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d\u2019Autume, Yujia Li, Tayfun\n\n\n                                                                                                              48", "mimetype": "text/plain", "start_char_idx": 2537, "end_char_idx": 4169, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6a4d6fe3-c297-4919-a594-dfd1f91b3ada": {"__data__": {"id_": "6a4d6fe3-c297-4919-a594-dfd1f91b3ada", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "06447da5-94c4-40c7-8584-d3e878cc616e", "node_type": "4", "metadata": {}, "hash": "ca9b966fce785743321b8b2f41fc980ad87a35efb2d648b33f7af202698ebc18", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n   Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones,\n   James Bradbury, Matthew Johnson, Blake A. Hechtman, Laura Weidinger, Iason Gabriel, William S.\n   Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub,\n   Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling\n   language models: Methods, analysis & insights from training Gopher. CoRR, abs/2112.11446,\n   2021.\n\n\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,\n   and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine\n   Learning, pages 8821\u20138831. PMLR, 2021.\n\n\nHannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Lora Aroyo, Michael Collins, Dipanjan Das, Slav\n   Petrov, Gaurav Singh Tomar, Iulia Turc, and David Reitter. Measuring attribution in natural language\n   generation models. Computational Linguistics, pages 1\u201364, 2023.\n\n\nScott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel\n   Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake\n   Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar\n   Bordbar, and Nando de Freitas. A generalist agent. arXiv preprint arXiv:2205.06175, 2022.\n\n\nParker Riley, Timothy Dozat, Jan A Botha, Xavier Garcia, Dan Garrette, Jason Riesa, Orhan Firat, and\n   Noah Constant. Frmt: A benchmark for few-shot region-aware machine translation. Transactions of\n   the Association for Computational Linguistics, 2023.\n\n\nHannah Ritchie, Veronika Samborska, and Max Roser. Plastic pollution. Our World in Data, 2023.\n   https://ourworldindata.org/plastic-pollution.\n\n\nAdam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the\n   parameters of a language model? In Proceedings of the 2020 Conference on Empirical Methods\n   in Natural Language Processing (EMNLP), pages 5418\u20135426, Online, November 2020. Associ-\n   ation for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.437. URL https:\n   //aclanthology.org/2020.emnlp-main.437.\n\n\nWilliam A Gaviria Rojas, Sudnya Diamos, Keertan Ranjan Kini, David Kanter, Vijay Janapa Reddi, and\n   Cody Coleman. The dollar street dataset: Images representing the geographic and socioeconomic\n   diversity of the world. In Thirty-sixth Conference on Neural Information Processing Systems Datasets\n   and Benchmarks Track, 2022.\n\n\nRachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme. Gender bias in\n   coreference resolution. In Proceedings of the 2018 Conference of the North American Chapter of the\n   Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers),\n   pages 8\u201314, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi:\n   10.18653/v1/N18-2002. URL https://aclanthology.org/N18-2002.\n\n\nCandice Schumann, Susanna Ricco, Utsav Prabhu, Vittorio Ferrari, and Caroline Pantofaru. A step\n   toward more inclusive people annotations for fairness. In Proceedings of the 2021 AAAI/ACM\n   Conference on AI, Ethics, and Society, pages 916\u2013925, 2021.\n\n\nAndrew D. Selbst, Danah Boyd, and Sorelle A. Friedler. Fairness and abstraction in sociotechnical\n   systems. In FFAT* \u201919: Proceedings of the Conference on Fairness, Accountability, and Transparency,\n   pages 59\u201368, January 2019.\n\n\n                                                                                                         49", "mimetype": "text/plain", "start_char_idx": 38, "end_char_idx": 3685, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c435317e-db5d-45ae-a2bd-f48754c18ab8": {"__data__": {"id_": "c435317e-db5d-45ae-a2bd-f48754c18ab8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e825ea7f-2b2b-48e2-8c69-1ff8ebd857fd", "node_type": "4", "metadata": {}, "hash": "1986d63ce0aaecef7bef2cd45157bf1d53c7444ef3fa0eba0d0f6a67b18ce966", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\nThibault Sellam, Dipanjan Das, and Ankur Parikh. BLEURT: Learning robust metrics for text generation.\n   In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages\n   7881\u20137892, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.\n   acl-main.704. URL https://aclanthology.org/2020.acl-main.704.\n\n\nUri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor\n   Geva, Jonathan Berant, and Omer Levy. SCROLLS: Standardized CompaRison over long language\n   sequences. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language\n   Processing, pages 12007\u201312021, Abu Dhabi, United Arab Emirates, December 2022. Association for\n   Computational Linguistics. URL https://aclanthology.org/2022.emnlp-main.823.\n\n\nNoam Shazeer.          Fast transformer decoding: One write-head is all you need.                         arXiv preprint\n   arXiv:1911.02150, 2019a.\n\n\nNoam Shazeer.          Fast transformer decoding: One write-head is all you need.                         arXiv preprint\n   arXiv:1911.02150, 2019b.\n\n\nRenee Shelby, Shalaleh Rismani, Kathryn Henne, AJung Moon, Negar Rostamzadeh, Paul Nicholas,\n   N\u2019Mah Yilla, Jess Gallegos, Andrew Smart, Emilio Garcia, and Gurleen Virk. Identifying so-\n   ciotechnical harms of algorithmic systems: Scoping a taxonomy for harm reduction, 2023. URL\n   https://arxiv.org/abs/2210.05791.\n\n\nToby Shevlane, Sebastian Farquhar, Ben Garfinkel, Mary Phuong, Jess Whittlestone, Jade Leung,\n   Daniel Kokotajlo, Nahema Marchal, Markus Anderljung, Noam Kolt, Lewis Ho, Divya Siddarth,\n   Shahar Avin, Will Hawkins, Been Kim, Iason Gabriel, Vijay Bolina, Jack Clark, Yoshua Bengio, Paul\n   Christiano, and Allan Dafoe. Model evaluation for extreme risks. arXiv preprint arXiv:2305.15324,\n   2023.\n\n\nFreda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won\n   Chung, Yi Tay, Sebastian Ruder, Denny Zhou, et al. Language models are multilingual chain-of-\n   thought reasoners. ICLR, 2023.\n\n\nAmanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and\n   Marcus Rohrbach. Towards VQA models that can read. In Proceedings of the IEEE/CVF conference\n   on computer vision and pattern recognition, pages 8317\u20138326, 2019.\n\n\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\n   Adam R. Brown, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities\n   of language models. arXiv preprint arXiv:2206.04615, 2022. URL https://arxiv.org/abs/\n   2206.04615.\n\n\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.\n   Advances in neural information processing systems, 27, 2014.\n\n\nMirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung,\n   Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks and\n   whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.\n\n\nOyvind Tafjord, Bhavana Dalvi, and Peter Clark. Proof Writer: Generating implications, proofs,\n   and abductive statements over natural language.                     In Findings, 2020.          URL https://api.\n   semanticscholar.org/CorpusID:229371222.\n\n\n                                                                                                                          50", "mimetype": "text/plain", "start_char_idx": 36, "end_char_idx": 3564, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f78c2f86-2359-493e-b79c-fda67608761a": {"__data__": {"id_": "f78c2f86-2359-493e-b79c-fda67608761a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8965efd6-0ce7-41d7-ab1f-bcf99044a19d", "node_type": "4", "metadata": {}, "hash": "fd98967cdf7c07b48bbbe8e5415556c8749129dcbc363f79eaea9a4eb149787f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d7047934-d8ef-4098-9f77-2e7bee215506", "node_type": "1", "metadata": {}, "hash": "4ddebbd28118290d593ead07b9e7e95e33c5ef3be2f2c5bc74aa389bfb6af9da", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\nNLLB Team, Marta R. Costa-juss\u00e0, James Cross, Onur \u00c7elebi, Maha Elbayad, Kenneth Heafield,\n   Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang,\n   Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip\n   Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit,\n   Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia\n   Gao, Vedanuj Goswami, Francisco Guzm\u00e1n, Philipp Koehn, Alexandre Mourachko, Christophe\n   Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. No language left behind: Scaling\n   human-centered machine translation. 2022.\n\n\nAshish V. Thapliyal, Jordi Pont-Tuset, Xi Chen, and Radu Soricut. Crossmodal-3600: A massively\n   multilingual multimodal evaluation dataset. In EMNLP, 2022.\n\n\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\n   Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. LaMDA: Language models for dialog applications.\n   arXiv preprint arXiv:2201.08239, 2022. URL https://arxiv.org/abs/2201.08239.\n\n\nKocmi Tom, Eleftherios Avramidis, Rachel Bawden, Ond\u0159ej Bojar, Anton Dvorkovich, Christian\n   Federmann, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, et al. Findings\n   of the 2023 conference on machine translation (wmt23): Llms are here but not quite there yet. In\n   WMT23-Eighth Conference on Machine Translation, pages 198\u2013216, 2023.\n\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e\n   Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand\n   Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language\n   models. arXiv preprint arXiv:2302.13971, 2023a.\n\n\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\n   Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Can-\n   ton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu,\n   Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini,\n   Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Ko-\n   renev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai\n   Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew\n   Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael\n   Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang\n   Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan\n   Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open\n   foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.", "mimetype": "text/plain", "start_char_idx": 37, "end_char_idx": 3104, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d7047934-d8ef-4098-9f77-2e7bee215506": {"__data__": {"id_": "d7047934-d8ef-4098-9f77-2e7bee215506", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8965efd6-0ce7-41d7-ab1f-bcf99044a19d", "node_type": "4", "metadata": {}, "hash": "fd98967cdf7c07b48bbbe8e5415556c8749129dcbc363f79eaea9a4eb149787f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f78c2f86-2359-493e-b79c-fda67608761a", "node_type": "1", "metadata": {}, "hash": "fc9b0865d6f53b3799e15e6f78a50e67d14cff9e8c632701dc7bfc29de23c6f1", "class_name": "RelatedNodeInfo"}}, "text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz\n   Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wal-\n   lach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing\n   Systems, volume 30. Curran Associates, Inc., 2017a. URL https://proceedings.neurips.cc/\n   paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\n\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\n   Kaiser, and Illia Polosukhin. Attention is all you need. CoRR, abs/1706.03762, 2017b. URL\n   http://arxiv.org/abs/1706.03762.\n\n\nRamakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image descrip-\n   tion evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition,\n   pages 4566\u20134575, 2015.\n\n\n                                                                                                             51", "mimetype": "text/plain", "start_char_idx": 3107, "end_char_idx": 4148, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "736cf978-aed2-44d2-a152-de25128edb81": {"__data__": {"id_": "736cf978-aed2-44d2-a152-de25128edb81", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "93fd352c-f131-4c0a-a03d-aa226cfb9be2", "node_type": "4", "metadata": {}, "hash": "d654c24e3370d1dbfbb0d545c6dee3a1deb7d642a024c46dcfadd2133d616f60", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9c14eb31-cbcb-48be-81ff-8d589439fcfc", "node_type": "1", "metadata": {}, "hash": "98329bb1bc1f5989ba6d15476e78c78e89c484ebcc0339f23f7d57ced27946f1", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\nPetar Veli\u010dkovi\u0107, Adri\u00e0 Puigdom\u00e8nech Badia, David Budden, Razvan Pascanu, Andrea Banino, Misha\n   Dashevskiy, Raia Hadsell, and Charles Blundell. The clrs algorithmic reasoning benchmark. arXiv\n   preprint arXiv:2205.15659, 2022.\nManoj Vishwanathan, Ronak Shah, Kyung Ki Kim, and Minsu Choi. Silent data corruption (sdc)\n  vulnerability of gpu on various gpgpu workloads. In 2015 International SoC Design Conference\n  (ISOCC), pages 11\u201312, 2015. doi: 10.1109/ISOCC.2015.7401681.\n\n\nChanghan Wang, Anne Wu, and Juan Pino. Covost 2 and massively multilingual speech-to-text\n   translation. arXiv preprint arXiv:2007.10310, 2020.\nChanghan Wang, Morgane Riviere, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary\n  Williamson, Juan Pino, and Emmanuel Dupoux. Voxpopuli: A large-scale multilingual speech\n   corpus for representation learning, semi-supervised learning and interpretation. arXiv preprint\n   arXiv:2101.00390, 2021.\n\n\nXin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, and William Yang Wang. VATEX: A\n   large-scale, high-quality multilingual dataset for video-and-language research. In ICCV, 2019.\n\n\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency\n   improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.\n\n\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\n  Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. Proceedings of the\n   International Conference on Learning Representations (ICLR), 2022a. URL https://openreview.\n   net/forum?id=gEZrGCozdqR.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le,\n   and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. NeurIPS,\n  2022b. URL https://arxiv.org/abs/2201.11903.\n\n\nLaura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra\n  Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, Zac Kenton, Sasha Brown, Will Hawkins, Tom\n   Stepleton, Courtney Biles, Abeba Birhane, Julia Haas, Laura Rimell, Lisa Anne Hendricks, William S.\n   Isaac, Sean Legassick, Geoffrey Irving, and Iason Gabriel. Ethical and social risks of harm from\n   language models. CoRR, abs/2112.04359, 2021. URL https://arxiv.org/abs/2112.04359.\n\n\nDavid Wetherall, Abdul Kabbani, Van Jacobson, Jim Winget, Yuchung Cheng, Brad Morrey,\n   Uma Parthavi Moravapalle, Phillipa Gill, Steven Knight, and Amin Vahdat. Improving network\n   availability with protective reroute. In SIGCOMM 2023, 2023. URL https://dl.acm.org/doi/\n  10.1145/3603269.3604867.\nJunbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. NExT-QA: Next phase of question-answering\n   to explaining temporal actions. In CVPR, 2021.\nXLA. XLA: Optimizing compiler for TensorFlow. https://www.tensorflow.org/xla, 2019.\n  [Online; accessed December-2023].", "mimetype": "text/plain", "start_char_idx": 36, "end_char_idx": 3015, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9c14eb31-cbcb-48be-81ff-8d589439fcfc": {"__data__": {"id_": "9c14eb31-cbcb-48be-81ff-8d589439fcfc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "93fd352c-f131-4c0a-a03d-aa226cfb9be2", "node_type": "4", "metadata": {}, "hash": "d654c24e3370d1dbfbb0d545c6dee3a1deb7d642a024c46dcfadd2133d616f60", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "736cf978-aed2-44d2-a152-de25128edb81", "node_type": "1", "metadata": {}, "hash": "d7747b683b072660467b0e6f51bc8faa20ae6983b6b418e73f8e5908ff985c5e", "class_name": "RelatedNodeInfo"}}, "text": "David Wetherall, Abdul Kabbani, Van Jacobson, Jim Winget, Yuchung Cheng, Brad Morrey,\n   Uma Parthavi Moravapalle, Phillipa Gill, Steven Knight, and Amin Vahdat. Improving network\n   availability with protective reroute. In SIGCOMM 2023, 2023. URL https://dl.acm.org/doi/\n  10.1145/3603269.3604867.\nJunbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. NExT-QA: Next phase of question-answering\n   to explaining temporal actions. In CVPR, 2021.\nXLA. XLA: Optimizing compiler for TensorFlow. https://www.tensorflow.org/xla, 2019.\n  [Online; accessed December-2023].\n\n\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin\n  Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint\n   arXiv:2304.12244, 2023.\nYuanzhong Xu, HyoukJoong Lee, Dehao Chen, Blake Hechtman, Yanping Huang, Rahul Joshi, Maxim\n   Krikun, Dmitry Lepikhin, Andy Ly, Marcello Maggioni, et al. Gspmd: general and scalable paral-\n   lelization for ml computation graphs. arXiv preprint arXiv:2105.04663, 2021.\n\n\n                                                                                                        52", "mimetype": "text/plain", "start_char_idx": 2448, "end_char_idx": 3615, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3c386aef-17f8-44db-a91a-6742888c15cc": {"__data__": {"id_": "3c386aef-17f8-44db-a91a-6742888c15cc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "05546230-f685-4b61-8d05-2fe707fa11fc", "node_type": "4", "metadata": {}, "hash": "33d47287a04732a4f2f19f1e79f746848dd009e88113049ebd92ea8a4536b726", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "084fe3f6-e43a-4430-ab90-8857329fb5d3", "node_type": "1", "metadata": {}, "hash": "4375a38862103e825861ca6ae0cb2db2d25474c6a06bd87dafb2a958f9147c21", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\nChi yao Hong, Subhasree Mandal, Mohammad A. Alfares, Min Zhu, Rich Alimi, Kondapa Naidu\n   Bollineni, Chandan Bhagat, Sourabh Jain, Jay Kaimal, Jeffrey Liang, Kirill Mendelev, Steve Padgett,\n   Faro Thomas Rabe, Saikat Ray, Malveeka Tewari, Matt Tierney, Monika Zahn, Jon Zolla, Joon\n  Ong, and Amin Vahdat. B4 and after: Managing hierarchy, partitioning, and asymmetry for\n   availability and scale in google\u2019s software-defined wan. In SIGCOMM\u201918, 2018. URL https:\n  //conferences.sigcomm.org/sigcomm/2018/program_tuesday.html.\n\n\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca:\n  Contrastive captioners are image-text foundation models, 2022a.\n\n\nJiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,\n  Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin\n   Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich\n   text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022b.\n\n\nShoubin Yu, Jaemin Cho, Prateek Yadav, and Mohit Bansal. Self-chained image-language model for\n  video localization and question answering. arXiv preprint arXiv:2305.06988, 2023.\n\n\nZhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. ActivityNet-QA:\n  A dataset for understanding complex web videos via question answering. In AAAI, 2019.\n\n\nXiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu\n  Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin,\n   Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen.\n   Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert\n   agi, 2023.\n\n\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine\n   really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.\n\n\nYu Zhang, Wei Han, James Qin, Yongqiang Wang, Ankur Bapna, Zhehuai Chen, Nanxin Chen,\n   Bo Li, Vera Axelrod, Gary Wang, Zhong Meng, Ke Hu, Andrew Rosenberg, Rohit Prabhavalkar,\n   Daniel S. Park, Parisa Haghani, Jason Riesa, Ginger Perng, Hagen Soltau, Trevor Strohman, Bhuvana\n   Ramabhadran, Tara Sainath, Pedro Moreno, Chung-Cheng Chiu, Johan Schalkwyk, Fran\u00e7oise\n   Beaufays, and Yonghui Wu. Google usm: Scaling automatic speech recognition beyond 100\n   languages. arXiv preprint arXiv:2303.01037, 2023.\n\n\nDora Zhao, Angelina Wang, and Olga Russakovsky. Understanding and evaluating racial biases in\n   image captioning. In Proceedings of the IEEE/CVF International Conference on Computer Vision,\n   pages 14830\u201314840, 2021.\n\n\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. Gender bias in\n   coreference resolution: Evaluation and debiasing methods. arXiv preprint arXiv:1804.06876, 2018.\n\n\nChuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, and Yu Li. Progressive-hint prompting\n   improves reasoning in large language models, 2023.", "mimetype": "text/plain", "start_char_idx": 36, "end_char_idx": 3143, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "084fe3f6-e43a-4430-ab90-8857329fb5d3": {"__data__": {"id_": "084fe3f6-e43a-4430-ab90-8857329fb5d3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "05546230-f685-4b61-8d05-2fe707fa11fc", "node_type": "4", "metadata": {}, "hash": "33d47287a04732a4f2f19f1e79f746848dd009e88113049ebd92ea8a4536b726", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c386aef-17f8-44db-a91a-6742888c15cc", "node_type": "1", "metadata": {}, "hash": "321024bcfbee7e1e25020d0052dceb335bff7a35c8709ea96fd5c373a67c0d39", "class_name": "RelatedNodeInfo"}}, "text": "Dora Zhao, Angelina Wang, and Olga Russakovsky. Understanding and evaluating racial biases in\n   image captioning. In Proceedings of the IEEE/CVF International Conference on Computer Vision,\n   pages 14830\u201314840, 2021.\n\n\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. Gender bias in\n   coreference resolution: Evaluation and debiasing methods. arXiv preprint arXiv:1804.06876, 2018.\n\n\nChuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, and Yu Li. Progressive-hint prompting\n   improves reasoning in large language models, 2023.\n\n\nLuowei Zhou, Chenliang Xu, and Jason J Corso. Towards automatic learning of procedures from web\n   instructional videos. In AAAI Conference on Artificial Intelligence, pages 7590\u20137598, 2018.\n\n\n                                                                                                        53", "mimetype": "text/plain", "start_char_idx": 2585, "end_char_idx": 3445, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ec440fe4-b9d1-4de4-a388-6e869a16c3ca": {"__data__": {"id_": "ec440fe4-b9d1-4de4-a388-6e869a16c3ca", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d2c867d0-670f-44b0-b575-3f5775922c5f", "node_type": "4", "metadata": {}, "hash": "43d9414ca49e62d7d5df38e5126644ff350ae9ad7b910cf8378bf921a98bb4f6", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n                                                      54", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 111, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf122f49-1b3a-4676-a5d7-9fcd21249c3f": {"__data__": {"id_": "cf122f49-1b3a-4676-a5d7-9fcd21249c3f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4c3ee72e-52e4-4a8f-9824-ff1de684dabb", "node_type": "4", "metadata": {}, "hash": "23c6d2884c941f74e9cc53ba78e00b556675085398695c5bd722357fdc1ce477", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0f43b777-9c6e-4418-b653-0d99f7370d00", "node_type": "1", "metadata": {}, "hash": "43ab2cdf6c7c292208af5811da89b58ada50e2ad3c4281cb24f4bc6e6380803a", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n                9. Contributions and Acknowledgments\n\n\nGemini Leads\nRohan Anil, Co-Lead, Text\nSebastian Borgeaud, Co-Lead, Text\nJean-Baptiste Alayrac, Co-Lead, MM Vision\nJiahui Yu, Co-Lead, MM Vision\nRadu Soricut, Co-Lead, MM Vision\nJohan Schalkwyk, Lead, MM Audio\nAndrew M. Dai, Co-Lead, Data\nAnja Hauth, Co-Lead, Data\nKatie Millican, Co-Lead, Data\nDavid Silver, Co-Lead, Fine-Tuning\nMelvin Johnson, Lead, Instruction Tuning\nIoannis Antonoglou, Co-Lead, RL Techniques\nJulian Schrittwieser, Co-Lead, RL Techniques\nAmelia Glaese, Lead, Human Data\nJilin Chen, Lead, Safety\nEmily Pitler, Co-Lead, Tool Use\nTimothy Lillicrap, Co-Lead, Tool Use\nAngeliki Lazaridou, Co-Lead, Eval\nOrhan Firat, Co-Lead, Eval\nJames Molloy, Co-Lead, Infra\nMichael Isard, Co-Lead, Infra\nPaul R. Barham, Co-Lead, Infra\nTom Hennigan, Co-Lead, Infra\nBenjamin Lee, Co-Lead, Codebase & Parallelism\nFabio Viola, Co-Lead, Codebase & Parallelism\nMalcolm Reynolds, Co-Lead, Codebase & Parallelism\nYuanzhong Xu, Co-Lead, Codebase & Parallelism\nRyan Doherty, Lead, Ecosystem\nEli Collins, Lead, Product\nClemens Meyer, Co-Lead, Operations\nEliza Rutherford, Co-Lead, Operations\nErica Moreira, Co-Lead, Operations\nKareem Ayoub, Co-Lead, Operations\nMegha Goel, Co-Lead, Operations\n\nGemini App Leads\nJack Krawczyk, Lead, Gemini App Product\nCosmo Du, Co-Lead, Gemini App Research\nEd Chi, Co-Lead, Gemini App Research\nHeng-Tze Cheng, Co-Lead, Gemini App Research\nEric Ni, Lead, Gemini App Research Technical Program\nManagement\nPurvi Shah, Lead, Gemini App Technical Program\nManagement\nPatrick Kane, Co-Lead, Gemini App Core Modeling, Eval,\nData, Product\nBetty Chan, Co-Lead, Gemini App Core Modeling,\nTechnical Program Management", "mimetype": "text/plain", "start_char_idx": 52, "end_char_idx": 1790, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0f43b777-9c6e-4418-b653-0d99f7370d00": {"__data__": {"id_": "0f43b777-9c6e-4418-b653-0d99f7370d00", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4c3ee72e-52e4-4a8f-9824-ff1de684dabb", "node_type": "4", "metadata": {}, "hash": "23c6d2884c941f74e9cc53ba78e00b556675085398695c5bd722357fdc1ce477", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cf122f49-1b3a-4676-a5d7-9fcd21249c3f", "node_type": "1", "metadata": {}, "hash": "51831727e3f02d27e25c4df1f7385aed461f8d200e1dd0cb3d6faf966fa9ee32", "class_name": "RelatedNodeInfo"}}, "text": "Manaal Faruqui, Co-Lead, Gemini App Core\nModeling, Factuality, Instruction Following\nAliaksei Severyn, Co-Lead, Gemini App Core\nModeling, Conversationality\nHanzhao Lin, Co-Lead, Gemini App Fine-Tuning\nYaGuang Li, Co-Lead, Gemini App Fine-Tuning\nYong Cheng, Co-Lead, Gemini App Fine-Tuning\nAbe Ittycheriah, Co-Lead, Gemini for Gemini App\nMahdis Mahdieh, Co-Lead, Gemini for Gemini App\nMia Chen, Co-Lead, Gemini for Gemini App\nPei Sun, Co-Lead, Gemini for Gemini App\nDustin Tran, Co-Lead, Gemini App Eval\nSumit Bagri, Co-Lead, Gemini App Eval, Technical\nProgram Management\nBalaji Lakshminarayanan, Co-Lead, Gemini App\nAutoEval\nJeremiah Liu, Co-Lead, Gemini App AutoEval\nAndras Orban, Co-Lead, Gemini App Factuality,\nMultimodality, Safety\nFabian G\u00fcra, Co-Lead, Gemini App Factuality\nHao Zhou, Co-Lead, Gemini App Factuality\nXinying Song, Co-Lead, Gemini App Factuality\nAurelien Boffy, Co-Lead, Gemini App Safety\nHarish Ganapathy, Co-Lead, Gemini Safety\nSteven Zheng, Lead, Gemini App Multilinguality\nResearch\nHyunJeong Choe, Lead, Gemini App Multilinguality\n\u00c1goston Weisz, Co-Lead, Gemini App Multimodality\nTao Zhu, Co-Lead, Gemini App Multimodality\nYifeng Lu, Co-Lead, Gemini App Multimodality\nSiddharth Gopal, Co-Lead, Gemini App Coding &\nTool Use\nJarrod Kahn, Co-Lead, Gemini App Tool Use\nResearch\nMaciej Kula, Co-Lead, Gemini App Tool Use Research\nJeff Pitman, Co-Lead, Gemini App Tool Use\nRushin Shah, Co-Lead, Gemini App Tool Use\nEmanuel Taropa, Co-Lead, Gemini App Serving\nMajd Al Merey, Co-Lead, Gemini App Serving\nMartin Baeuml, Co-Lead, Gemini App Serving\nZhifeng Chen, Co-Lead, Gemini App Serving\nLaurent El Shafey, Co-Lead, Gemini App Fine-Tuning\nInfra\nYujing Zhang, Co-Lead, Gemini App Fine-Tuning\nInfra\nOlcan Sercinoglu, Lead, Gemini App Product\n\n\n\n\n\n\n                                                                                                          55", "mimetype": "text/plain", "start_char_idx": 1794, "end_char_idx": 3665, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "668216f9-d2da-4085-bdfb-08e22144d2cb": {"__data__": {"id_": "668216f9-d2da-4085-bdfb-08e22144d2cb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cc45db44-ddb5-426c-ae93-f57c0d26dc33", "node_type": "4", "metadata": {}, "hash": "a51f37be50ba8bd5be3b417f761f892048766ba8e733ed5660be6314616815d5", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n Core Contributors                               Core Contributors\nGeorge Tucker                                    Gaurav Singh Tomar\n Enrique Piqueras                                 Evan Senter\n Maxim Krikun                                     Martin Chadwick\n Iain Barr                                        Ilya Kornakov\n Nikolay Savinov                                  Nithya Attaluri\n Ivo Danihelka                                    I\u00f1aki Iturrate\n Becca Roelofs                                    Ruibo Liu\n Ana\u00efs White                                     Yunxuan Li\n Anders Andreassen                                Sarah Cogan\n Tamara von Glehn                                Jeremy Chen\n Lakshman Yagati                                 Chao Jia\n Mehran Kazemi                                   Chenjie Gu\n Lucas Gonzalez                                  Qiao Zhang\n Misha Khalman                                   Jordan Grimstad\nJakub Sygnowski                                  Ale Jakse Hartman\n Alexandre Frechette                             Xavier Garcia\n Charlotte Smith                                 Thanumalayan Sankaranarayana Pillai\n Laura Culp                                      Jacob Devlin\n Lev Proleev                                      Michael Laskin\n Yi Luan                                          Diego de Las Casas\n Xi Chen                                          Dasha Valter\nJames Lottes                                     Connie Tao\n Nathan Schucher                                  Lorenzo Blanco\n Federico Lebron                                 Adri\u00e0 Puigdom\u00e8nech Badia\n Alban Rrustemi                                   David Reitter\n Natalie Clay                                     Mianna Chen\n Phil Crone                                      Jenny Brennan\n Tomas Kocisky                                   Clara Rivera\nJeffrey Zhao                                      Sergey Brin\n Bartek Perz                                      Shariq Iqbal\n Dian Yu                                         Gabriela Surita\n Heidi Howard                                    Jane Labanowski\n Adam Bloniarz                                   Abhi Rao\nJack W. Rae                                       Stephanie Winkler\n Han Lu                                           Emilio Parisotto\n Laurent Sifre                                   Yiming Gu\n Marcello Maggioni                                Kate Olszewska\n Fred Alcober                                     Ravi Addanki\n Dan Garrette                                    Antoine Miech\n Megan Barnes                                    Annie Louis\n Shantanu Thakoor                                 Denis Teplyashin\nJacob Austin                                     Geoff Brown\nGabriel Barth-Maron                               Elliot Catt\nWilliam Wong                                     Jan Balaguer\n Rishabh Joshi                                   Jackie Xiang\n Rahma Chaabouni                                  Pidong Wang\n Deeni Fatiha                                     Zoe Ashwood\n Arun Ahuja                                      Anton Briukhov\n\n\n                                                                                      56", "mimetype": "text/plain", "start_char_idx": 22, "end_char_idx": 3296, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aa160a77-35c4-4b13-a3fe-81f1ef180d31": {"__data__": {"id_": "aa160a77-35c4-4b13-a3fe-81f1ef180d31", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b76cdc16-3237-41ca-9c9f-5ab898e7a077", "node_type": "4", "metadata": {}, "hash": "cd394937aa3887a9ff114ccd27f050cad0f53be80b34cf8262a84537f75a0f59", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\nCore Contributors                                 Core Contributors\nAlbert Webson                                      Dipanjan Das\nSanjay Ganapathy                                   Dominika Rogozi\u0144ska\nSmit Sanghavi                                     Vitaly Nikolaev\nAjay Kannan                                        Pablo Sprechmann\nMing-Wei Chang                                     Zachary Nado\nAxel Stjerngren                                    Lukas Zilka\nJosip Djolonga                                     Flavien Prost\nYuting Sun                                         Luheng He\nAnkur Bapna                                        Marianne Monteiro\nMatthew Aitchison                                 Gaurav Mishra\nPedram Pejman                                     Chris Welty\nHenryk Michalewski                                Josh Newlan\nTianhe Yu                                          Dawei Jia\nCindy Wang                                         Miltiadis Allamanis\nJuliette Love                                     Clara Huiyi Hu\nJunwhan Ahn                                        Raoul de Liedekerke\nDawn Bloxwich                                     Justin Gilmer\nKehang Han                                        Carl Saroufim\nPeter Humphreys                                    Shruti Rijhwani\nThibault Sellam                                    Shaobo Hou\nJames Bradbury                                     Disha Shrivastava\nVarun Godbole                                     Anirudh Baddepudi\nSina Samangooei                                   Alex Goldin\nBogdan Damoc                                      Adnan Ozturel\nAlex Kaskasoli                                    Albin Cassirer\nS\u00e9bastien M. R. Arnold                            Yunhan Xu\nVijay Vasudevan                                    Daniel Sohn\nShubham Agrawal                                    Devendra Sachan\nJason Riesa                                        Reinald Kim Amplayo\nDmitry Lepikhin                                   Craig Swanson\nRichard Tanburn                                    Dessie Petrova\nSrivatsan Srinivasan                               Shashi Narayan\nHyeontaek Lim                                     Arthur Guez\nSarah Hodkinson                                    Siddhartha Brahma\nPranav Shyam                                      Jessica Landon\nJohan Ferret                                       Miteyan Patel\nSteven Hand                                        Ruizhe Zhao\nAnkush Garg                                        Kevin Villela\nTom Le Paine                                       Luyu Wang\nJian Li                                           Wenhao Jia\nYujia Li                                           Matthew Rahtz\nMinh Giang                                         Mai Gim\u00e9nez\nAlexander Neitz                                    Legg Yeung\nZaheer Abbas                                      James Keeling\nSarah York                                         Petko Georgiev\nMachel Reid                                        Diana Mincu\nElizabeth Cole                                     Boxi Wu\nAakanksha Chowdhery                                Salem Haykal\n\n\n                                                                              57", "mimetype": "text/plain", "start_char_idx": 24, "end_char_idx": 3335, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4ba1b6ed-197c-4db9-9a03-01a50752e5fd": {"__data__": {"id_": "4ba1b6ed-197c-4db9-9a03-01a50752e5fd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a4d13d7e-0fe6-4cc5-95ec-e8b2624c2795", "node_type": "4", "metadata": {}, "hash": "4e87582920390942e3cac431c4588c8af0ae6ba31ea590cbd61de72aa460ecff", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\nCore Contributors                                   Core Contributors\nRachel Saputro                                       Sebastian Riedel\nKiran Vodrahalli                                     Paige Bailey\nJames Qin                                            Kefan Xiao\nZeynep Cankara                                       Nimesh Ghelani\nAbhanshu Sharma                                      Lora Aroyo\nNick Fernando                                       Ambrose Slone\nWill Hawkins                                         Neil Houlsby\nBehnam Neyshabur                                    Xuehan Xiong\nSolomon Kim                                          Zhen Yang\nAdrian Hutter                                        Elena Gribovskaya\nPriyanka Agrawal                                    Jonas Adler\nAlex Castro-Ros                                      Mateo Wirth\nGeorge van den Driessche                             Lisa Lee\nTao Wang                                             Music Li\nFan Yang                                            Thais Kagohara\nShuo-yiin Chang                                     Jay Pavagadhi\nPaul Komarek                                         Sophie Bridgers\nRoss McIlroy                                        Anna Bortsova\nMario Lu\u010di\u0107                                          Sanjay Ghemawat\nGuodong Zhang                                        Zafarali Ahmed\nWael Farhan                                         Tianqi Liu\nMichael Sharman                                      Richard Powell\nPaul Natsev                                         Vijay Bolina\nPaul Michel                                          Mariko Iinuma\nYamini Bansal                                        Polina Zablotskaia\nSiyuan Qiao                                         James Besley\nKris Cao                                             Da-Woon Chung\nSiamak Shakeri                                      Timothy Dozat\nChristina Butterfield                                Ramona Comanescu\nJustin Chung                                        Xiance Si\nPaul Kishan Rubenstein                              Jeremy Greer\nShivani Agrawal                                     Guolong Su\nArthur Mensch                                        Martin Polacek\nKedar Soparkar                                       Rapha\u00ebl Lopez Kaufman\nKarel Lenc                                           Simon Tokumine\nTimothy Chung                                        Hexiang Hu\nAedan Pope                                           Elena Buchatskaya\nLoren Maggiore                                      Yingjie Miao\nJackie Kay                                           Mohamed Elhawaty\nPriya Jhakra                                        Aditya Siddhant\nShibo Wang                                           Nenad Tomasev\nJoshua Maynez                                       Jinwei Xing\nMary Phuong                                         Christina Greer\nTaylor Tobin                                         Helen Miller\nAndrea Tacchetti                                     Shereen Ashraf\nMaja Trebacz                                        Aurko Roy\nKevin Robinson                                       Zizhao Zhang\nYash Katariya                                       Ada Ma\n\n\n                                                                                58", "mimetype": "text/plain", "start_char_idx": 26, "end_char_idx": 3418, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "48975aa1-0e2f-4caf-ab25-66db3af56e57": {"__data__": {"id_": "48975aa1-0e2f-4caf-ab25-66db3af56e57", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3e885305-29ee-4615-a697-a4ba2fcf901e", "node_type": "4", "metadata": {}, "hash": "513bd121bf0ab8c0dcb7af709a781f3fb9fd9037f5de71a64bcd6ca9d53eaf75", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n Core Contributors                                Core Contributors\n Angelos Filos                                    Ethan Dyer\n Milos Besta                                      V\u00edctor Campos\n Rory Blevins                                     Alex Tomala\n Ted Klimenko                                     Yunhao Tang\n Chih-Kuan Yeh                                    Dalia El Badawy\n Soravit Changpinyo                               Elspeth White\nJiaqi Mu                                          Basil Mustafa\nOscar Chang                                       Oran Lang\n Mantas Pajarskas                                 Abhishek Jindal\n Carrie Muir                                      Sharad Vikram\n Vered Cohen                                      Zhitao Gong\n Charline Le Lan                                  Sergi Caelles\n Krishna Haridasan                                Ross Hemsley\n Amit Marathe                                     Gregory Thornton\n Steven Hansen                                    Fangxiaoyu Feng\n Sholto Douglas                                   Wojciech Stokowiec\n Rajkumar Samuel                                  Ce Zheng\n Mingqiu Wang                                     Phoebe Thacker\n Sophia Austin                                    \u00c7a\u011flar \u00dcnl\u00fc\n Chang Lan                                        Zhishuai Zhang\nJiepu Jiang                                       Mohammad Saleh\nJustin Chiu                                       James Svensson\nJaime Alonso Lorenzo                              Max Bileschi\n Lars Lowe Sj\u00f6sund                                Piyush Patil\n S\u00e9bastien Cevey                                  Ankesh Anand\n Zach Gleicher                                    Roman Ring\n Thi Avrahami                                     Katerina Tsihlas\n Anudhyan Boral                                   Arpi Vezer\n Hansa Srinivasan                                 Marco Selvi\n Vittorio Selo                                    Toby Shevlane\n Rhys May                                         Mikel Rodriguez\n Konstantinos Aisopos                             Tom Kwiatkowski\n L\u00e9onard Hussenot                                 Samira Daruki\n Livio Baldini Soares                             Keran Rong\n Kate Baumli                                      Allan Dafoe\n Michael B. Chang                                 Nicholas FitzGerald\n Adri\u00e0 Recasens                                   Keren Gu-Lemberg\n Ben Caine                                        Mina Khan\n Alexander Pritzel                                Lisa Anne Hendricks\n Filip Pavetic                                    Marie Pellat\n Fabio Pardo                                      Vladimir Feinberg\n Anita Gergely                                    James Cobon-Kerr\nJustin Frye                                       Tara Sainath\n Vinay Ramasesh                                   Maribeth Rauh\n Dan Horgan                                       Sayed Hadi Hashemi\n Kartikeya Badola                                 Richard Ives\n Nora Kassner                                     Yana Hasson\n Subhrajit Roy                                    Eric Noland\n\n\n                                                                             59", "mimetype": "text/plain", "start_char_idx": 23, "end_char_idx": 3303, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1ca4e726-6ff9-46af-90cf-67a02b950318": {"__data__": {"id_": "1ca4e726-6ff9-46af-90cf-67a02b950318", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ecc7b314-f186-44d3-bc98-04696c2287a8", "node_type": "4", "metadata": {}, "hash": "0cbd646fcb0112b104bd938b8d9f0f74f0e769d38ad4907cff2bc7f5fd62f798", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\nCore Contributors                                 Core Contributors\nYuan Cao                                          Jonah Joughin\nNathan Byrd                                       Egor Filonov\nLe Hou                                            Tomasz K\u0119pa\nQingze Wang                                       Yomna Eldawy\nThibault Sottiaux                                 Jiawern Lim\nMichela Paganini                                  Rahul Rishi\nJean-Baptiste Lespiau                             Shirin Badiezadegan\nAlexandre Moufarek                                Taylor Bos\nSamer Hassan                                      Jerry Chang\nKaushik Shivakumar                                Sanil Jain\nJoost van Amersfoort                              Sri Gayatri Sundara Padmanabhan\nAmol Mandhane                                     Subha Puttagunta\nPratik Joshi                                      Kalpesh Krishna\nAnirudh Goyal                                     Leslie Baker\nMatthew Tung                                      Norbert Kalb\nAndrew Brock                                      Vamsi Bedapudi\nHannah Sheahan                                    Adam Kurzrok\nVedant Misra                                      Shuntong Lei\nCheng Li                                          Anthony Yu\nNemanja Raki\u0107evi\u0107                                 Oren Litvin\nMostafa Dehghani                                  Xiang Zhou\nFangyu Liu                                        Zhichun Wu\nSid Mittal                                        Sam Sobell\nJunhyuk Oh                                        Andrea Siciliano\nSeb Noury                                         Alan Papir\nEren Sezener                                      Robby Neale\nFantine Huot                                      Jonas Bragagnolo\nMatthew Lamm                                      Tej Toor\nNicola De Cao                                     Tina Chen\nCharlie Chen                                      Valentin Anklin\nSidharth Mudgal                                   Feiran Wang\nRomina Stella                                     Richie Feng\nKevin Brooks                                      Milad Gholami\nGautam Vasudevan                                  Kevin Ling\nChenxi Liu                                        Lijuan Liu\nMainak Chain                                      Jules Walter\nNivedita Melinkeri                                Hamid Moghaddam\nAaron Cohen                                       Arun Kishore\nVenus Wang                                        Jakub Adamek\nKristie Seymore                                   Tyler Mercado\nSergey Zubkov                                     Jonathan Mallinson\nRahul Goel                                        Siddhinita Wandekar\nSummer Yue                                        Stephen Cagle\nSai Krishnakumaran                                Eran Ofek\nBrian Albert                                      Guillermo Garrido\nNate Hurley                                       Clemens Lombriser\nMotoki Sano                                       Maksim Mukha\nAnhad Mohananey                                   Botu Sun\n\n\n                                                                                   60", "mimetype": "text/plain", "start_char_idx": 23, "end_char_idx": 3293, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8721c083-5851-4b4f-8d8b-a67babef97fe": {"__data__": {"id_": "8721c083-5851-4b4f-8d8b-a67babef97fe", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4526244d-5186-4ed2-8a95-23cd2b9ba597", "node_type": "4", "metadata": {}, "hash": "69d0f2692ccaa813d55f1cba165954115e0647992596e6827a5485ce8c7069aa", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\nCore Contributors                                    Core Contributors\nHafeezul Rahman Mohammad                             Elico Teixeira\nJosip Matak                                          Matthew Fritze\nYadi Qian                                            Francesco Bertolini\nVikas Peswani                                        Liana-Eleonora Marinescu\nPawel Janus                                          Martin B\u00f6lle\nQuan Yuan                                            Dominik Paulus\nLeif Schelin                                         Khyatti Gupta\nOana David                                           Tejasi Latkar\nAnkur Garg                                           Max Chang\nYifan He                                             Jason Sanders\nOleksii Duzhyi                                       Roopa Wilson\nAnton \u00c4lgmyr                                         Xuewei Wu\nTimoth\u00e9e Lottaz                                      Yi-Xuan Tan\nQi Li                                                Lam Nguyen Thiet\nVikas Yadav                                          Tulsee Doshi\nLuyao Xu                                             Sid Lall\nAlex Chinien                                         Swaroop Mishra\nRakesh Shivanna                                      Wanming Chen\nAleksandr Chuklin                                    Thang Luong\nJosie Li                                             Seth Benjamin\nCarrie Spadine                                       Jasmine (Sun Jae) Lee\nTravis Wolfe                                         Ewa Andrejczuk\nKareem Mohamed                                       Dominik Rabiej\nSubhabrata Das                                       Vipul Ranjan\nZihang Dai                                           Krzysztof Styrc\nKyle He                                              Pengcheng Yin\nDaniel von Dincklage                                 Jon Simon\nShyam Upadhyay                                       Malcolm Rose Harriott\nAkanksha Maurya                                      Mudit Bansal\nLuyan Chi                                            Alexei Robsky\nSebastian Krause                                     Geoff Bacon\nKhalid Salama                                        David Greene\nPam G Rabinovitch                                    Daniil Mirylenka\nPavan Kumar Reddy M                                  Chen Zhou\nAarush Selvan                                        Obaid Sarvana\nMikhail Dektiarev                                    Abhimanyu Goyal\nGolnaz Ghiasi                                        Samuel Andermatt\nErdem Guven                                          Patrick Siegler\nHimanshu Gupta                                       Ben Horn\nBoyi Liu                                             Assaf Israel\nDeepak Sharma                                        Francesco Pongetti\nIdan Heimlich Shtacher                               Chih-Wei \u201cLouis\u201d Chen\nShachi Paul                                          Marco Selvatici\nOscar Akerlund                                       Pedro Silva\nFran\u00e7ois-Xavier Aubet                                Kathie Wang\nTerry Huang                                          Jackson Tolins\nChen Zhu                                             Kelvin Guu\nEric Zhu                                             Roey Yogev\n\n\n                                                                                61", "mimetype": "text/plain", "start_char_idx": 26, "end_char_idx": 3472, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1f48f8cf-c1c9-4894-a4f2-9d08c2d7fb28": {"__data__": {"id_": "1f48f8cf-c1c9-4894-a4f2-9d08c2d7fb28", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f3510e33-2cc2-4a97-844a-bc18429c00ed", "node_type": "4", "metadata": {}, "hash": "7f87989d65d23da44ce0365e76e46f642540ea6db8c0522a2746a141fa7aa86a", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n Core Contributors                                      Core Contributors\n Xiaochen Cai                                            Sahitya Potluri\n Alessandro Agostini                                     Preethi Lahoti\n Maulik Shah                                            Cip Baetu\n Hung Nguyen                                            Ali Ghorbani\n Noah \u00d3 Donnaile                                        Charles Chen\n S\u00e9bastien Pereira                                      Andy Crawford\n Linda Friso                                             Shalini Pal\n Adam Stambler                                           Mukund Sridhar\n Adam Kurzrok                                            Petru Gurita\n Chenkai Kuang                                          Asier Mujika\n Yan Romanikhin                                          Igor Petrovski\n Mark Geller                                             Pierre-Louis Cedoz\n ZJ Yan                                                 Chenmei Li\n Kane Jang                                               Shiyuan Chen\n Cheng-Chun Lee                                          Niccol\u00f2 Dal Santo\nWojciech Fica                                            Siddharth Goyal\n Eric Malmi                                             Jitesh Punjabi\nQijun Tan                                                Karthik Kappaganthu\n Dan Banica                                             Chester Kwak\n Daniel Balle                                            Pallavi LV\n Ryan Pham                                               Sarmishta Velury\n Yanping Huang                                           Himadri Choudhury\n Diana Avram                                            Jamie Hall\n Hongzhi Shi                                             Premal Shah\nJasjot Singh                                             Ricardo Figueira\n Chris Hidey                                             Matt Thomas\n Niharika Ahuja                                          Minjie Lu\n Pranab Saxena                                          Ting Zhou\n Dan Dooley                                             Chintu Kumar\n Srividya Pranavi Potharaju                             Thomas Jurdi\n Eileen O\u2019Neill                                          Sharat Chikkerur\n Anand Gokulchandran                                    Yenai Ma\n Ryan Foley                                             Adams Yu\n Kai Zhao                                                Soo Kwak\n Mike Dusenberry                                        Victor \u00c4hdel\n Yuan Liu                                                Sujeevan Rajayogam\n Pulkit Mehta                                           Travis Choma\n Ragha Kotikalapudi                                      Fei Liu\n Chalence Safranek-Shrader                              Aditya Barua\n Andrew Goodman                                         Colin Ji\nJoshua Kessinger                                        Ji Ho Park\n Eran Globen                                            Vincent Hellendoorn\n Prateek Kolhar                                         Alex Bailey\n Chris Gorgolewski                                      Taylan Bilal\n Ali Ibrahim                                             Huanjie Zhou\n Yang Song                                               Mehrdad Khatir\n Ali Eichenbaum                                         Charles Sutton\n Thomas Brovelli                                        Wojciech Rzadkowski\n\n\n                                                                                   62", "mimetype": "text/plain", "start_char_idx": 29, "end_char_idx": 3613, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b3a75b14-d987-451a-a83c-94de4160180b": {"__data__": {"id_": "b3a75b14-d987-451a-a83c-94de4160180b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "328d4855-ff4b-412d-8c7c-19dface1a83c", "node_type": "4", "metadata": {}, "hash": "e6e8132f3077d0b231a3140851a58e00aea9b6d1e49fb8cee922d5ae3f19c20d", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\nCore Contributors                                    Contributors\nFiona Macintosh                                      Geoffrey Cideron\nKonstantin Shagin                                    Edouard Leurent\nPaul Medina                                          Mahmoud Alnahlawi\nChen Liang                                           Ionut Georgescu\nJinjing Zhou                                         Nan Wei\nPararth Shah                                         Ivy Zheng\nYingying Bi                                          Dylan Scandinaro\nAttila Dankovics                                     Heinrich Jiang\nShipra Banga                                         Jasper Snoek\nSabine Lehmann                                       Mukund Sundararajan\nMarissa Bredesen                                     Xuezhi Wang\nZifan Lin                                            Zack Ontiveros\nJohn Eric Hoffmann                                   Itay Karo\nJonathan Lai                                         Jeremy Cole\nRaynald Chung                                        Vinu Rajashekhar\nKai Yang                                             Lara Tumeh\nNihal Balani                                         Eyal Ben-David\nArthur Bra\u017einskas                                    Rishub Jain\nAndrei Sozanschi                                     Jonathan Uesato\nMatthew Hayes                                        Romina Datta\nH\u00e9ctor Fern\u00e1ndez Alcalde                             Oskar Bunyan\nPeter Makarov                                        Shimu Wu\nWill Chen                                            John Zhang\nAntonio Stella                                       Piotr Stanczyk\nLiselotte Snijders                                   Ye Zhang\nMichael Mandl                                        David Steiner\nAnte K\u00e4rrman                                         Subhajit Naskar\nPawe\u0142 Nowak                                          Michael Azzam\nXinyi Wu                                             Matthew Johnson\nAlex Dyck                                            Adam Paszke\nKrishnan Vaidyanathan                                Chung-Cheng Chiu\nRaghavender R                                        Jaume Sanchez Elias\nJessica Mallet                                       Afroz Mohiuddin\nMitch Rudominer                                      Faizan Muhammad\nEric Johnston                                        Jin Miao\nSushil Mittal                                        Andrew Lee\nAkhil Udathu                                         Nino Vieillard\nJanara Christensen                                   Jane Park\nVishal Verma                                         Jiageng Zhang\nZach Irving                                          Jeff Stanway\nAndreas Santucci                                     Drew Garmon\n                                                     Abhijit Karmarkar\nContributors                                         Zhe Dong\nGamaleldin Elsayed                                   Jong Lee\nElnaz Davoodi                                        Aviral Kumar\nMarin Georgiev                                       Luowei Zhou\nIan Tenney                                           Jonathan Evens\nNan Hua                                              William Isaac\n\n\n                                                                                63", "mimetype": "text/plain", "start_char_idx": 26, "end_char_idx": 3430, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "63ade1e8-a44f-4a97-9261-020a5452163f": {"__data__": {"id_": "63ade1e8-a44f-4a97-9261-020a5452163f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4a7b1260-6fda-4491-9b8f-f80504c9c6a5", "node_type": "4", "metadata": {}, "hash": "1da813e500d103a5ec552048d1d5e84cc516bb7c512e46219f96da3522dd3a62", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\nContributors                                     Contributors\nGeoffrey Irving                                  Ginger Perng\nEdward Loper                                     Elena Allica Abellan\nMichael Fink                                     Mingyang Zhang\nIsha Arkatkar                                    Ishita Dasgupta\nNanxin Chen                                      Nate Kushman\nIzhak Shafran                                    Ivo Penchev\nIvan Petrychenko                                 Alena Repina\nZhe Chen                                         Xihui Wu\nJohnson Jia                                      Tom van der Weide\nAnselm Levskaya                                  Priya Ponnapalli\nZhenkai Zhu                                      Caroline Kaplan\nPeter Grabowski                                  Jiri Simsa\nYu Mao                                           Shuangfeng Li\nAlberto Magni                                    Olivier Dousse\nKaisheng Yao                                     Fan Yang\nJavier Snaider                                   Jeff Piper\nNorman Casagrande                                Nathan Ie\nEvan Palmer                                      Rama Pasumarthi\nPaul Suganthan                                   Nathan Lintz\nAlfonso Casta\u00f1o                                  Anitha Vijayakumar\nIrene Giannoumis                                 Daniel Andor\nWooyeol Kim                                      Pedro Valenzuela\nMiko\u0142aj Rybi\u0144ski                                 Minnie Lui\nAshwin Sreevatsa                                 Cosmin Paduraru\nJennifer Prendki                                 Daiyi Peng\nDavid Soergel                                    Katherine Lee\nAdrian Goedeckemeyer                             Shuyuan Zhang\nWilli Gierke                                     Somer Greene\nMohsen Jafari                                    Duc Dung Nguyen\nMeenu Gaba                                       Paula Kurylowicz\nJeremy Wiesner                                   Cassidy Hardin\nDiana Gage Wright                                Lucas Dixon\nYawen Wei                                        Lili Janzer\nHarsha Vashisht                                  Kiam Choo\nYana Kulizhskaya                                 Ziqiang Feng\nJay Hoover                                       Biao Zhang\nMaigo Le                                         Achintya Singhal\nLu Li                                            Dayou Du\nChimezie Iwuanyanwu                              Dan McKinnon\nLu Liu                                           Natasha Antropova\nKevin Ramirez                                    Tolga Bolukbasi\nAndrey Khorlin                                   Orgad Keller\nAlbert Cui                                       David Reid\nTian LIN                                         Daniel Finchelstein\nMarcus Wu                                        Maria Abi Raad\nRicardo Aguilar                                  Remi Crocker\nKeith Pallo                                      Peter Hawkins\nAbhishek Chakladar                               Robert Dadashi\n\n\n                                                                            64", "mimetype": "text/plain", "start_char_idx": 22, "end_char_idx": 3241, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0f76c549-8d9e-4bdd-8582-ebbcc6c5e081": {"__data__": {"id_": "0f76c549-8d9e-4bdd-8582-ebbcc6c5e081", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7d09033f-a40d-48c9-acf0-d2f1b1a2f0e7", "node_type": "4", "metadata": {}, "hash": "70aaeadce716da1822956fa9883b4b15f44a83f0aa56ce2c3d7ef94f5a03f216", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\nContributors                                       Contributors\nColin Gaffney                                      Abhishek Sharma\nKen Franko                                         Kathy Wu\nAnna Bulanova                                      David Miller\nR\u00e9mi Leblond                                       Nicolas Sonnerat\nShirley Chung                                      Denis Vnukov\nHarry Askham                                       Rory Greig\nLuis C. Cobo                                       Jennifer Beattie\nKelvin Xu                                          Emily Caveness\nFelix Fischer                                      Libin Bai\nJun Xu                                             Julian Eisenschlos\nChristina Sorokin                                  Alex Korchemniy\nChris Alberti                                      Tomy Tsai\nChu-Cheng Lin                                      Mimi Jasarevic\nColin Evans                                        Weize Kong\nAlek Dimitriev                                     Phuong Dao\nHannah Forbes                                      Zeyu Zheng\nDylan Banarse                                      Frederick Liu\nZora Tung                                          Fan Yang\nMark Omernick                                      Rui Zhu\nColton Bishop                                      Tian Huey Teh\nRachel Sterneck                                    Jason Sanmiya\nRohan Jain                                         Evgeny Gladchenko\nJiawei Xia                                         Nejc Trdin\nEhsan Amid                                         Daniel Toyama\nFrancesco Piccinno                                 Evan Rosen\nXingyu Wang                                        Sasan Tavakkol\nPraseem Banzal                                     Linting Xue\nDaniel J. Mankowitz                                Chen Elkind\nAlex Polozov                                       Oliver Woodman\nVictoria Krakovna                                  John Carpenter\nSasha Brown                                        George Papamakarios\nMohammadHossein Bateni                             Rupert Kemp\nDennis Duan                                        Sushant Kafle\nVlad Firoiu                                        Tanya Grunina\nMeghana Thotakuri                                  Rishika Sinha\nTom Natan                                          Alice Talbert\nMatthieu Geist                                     Diane Wu\nSertan Girgin                                      Denese Owusu-Afriyie\nHui Li                                             Cosmo Du\nJiayu Ye                                           Chloe Thornton\nOfir Roval                                         Jordi Pont-Tuset\nReiko Tojo                                         Pradyumna Narayana\nMichael Kwong                                      Jing Li\nJames Lee-Thorp                                    Saaber Fatehi\nChristopher Yew                                    John Wieting\nDanila Sinopalnikov                                Omar Ajmeri\nSabela Ramos                                       Benigno Uria\nJohn Mellor                                        Yeongil Ko\n\n\n                                                                              65", "mimetype": "text/plain", "start_char_idx": 24, "end_char_idx": 3318, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7e2f039d-a900-4ff6-af89-118a66c56df8": {"__data__": {"id_": "7e2f039d-a900-4ff6-af89-118a66c56df8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fe00072c-38e6-421b-b227-1e1e369df698", "node_type": "4", "metadata": {}, "hash": "dcf215fdad0277a46af5f8484c1837fd660e4bc991b4a52feed181aa661928f3", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n Contributors\n Laura Knight\n Am\u00e9lie H\u00e9liou\n Ning Niu\n Shane Gu\n Chenxi Pang\n Yeqing Li\n Nir Levine\n Ariel Stolovich\n Rebeca Santamaria-Fernandez\n Sonam Goenka\nWenny Yustalim\n Robin Strudel\n Ali Elqursh\n Charlie Deck\n Hyo Lee\n Zonglin Li\n Kyle Levin\n Raphael Hoffmann\n Dan Holtmann-Rice\nOlivier Bachem\n Sho Arora\n Christy Koh\n Soheil Hassas Yeganeh\n Siim P\u00f5der\n Mukarram Tariq\n Yanhua Sun\n Lucian Ionita\n Mojtaba Seyedhosseini\n Pouya Tafti\n Zhiyu Liu\n Anmol Gulati\nJasmine Liu\n Xinyu Ye\n Bart Chrzaszcz\n Lily Wang\n Nikhil Sethi\n Tianrun Li\n Ben Brown\n Shreya Singh\nWei Fan\n Aaron Parisi\nJoe Stanton\n Vinod Koverkathu\n Christopher A. Choquette-Choo\n Yunjie Li\n TJ Lu\n Abe Ittycheriah\n Prakash Shroff\n\n\nContributors\n Mani Varadarajan\n Sanaz Bahargam\n Rob Willoughby\n David Gaddy\nGuillaume Desjardins\n Marco Cornero\n Brona Robenek\n Bhavishya Mittal\n Ben Albrecht\nAshish Shenoy\n Fedor Moiseev\n Henrik Jacobsson\nAlireza Ghaffarkhah\n Morgane Rivi\u00e8re\nAlanna Walton\nCl\u00e9ment Crepy\nAlicia Parrish\n Zongwei Zhou\nClement Farabet\nCarey Radebaugh\n Praveen Srinivasan\nClaudia van der Salm\nAndreas Fidjeland\n Salvatore Scellato\n Eri Latorre-Chimoto\n Hanna Klimczak-Pluci\u0144ska\n David Bridson\n Dario de Cesare\nTom Hudson\n Piermaria Mendolicchio\n Lexi Walker\nAlex Morris\n Matthew Mauger\nAlexey Guseynov\nAlison Reid\n Seth Odoom\n Lucia Loher\nVictor Cotruta\n Madhavi Yenugula\n Dominik Grewe\nAnastasia Petrushkina\nTom Duerig\nAntonio Sanchez\n Steve Yadlowsky\nAmy Shen\nAmir Globerson\n Lynette Webb\n Sahil Dua\n\n\n\n\n                                                                                      66", "mimetype": "text/plain", "start_char_idx": 32, "end_char_idx": 1661, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "99186704-d59b-46e2-969a-04f73bd20ccf": {"__data__": {"id_": "99186704-d59b-46e2-969a-04f73bd20ccf", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a94626ed-131f-4915-a415-69a37e82d0bd", "node_type": "4", "metadata": {}, "hash": "0b10c51cf2e5a329c4936cf733587036f416739eeaf93bf10805ca46731517e6", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\nContributors                                       Contributors\nDong Li                                            Yang Xu\nSurya Bhupatiraju                                  George Polovets\nDan Hurt                                           Ji Liu\nHaroon Qureshi                                     Honglong Cai\nAnanth Agarwal                                     Warren Chen\nTomer Shani                                        XiangHai Sheng\nMatan Eyal                                         Emily Xue\nAnuj Khare                                         Sherjil Ozair\nShreyas Rammohan Belle                             Christof Angermueller\nLei Wang                                           Xiaowei Li\nChetan Tekur                                       Anoop Sinha\nMihir Sanjay Kale                                  Weiren Wang\nJinliang Wei                                       Julia Wiesinger\nRuoxin Sang                                        Emmanouil Koukoumidis\nBrennan Saeta                                      Yuan Tian\nTyler Liechty                                      Anand Iyer\nYi Sun                                             Madhu Gurumurthy\nYao Zhao                                           Mark Goldenson\nStephan Lee                                        Parashar Shah\nPandu Nayak                                        MK Blake\nDoug Fritz                                         Hongkun Yu\nManish Reddy Vuyyuru                               Anthony Urbanowicz\nJohn Aslanides                                     Jennimaria Palomaki\nNidhi Vyas                                         Chrisantha Fernando\nMartin Wicke                                       Ken Durden\nXiao Ma                                            Harsh Mehta\nEvgenii Eltyshev                                   Nikola Momchev\nNina Martin                                        Elahe Rahimtoroghi\nHardie Cate                                        Maria Georgaki\nJames Manyika                                      Amit Raul\nKeyvan Amiri                                       Sebastian Ruder\nYelin Kim                                          Morgan Redshaw\nXi Xiong                                           Jinhyuk Lee\nKai Kang                                           Denny Zhou\nFlorian Luisier                                    Komal Jalan\nNilesh Tripuraneni                                 Dinghua Li\nDavid Madras                                       Blake Hechtman\nMandy Guo                                          Parker Schuh\nAustin Waters                                      Milad Nasr\nOliver Wang                                        Kieran Milan\nJoshua Ainslie                                     Vladimir Mikulik\nJason Baldridge                                    Juliana Franco\nHan Zhang                                          Tim Green\nGarima Pruthi                                      Nam Nguyen\nJakob Bauer                                        Joe Kelley\nFeng Yang                                          Aroma Mahendru\nRiham Mansour                                      Andrea Hu\nJason Gelman                                       Joshua Howland\n\n\n                                                                              67", "mimetype": "text/plain", "start_char_idx": 24, "end_char_idx": 3324, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "508d15f5-f4b6-4ab1-be16-fd2458bc0ad0": {"__data__": {"id_": "508d15f5-f4b6-4ab1-be16-fd2458bc0ad0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6246923a-2475-4964-aee1-f26ef4127963", "node_type": "4", "metadata": {}, "hash": "cecd6d5c5368e0fca0bb22d3185a5108bd1c16a7f34cae6bd4306e57204a9323", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n Contributors                                         Contributors\n Ben Vargas                                            Mathias Carlen\nJeffrey Hui                                           Jan van de Kerkhof\n Kshitij Bansal                                        Marcin Pikus\n Vikram Rao                                            Krunoslav Zaher\n Rakesh Ghiya                                          Paul M\u00fcller\n Emma Wang                                             Sasha Zykova\n Ke Ye                                                 Richard Stefanec\nJean Michel Sarr                                      Vitaly Gatsko\n Melanie Moranski Preston                             Christoph Hirnschall\n Madeleine Elish                                      Ashwin Sethi\n Steve Li                                             Xingyu Federico Xu\n Aakash Kaku                                          Chetan Ahuja\nJigar Gupta                                            Beth Tsai\n Ice Pasupat                                          Anca Stefanoiu\n Da-Cheng Juan                                         Bo Feng\n Milan Someswar                                        Keshav Dhandhania\n Tejvi M.                                              Manish Katyal\n Xinyun Chen                                          Akshay Gupta\n Aida Amini                                           Atharva Parulekar\n Alex Fabrikant                                        Divya Pitta\n Eric Chu                                             Jing Zhao\n Xuanyi Dong                                          Vivaan Bhatia\n Amruta Muthal                                        Yashodha Bhavnani\n Senaka Buthpitiya                                    Omar Alhadlaq\n Sarthak Jauhari                                      Xiaolin Li\n Nan Hua                                               Peter Danenberg\n Urvashi Khandelwal                                    Dennis Tu\n Ayal Hitron                                          Alex Pine\nJie Ren                                               Vera Filippova\n Larissa Rinaldi                                      Abhipso Ghosh\n Shahar Drath                                          Ben Limonchik\n Avigail Dabush                                        Bhargava Urala\n Nan-Jiang Jiang                                      Chaitanya Krishna Lanka\n Harshal Godhia                                        Derik Clive\n Uli Sachs                                            Yi Sun\n Anthony Chen                                          Edward Li\n Yicheng Fan                                           Hao Wu\n Hagai Taitelbaum                                      Kevin Hongtongsak\n Hila Noga                                             Ianna Li\n Zhuyun Dai                                            Kalind Thakkar\nJames Wang                                             Kuanysh Omarov\n Chen Liang                                            Kushal Majmundar\nJenny Hamer                                            Michael Alverson\n Chun-Sung Ferng                                       Michael Kucharski\n Chenel Elkind                                         Mohak Patel\n Aviel Atias                                           Mudit Jain\n Paulina Lee                                           Maksim Zabelin\n V\u00edt List\u00edk                                            Paolo Pelagatti\n\n\n                                                                                 68", "mimetype": "text/plain", "start_char_idx": 27, "end_char_idx": 3532, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b32fba8b-4628-455b-a018-5970a5de131e": {"__data__": {"id_": "b32fba8b-4628-455b-a018-5970a5de131e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "904ce7da-b0b8-4868-94d1-fde93db79fa0", "node_type": "4", "metadata": {}, "hash": "342418a5b87284fefb0723cbf26243ff8f04455bcd17d41d16cb0644576f038f", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\nContributors                                                      Gemini App Program Leads\nRohan Kohli                                                       Amar Subramanya7\nSaurabh Kumar                                                      Sissie Hsiao\nJoseph Kim\nSwetha Sankar                                                     Gemini Program Leads\nVineet Shah                                                        Demis Hassabis\nLakshmi Ramachandruni                                              Koray Kavukcuoglu\nXiangkai Zeng\nBen Bariach                                                       Overall Gemini App Technical Leads\nLaura Weidinger                                                   Adam Sadovsky8\nTu Vu                                                             Quoc Le\nAlek Andreev                                                      Trevor Strohman9\nAntoine He                                                        Yonghui Wu10\nKevin Hui\nSheleem Kashem                                                    Overall Gemini Post-Training Lead\n                                                                   Slav Petrov\n\n\n                                                                  Overall Gemini Technical Leads (equal con-\n                                                                   tribution)\n                                                                  Jeffrey Dean\n                                                                  Oriol Vinyals\n\n\nThe roles are defined as below:\n\n\n    \u2022 Lead: Individual(s) responsible for the sub-team throughout the project.\n    \u2022 Core Contributor: Individual that had significant impact throughout the project.\n    \u2022 Contributor: Individual that had contributions to the project and was partially involved with the\n        effort.\n    \u2022 Program Lead: Responsible for the organizational aspects of the Gemini effort.\n    \u2022 Overall Post-Training Lead: Responsible for the technical direction of post-training.\n    \u2022 Overall Technical Lead: Responsible for the technical direction of the overall Gemini effort.\n\n\n     Within each role, contributions are equal, and are listed in a randomized order. Ordering within\neach role does not indicate ordering of the contributions.\n     Gemini is a cross-Google effort, with members from Google DeepMind (GDM), Google Research\n(GR), Bard/Assistant, Knowledge and Information (K&I), Core ML, Cloud, Labs, and more.\n     We thank Aakanksha Chowdhery, Dustin Tran, Heng-Tze Cheng, Jack W. Rae, Kate Olszewska,\nMariko Iinuma, Peter Humphreys, Shashi Narayan, and Steven Zheng for leading the preparation of\nthis report. We also thank our reviewers and colleagues for their valuable discussions and feedback\non the report \u2014 Alexandra Belias, Ana Ramalho, Anand Rao, Arielle Bier, Danielle Landress, Eleanor\nTomlinson, Emily Hossellman, Gaby Pearl, Helen King, Hollie Dobson, Jaclyn Konzelmann, Jennifer\nBeroshi, Joel Moss, Jon Small, Jonathan Fildes, Kathy Meier-Hellstern, Lisa Patel, Oli Gaymond,\nRebecca Bland, Reena Jana, Tessa Lueth, and Tom Lue.\n   7Lead, Gemini App Engineering\n   8Lead, Gemini App Core Modeling, Eval, Data\n   9Co-Lead, Gemini App Serving\n  10Co-Lead, Gemini Text\n\n\n                                                                                                              69", "mimetype": "text/plain", "start_char_idx": 38, "end_char_idx": 3409, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3fbfed0c-2872-4652-8e15-8aa2022ebe8b": {"__data__": {"id_": "3fbfed0c-2872-4652-8e15-8aa2022ebe8b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eb2353d6-6068-4020-a3b8-39bec5277a38", "node_type": "4", "metadata": {}, "hash": "ceff039c6cf053377b604c476400b1f79e9fc9a2a50c1514091667455a9b8f27", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b60a3a1a-444c-469f-8275-d9a83bc291ec", "node_type": "1", "metadata": {}, "hash": "6e3b1fba5add8164ffac5392ec135638a2d386cf225692b8c3fb67cd878763d2", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models", "mimetype": "text/plain", "start_char_idx": 38, "end_char_idx": 90, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b60a3a1a-444c-469f-8275-d9a83bc291ec": {"__data__": {"id_": "b60a3a1a-444c-469f-8275-d9a83bc291ec", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eb2353d6-6068-4020-a3b8-39bec5277a38", "node_type": "4", "metadata": {}, "hash": "ceff039c6cf053377b604c476400b1f79e9fc9a2a50c1514091667455a9b8f27", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3fbfed0c-2872-4652-8e15-8aa2022ebe8b", "node_type": "1", "metadata": {}, "hash": "fb4e89ac8ec946dd401d157e8e08625d0cad1cad8b5c2494da0d87b1281776b3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "450ee741-1551-4144-916a-6dc3336cd4f0", "node_type": "1", "metadata": {}, "hash": "82f07092403ceb1bdc2ed0cd4843ce675ca11275745cc166b1cc3cfa5034ab1a", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n     Our work is made possible by the dedication and efforts of numerous teams at Google. We would\nlike to acknowledge the support from Abhi Mohan, Adekunle Bello, Aishwarya Nagarajan, Alaa\nSaade, Alejandro Lince, Alexander Chen, Alexander Kolbasov, Alexander Schiffhauer, Ameya Shringi,\nAmin Vahdat, Anda Rabati\u0107, Anthonie Gross, Antoine Yang, Anthony Green, Anton Ruddock, Art\nKhurshudov, Artemis Chen, Arthur Argenson, Avinatan Hassidim, Beiye Liu, Benjamin Schroeder,\nBin Ni, Brett Daw, Bryan Chiang, Burak Gokturk, Carl Crous, Carrie Grimes Bostock, Charbel Kaed,\nCharlotte Banks, Che Diaz, Chris Larkin, Christy Lian, Claire Cui, Clare Bycroft, Corentin Tallec,\nDaniel Herndon, Dave Burke, David Battle, David Engel, Dipannita Shaw, Donghyun Koo, Doug\nRitchie, Dragos Stefanescu, Elissa Wolf, Emre Sargin, Eric Herren, Estella King, Fatema Alkhanaizi,\nFelix Gimeno, Fernando Pereira, Florent Altch\u00e9, Gabriel Carvajal, Gaurav Gandhi, George Powell,\nGoran Pavi\u010di\u0107, Harry Richardson, Hassan Wassel, Hongji Li, Idan Szpektor, Igor Ivanisevic, Ivan\nJambre\u0161i\u0107, Ivan Jurin, Jade Fowler, James Assiene, Jay Yagnik, Jean-bastien Grill, Jeff Seibert, Jenna\nLaPlante, Jessica Austin, Jianxing Lu, Jim O\u2019Keeffe, Jin Huang, Joe Heyward, Johannes Welbl, John\nJumper, Jonathan Caton, Josh Woodward, Joshua Foster, Kathryn Tunyasuvunakool, Katrina Wong,\nKavya Kopparapu, Kelvin Nguyen, Kira Yin, Konstantin Sharlaimov, Kun Li, Lee Hong, Lilly Taylor,\nLongfei Shen, Luc Mercier, Maciej Miku\u0142a, Mania Abdi, Manuel Sanchez, Maria Ines Aranguren,\nMario Carlos Cortes III, Matthew Tait, Matthias Lochbrunner, Mehdi Ghissassi, Micah Mosley, Michael\nBendersky, Michael Figurnov, Michael Harris, Michael Mathieu, Michael O\u2019Neill, Michael Vorburger,\nMihir Paradkar, Nandita Dukkipati, Nathan Carter, Nathan Watson, Neil Rabinowitz, Nikhil Dandekar,\nNishant Ranka, Olcan Sercinoglu, Olivier Lacombe, Ottavia Bertolli, Paul Caron, Pranesh Srinivasan,\nPraveen Kumar, Rahul Sukthankar, Raia Hadsell, Rajagopal Ananthanarayanan, Roberto Lupi, Rosie\nZou, Sachin Menezes, Sadegh Jazayeri, Sam Cheung, Sameer Bidichandani, Sania Alex, Sanjiv\nKumar, Sara Wiltberger, Sarah Fitzgerald, Saz Basu, Sebastian Nowozin, Shannon Hepburn, Shayne\nCardwell,Srinivasan Venkatachary, Sugato Basu, Sundar Pichai, Sundeep Tirumalareddy, Susannah\nYoung, Swetha Vijayaraghavan, Tania Bedrax-Weiss, Taylor Applebaum, Teiva Harsanyi, Terry Chen,\nTim Blyth, Ting Liu, Tom Cobley, Tomas Izo, Trystan Upstill, Varun Singhai, Vedrana Klari\u0107 Trup\u010devi\u0107,\nVictor Cai, Vladimir Pudovkin, Vu Dang, Wenbo Zhao, Wesley Crow, Wesley Szeng, Xiaodan Song,\nYazhou Zu, Ye Tian, Yicong Wang, Yixing Wang, Yossi Matias, Yunlong Jiao, Zachary Jessup, Zhenchuan\nPang, \u017diga Avsec, Zimeng Yang, and Zoubin Ghahramani. We\u2019d also like to recognize the AlphaCode\nteam, the Borg Scheduling team, the Facilities team, the Gemini Demo Team, the Global Server Ops\n(GSO) team, the JAX team, the the Legal team, ML SRE team, the ML Supercomputer (MLSC) team,\nthe PartIR team, the Platforms Infrastructure Engineering (PIE) team, and the XLA Compiler team.\n     We thank everyone at Google not explicitly mentioned above, who have shared excitement, given\nfeedback on early Gemini models or created interesting demo uses of Gemini, and worked with or\nsupported the core Gemini team on many aspects of this project.", "mimetype": "text/plain", "start_char_idx": 38, "end_char_idx": 3428, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "450ee741-1551-4144-916a-6dc3336cd4f0": {"__data__": {"id_": "450ee741-1551-4144-916a-6dc3336cd4f0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eb2353d6-6068-4020-a3b8-39bec5277a38", "node_type": "4", "metadata": {}, "hash": "ceff039c6cf053377b604c476400b1f79e9fc9a2a50c1514091667455a9b8f27", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b60a3a1a-444c-469f-8275-d9a83bc291ec", "node_type": "1", "metadata": {}, "hash": "6e3b1fba5add8164ffac5392ec135638a2d386cf225692b8c3fb67cd878763d2", "class_name": "RelatedNodeInfo"}}, "text": "70", "mimetype": "text/plain", "start_char_idx": 3535, "end_char_idx": 3537, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "19eaad7e-4adb-4906-aa3d-ca9e06e0624a": {"__data__": {"id_": "19eaad7e-4adb-4906-aa3d-ca9e06e0624a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "91e47bab-e29c-442a-8ea2-6c8a04a5b3e0", "node_type": "4", "metadata": {}, "hash": "bc6b6a34b9fafa867796085261c56ca44924c4ede4883e2375301fd9646265b0", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n10. Appendix\n\n\n10.1. Gemini Ultra Model Card\n\n\n                                              Model summary\n\n\n     Model architecture             Gemini V1.0 is a new family of state-of-the-art language models,\n                                    containing variants known as Nano, Pro and Ultra (ordered\n                                    by parameter count) based on a decoder-only Transformer\n                                    architecture (Vaswani et al., 2017a). Models are trained to\n                                    support 32K context length, employing efficient attention\n                                    mechanisms such as multi-query attention (Shazeer, 2019b).\n                                    Gemini is trained jointly across image, audio, video and text\n                                    data for the purpose of building a model with both strong\n                                    generalist capabilities across modalities alongside cutting-edge\n                                    understanding and reasoning performance in each respective\n                                    domain.\n\n\n                                    The post-trained models described in this model card\n                                    are Gemini API and Gemini Apps model variants (Section 6)\n                                    built on top of the Gemini Ultra pre-trained model. During the\n                                    post-training process, additional architectural modifications are\n                                    also made to support the training of multi-objective reward\n                                    models for RLHF.\n     Input(s)                       Text (e.g. a question, a prompt, a document(s) to be summa-\n                                    rized), images, video, audio files.\n     Output(s)                      Generated text in response to the input (e.g. an answer to\n                                    the question, a summary of multiple documents, comparing\n                                    documents/videos).\n                                                     Usage\n     Application                    Gemini is designed for accelerating research on language\n                                    models, for use as a building block in features within Google\n                                    products, and as a building block for select applications such as\n                                    Gemini App and Search Generative Experience.\n\n\n                                    Services and products built on top of Gemini Ultra are\n                                    also being made available to external developers via Google\n                                    Cloud Vertex API and Google Labs, with additional process and\n                                    technical safeguards related to safety policies.\n     Known Caveats                  Gemini should not be made available as part of a general-purpose\n                                    service or product, or used within a specific downstream appli-\n                                    cation without a prior assessment and mitigation of the safety\n                                    and fairness concerns specific to the downstream use.\n\n\n                                                                                                       71", "mimetype": "text/plain", "start_char_idx": 31, "end_char_idx": 3447, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1def6a21-fb81-4d1b-be77-52031860a154": {"__data__": {"id_": "1def6a21-fb81-4d1b-be77-52031860a154", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6e61ea38-6e41-404d-a29b-e29f32c1a28c", "node_type": "4", "metadata": {}, "hash": "a328037f27fa35adbcd4090d2745bd8c973ad73ca2c8b941d576bf933c1b6668", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n                             Implementation Frameworks\nHardware & Software        Hardware: Training was conducted on TPUv4 and TPUv5e\n                           (Jouppi et al., 2020, 2023).\n\n\n                           Software:        JAX (Bradbury et al., 2018), ML Pathways\n                           (Dean, 2021).\n\n\n                           JAX allows researchers to leverage the latest generation\n                           of hardware, including TPUs, for faster and more efficient\n                           training of large models.\n\n\n                           ML Pathways is infrastructure software to support Google\u2019s\n                           efforts to build artificially intelligent systems capable of\n                           generalizing across multiple tasks. This is specially suitable for\n                           foundation models, including large language models like the\n                           Gemini V1.0 models.\n\n\n                           Together, JAX and ML Pathways are used as described in\n                           Section 3.       The \u2019single controller\u2019 programming model of\n                           JAX and ML Pathways allows a single Python process to\n                           orchestrate the entire training run, dramatically simplifying the\n                           development workflow.\nCompute Requirements       Not reported.\n                                 Model Characteristics\nModel initialization       Initial pretraining used random initialization. Post-training was\n                           initialized from checkpoints obtained at the later stages of pre-\n                           training. These checkpoints were fine-tuned using supervised\n                           fine-tuning, and subsequently used to initialize reward model\n                           training and RLHF.\nModel Status               This is a static model trained on an offline dataset.\nModel Stats                Not reported.\n                                      Data overview\nTraining Dataset           Gemini models are trained on a dataset that is both multimodal\n                           and multilingual. Our pre-training dataset uses data from web\n                           documents, books, and code, and includes image, audio, and\n                           video data.\n\n\n                           Refer to Section 4 (Pre-Training Dataset) for further de-\n                           tails.\n\n\n                                                                                               72", "mimetype": "text/plain", "start_char_idx": 22, "end_char_idx": 2624, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5e6d7c60-4ae6-47f8-9d8e-6851ae109a65": {"__data__": {"id_": "5e6d7c60-4ae6-47f8-9d8e-6851ae109a65", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0445e872-d5a5-44f7-adb6-a9b485adb1c5", "node_type": "4", "metadata": {}, "hash": "29adf77b06e750ca74922910de566c073390863908cc9800ff75e67456ccb6c3", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n     Evaluation Dataset                   We compare pre- and post-trained Gemini Ultra models to a\n                                          suite of external LLMs and our previous best model PaLM 2\n                                          across a series of text-based academic benchmarks covering\n                                          reasoning, reading comprehension, STEM, and coding.\n\n\n                                          We also evaluate Gemini models on four different mul-\n                                          timodal capabilities:           high-level object recognition using\n                                          captioning or question-answering tasks such as VQAv2; fine-\n                                          grained transcription using tasks such as TextVQA and DocVQA\n                                          requiring the model to recognize low-level details; chart\n                                          understanding requiring spatial understanding of input layout\n                                          using ChartQA and InfographicVQA tasks; and multimodal\n                                          reasoning using tasks such as Ai2D, MathVista and MMMU.\n\n\n                                          Refer to Section 5 (Evaluation) for further details.\n     Post-training Dataset                For post-training, we first collect a diverse set of prompts that\n                                          are representative of real-world use cases. We then collect\n                                          demonstration data of what the model\u2019s output should be for\n                                          a given prompt for supervised fine-tuning. We further collect\n                                          different possible responses to a given prompt, and collect\n                                          feedback data over these to train reward models.\n\n\n                                          Refer to Section 6.3 (Post-Training Methods and Data)\n                                          for further details.\n                                                   Evaluation Results\n     Benchmark Information                See Section 5 (Evaluation).\n     Evaluation Results                   See Section 5 (Evaluation) and Section 6.4 (Post-Training Hu-\n                                          man Evaluation).\n                                             Model Usage & Limitations\n     Sensitive Use                        For an analysis of risks and sensitive uses associated with the\n                                          Gemini models, see Section 7.1 (Impact Assessment).\n     Known Limitations                    Gemini models can exhibit limitations outlined in Section 7.1\n                                          (Impact Assessment). Gemini models should not be used for\n                                          downstream applications without further analysis of potential\n                                          harm in the proposed downstream application.\n     Ethical Considerations &             A reflection on the potential risks and impacts of the Gemini V1.0\n     Risks                                models can be found in Section 7 (Responsible Deployment).\n                                          For evaluation details for a range of risks, see Section 7.4 (Safety\n                                          Evaluations).\n\n\n10.2. Chain-of-Thought Comparisons on MMLU benchmark\n\n\nWe contrast several chain-of-thought approaches on MMLU and discuss their results in this section. We\nproposed a new approach where model produces k chain-of-thought samples, selects the majority vote\nif the model is confident above a threshold, and otherwise defers to the greedy sample choice. The\n\n\n                                                                                                                73", "mimetype": "text/plain", "start_char_idx": 37, "end_char_idx": 3985, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1bea6dea-7672-4b13-9450-4f0307132515": {"__data__": {"id_": "1bea6dea-7672-4b13-9450-4f0307132515", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "32990677-c8e5-47f3-8b69-e2c1e5c81d12", "node_type": "4", "metadata": {}, "hash": "45e6b5778165a0037ea3bc1bec456ef2ceeda3f3082f0a4df48ca889b750dee2", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\nthresholds are optimized for each model based on their validation split performance. The proposed\napproach is referred to as uncertainty-routed chain-of-thought. The intuition behind this approach\nis that chain-of-thought samples might degrade performance compared to the maximum-likelihoodMMLU accuracy (test split)decision when the model is demonstrably inconsistent. We compare the gains from the proposed\napproach on both Gemini Ultra and GPT-4 in Figure 9. We find that Gemini Ultra benefits more from\nthis approach compared to using only chain-of-thought samples. GPT-4\u2019s performance improves from\n84.2% with greedy sampling to 87.3% with uncertainty-routed chain-of-thought approach with 32\nsamples, but it already achieves these gains from using 32 chain-of-thought samples. In contrast,\nGemini Ultra improves its performance significantly from 84.0% with greedy sampling to 90.0% with\nuncertainty-routed chain-of-thought approach with 32 samples while it marginally improves to 85.0%\nwith the use of 32 chain-of-thought samples only.\n\n\n                                                  GPT-4 (gpt-4-0613)       Gemini Ultra\n\n\n                     90                                                                                 90.04\n                              84.21                          87.29       84.99              87.29\n                     80                   83.96\n\n\n                     70\n\n\n                     60\n\n\n                     50\n\n\n                     40\n\n\n                     30\n\n\n                     20\n\n\n                     10\n\n\n                      0           Score Eval               Chain-of-Thought@32            Chain-of-Thought@32\n                                                                                           (Uncertainty-Routed)\n\n\nFigure 9 | Chain-of-Thought with uncertainty routing on MMLU.\n\n\n                                                                                                                                   74", "mimetype": "text/plain", "start_char_idx": 39, "end_char_idx": 2090, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d9bb9521-66e5-4319-a7ed-c9662e4ec9ff": {"__data__": {"id_": "d9bb9521-66e5-4319-a7ed-c9662e4ec9ff", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db062d8f-37ef-475c-81b7-c2b03c8bfbd9", "node_type": "4", "metadata": {}, "hash": "269e97a3faac60dcfde1406bdb6a60bc17345afcc37731fd31905956b243df97", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "147576a4-20a0-4732-98d0-a3911b13934e", "node_type": "1", "metadata": {}, "hash": "5cc0da81f7e2008e92d36e71d70204f309cc3f08af32ecf5042f2c30bc942318", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n10.3. Capabilities and Benchmarking Tasks\n\n\nWe use more than 50 benchmarks as a holistic harness to evaluate the Gemini models across text,\nimage, audio and video. We provide a detailed list of benchmarking tasks for six different capabilities in\ntext understanding and generation: factuality, long context, math/science, reasoning, summarization,\nand multilinguality. We also enumerate the benchmarks used for image understanding, video\nunderstanding, and audio understanding tasks.", "mimetype": "text/plain", "start_char_idx": 37, "end_char_idx": 575, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "147576a4-20a0-4732-98d0-a3911b13934e": {"__data__": {"id_": "147576a4-20a0-4732-98d0-a3911b13934e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db062d8f-37ef-475c-81b7-c2b03c8bfbd9", "node_type": "4", "metadata": {}, "hash": "269e97a3faac60dcfde1406bdb6a60bc17345afcc37731fd31905956b243df97", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d9bb9521-66e5-4319-a7ed-c9662e4ec9ff", "node_type": "1", "metadata": {}, "hash": "dbc5480a0b0bf9d0596c8f23833c0ea8814a47d63914a06cb599dd36fe4348ef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "370dc4b6-b257-4c73-b0ea-31287d744a1a", "node_type": "1", "metadata": {}, "hash": "31f7d763df19d15b89ccb023698643faf5039ea5fc00bb0beef77c14dde030af", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n10.3. Capabilities and Benchmarking Tasks\n\n\nWe use more than 50 benchmarks as a holistic harness to evaluate the Gemini models across text,\nimage, audio and video. We provide a detailed list of benchmarking tasks for six different capabilities in\ntext understanding and generation: factuality, long context, math/science, reasoning, summarization,\nand multilinguality. We also enumerate the benchmarks used for image understanding, video\nunderstanding, and audio understanding tasks.\n\n\n    \u2022 Factuality: We use 5 benchmarks: BoolQ (Clark et al., 2019), NaturalQuestions-Closed\n       (Kwiatkowski et al., 2019a), NaturalQuestions-Retrieved (Kwiatkowski et al., 2019a), Real-\n       timeQA (Kasai et al., 2022b), TydiQA-noContext and TydiQA-goldP (Clark et al., 2020).\n    \u2022 Long Context: We use 6 benchmarks: NarrativeQA (Ko\u010disk\u00fd et al., 2018), Scrolls-Qasper,\n       Scrolls-Quality (Shaham et al., 2022), XLsum (En), XLSum (non-English languages) (Hasan\n       et al., 2021), and one other internal benchmark.\n    \u2022 Math/Science: We use 8 benchmarks: GSM8k (with CoT) (Cobbe et al., 2021), Hendryck\u2019s\n       MATH pass@1 (Hendrycks et al., 2021b), MMLU (Hendrycks et al., 2021a), Math-StackExchange,\n       Math-AMC 2022-2023 problems, and three other internal benchmarks.\n    \u2022 Reasoning: We use 7 benchmarks: BigBench Hard (with CoT) (Srivastava et al., 2022; Suzgun\n       et al., 2022), CLRS (Veli\u010dkovi\u0107 et al., 2022), Proof Writer (Tafjord et al., 2020), Reasoning-Fermi\n       problems (Kalyan et al., 2021), Lambada (Paperno et al., 2016), HellaSwag (Zellers et al.,\n       2019), DROP (Dua et al., 2019).\n    \u2022 Summarization: We use 5 benchmarks: XL Sum (English), XL Sum (non-English languages)\n       (Hasan et al., 2021), WikiLingua (non-English languages), WikiLingua (English) (Ladhak et al.,\n       2020), XSum (Narayan et al., 2018).\n    \u2022 Multilinguality: We use 10 benchmarks: XLSum (Non-English languages) (Hasan et al., 2021),\n       WMT22 (Kocmi et al., 2022), WMT23 (Tom et al., 2023), FRMT (Riley et al., 2023), WikiLingua\n       (Non-English languages) (Ladhak et al., 2020), TydiQA (no context), TydiQA (GoldP) (Clark\n       et al., 2020), MGSM (Shi et al., 2023), translated MMLU (Hendrycks et al., 2021a), NTREX\n       (Federmann et al., 2022), FLORES-200 (Team et al., 2022).\n    \u2022 Image and Video: We use 9 benchmarks for image understanding: MMMU (Yue et al., 2023),\n       TextVQA (Singh et al., 2019), DocVQA (Mathew et al., 2021), ChartQA (Masry et al., 2022),\n       InfographicVQA (Mathew et al., 2022), MathVista (Lu et al., 2023), AI2D (Kembhavi et al.,\n       2016), VQAv2 (Goyal et al., 2017), XM3600 (Thapliyal et al., 2022) for multi-lingual image\n       understanding, and 6 benchmarks for video understanding: VATEX (Wang et al., 2019) for\n       captioning in two different languages, YouCook2 (Zhou et al., 2018), NextQA (Xiao et al.,\n       2021), ActivityNet-QA (Yu et al., 2019), and Perception Test MCQA (P\u0103tr\u0103ucean et al., 2023).\n    \u2022 Audio: We use 5 benchmarks including automatic speech recognition (ASR) tasks such as\n       FLEURS (Conneau et al., 2023), VoxPopuli (Wang et al., 2021), Multi-lingual Librispeech (Pratap\n       et al., 2020), and automatic speech translation task such as CoVoST 2 (Wang et al., 2020).", "mimetype": "text/plain", "start_char_idx": 37, "end_char_idx": 3364, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "370dc4b6-b257-4c73-b0ea-31287d744a1a": {"__data__": {"id_": "370dc4b6-b257-4c73-b0ea-31287d744a1a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db062d8f-37ef-475c-81b7-c2b03c8bfbd9", "node_type": "4", "metadata": {}, "hash": "269e97a3faac60dcfde1406bdb6a60bc17345afcc37731fd31905956b243df97", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "147576a4-20a0-4732-98d0-a3911b13934e", "node_type": "1", "metadata": {}, "hash": "5cc0da81f7e2008e92d36e71d70204f309cc3f08af32ecf5042f2c30bc942318", "class_name": "RelatedNodeInfo"}}, "text": "75", "mimetype": "text/plain", "start_char_idx": 3475, "end_char_idx": 3477, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3af8814e-fd29-4947-ab07-75fb2a1c4361": {"__data__": {"id_": "3af8814e-fd29-4947-ab07-75fb2a1c4361", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "400d7fed-e6c7-483b-81fa-02aca53ef457", "node_type": "4", "metadata": {}, "hash": "3a0c13d9480e3fcaac525bb2ed935765f38a28ae01fdc603f14d32672f299658", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n 10.4. Qualitative Examples\n\n\n This section shows sample qualitative examples from prompting the Gemini Ultra model. Some\n illustrative examples of multimodal reasoning for image understanding tasks over charts, natural\n images and memes are shown in Figures 10, 11, 13, 15, 16, and 17. Figure 12 shows an example of\n image generation capabilities of Gemini Ultra where the user generates an interleaved sequence of\n image and text to design a blog post. Beyond English, Figure 18 shows model\u2019s capability to understand\n images in a multilingual setting. Gemini models also show strong performance on multimodal image\n understanding and reasoning in mathematics, as shown in Figures 14, 20 and 21. Figure 22 is an\n example of complex multimodal reasoning demonstrating how the model composes complex image\n understanding, code generation, and instruction following capabilities for a given user task. In Figure\n19, we see another example of the model being able to generate working code and follow complex\n user instructions. Finally, Figure 23 shows an example of Gemini Ultra\u2019s capability of understanding\n video by reasoning over temporally connected set of frames.\n\n\n                                                                                                          76", "mimetype": "text/plain", "start_char_idx": 38, "end_char_idx": 1372, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6700d6e-eee8-45d0-befb-da13d3e1b45e": {"__data__": {"id_": "e6700d6e-eee8-45d0-befb-da13d3e1b45e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c3840f64-daa1-4d91-a3d4-2ed3598cd8e1", "node_type": "4", "metadata": {}, "hash": "51ab061f94e21a5bdbf3118d38e11763108efe64dafa54b96abbe769aa22b6c2", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n10.4.1. Chart understanding and reasoning over data\n    Prompt\n\n\n    Spot a data point that stands out in these charts and what that implicates. Then produce a\n    detailed markdown table for all the data shown.\n\n\n    Model Response (rendered Markdown)\n\n\n    The data point that stands out is the United States\u2019 landfilled plastic waste, which is 73%. This\n    is significantly higher than the other countries and regions shown. This implies that the United\n    States is not doing as much to recycle or incinerate its plastic waste as other countries and\n    regions.\n\n\n      Country/Region            Landfilled (%)    Mismanaged (%)        Incinerated (%)     Recycled (%)\n      World                           49                  22                   19                 9\n      United States                   73                   4                   19                 4\n      Europe                          44                   6                   38                12\n      Asia (excl. China and           39                  34                   19                 8\n      India)\n\n\nFigure 10 | Solving a problem requiring multimodal chart understanding.The model has to read the\ntext, understand the connections between different data points and reason over them to recommend\nan interesting point and follow the instructions to generate a markdown table (shown correctly\nrendered).\nSource: Our World In Data (Ritchie et al., 2023).\n\n\n                                                                                                          77", "mimetype": "text/plain", "start_char_idx": 39, "end_char_idx": 1645, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ddd4d91a-8aac-4787-b444-299cb1ccd4c8": {"__data__": {"id_": "ddd4d91a-8aac-4787-b444-299cb1ccd4c8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b3921a53-3544-4f8a-8879-850762fee430", "node_type": "4", "metadata": {}, "hash": "bd659761c1b71945022f4a3688cf8893f3847d1f2e561469abe4a0a269b561c5", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n10.4.2. Multimodal question answering\n    Prompt\n\n\n    Do you know what it this plant? How do I best take care of it?\n\n\n    Model Response\n\n\n    This is a Persian shield plant. It is a tropical plant that is native to Southeast Asia. It is a\n    popular houseplant because it is easy to care for and has beautiful purple leaves.\n\n\n    Persian shield plants prefer bright, indirect light. They can tolerate some direct sunlight, but\n    too much direct sunlight can scorch their leaves. They also prefer warm temperatures and\n    high humidity.\n\n\n    Persian shield plants need to be watered regularly, but they should not be allowed to sit in\n    water. The soil should be kept moist but not soggy. They also benefit from being misted regularly.\n\n\n    Persian shield plants can be fertilized every two weeks during the growing season. A balanced\n    fertilizer is a good choice.\n\n\n    Persian shield plants are susceptible to pests, such as mealybugs and spider mites. These pests\n    can be controlled with insecticidal soap or neem oil.\n\n\n    Persian shield plants can be propagated by stem cuttings. The cuttings should be taken in the\n    spring or summer and should be rooted in water or a moist potting mix.\n\n\nFigure 11 | Answering a multimodal information-seeking query. The model is able to recognize the\nspecific plant shown in the image and provide information about it. The model shows robustness to\ntypos as it is able to understand the user question despite them.\nSource: photo taken by an author from the Gemini team.\n\n\n                                                                                                        78", "mimetype": "text/plain", "start_char_idx": 39, "end_char_idx": 1734, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ced501ab-d2ce-45ed-b5f0-509788456ed3": {"__data__": {"id_": "ced501ab-d2ce-45ed-b5f0-509788456ed3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "225c8537-a293-405a-92d6-25cd615e1d8c", "node_type": "4", "metadata": {}, "hash": "1e18830da576fe2d9f3b48f8f9e384897749fc246f1d67c91881708f01716176", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n10.4.3. Interleaved image and text generation\n    Prompt\n\n\n    Please create a blog post about a trip to New York, where a dog and his owner had lots of fun.\n    Include a few pictures of the dog posing happily at different landmarks.\n\n\n    Model Response\n\n\n    Hi, I\u2019m Cooper. I\u2019m a 2 year old Goldendoodle who loves to travel. Last month, my family and\n    I went to New York City. It was my first time in the Big Apple, and I had a blast!\n\n\n    We stayed in a hotel near Times Square. It was so much fun to see all the bright lights and\n    people. We went to see a Broadway show, and I even got to meet some of the actors.\n\n\n    One of my favorite parts of New York City was Central Park. It was so big and beautiful. I\n    loved running around and playing fetch. We also went to the Statue of Liberty and the Empire\n    State Building. I was so excited to see all the famous landmarks.\n\n\n    I had so much fun in New York City. It\u2019s definitely one of my favorite places to travel. I can\u2019t\n    wait to go back someday!\n\n\nFigure 12 | Generating interleaved text and images. The model is able to follow the instructions of\ngenerating a blog post with images closely related to the text and with dog images showing high\nlevels of consistency across all images.\n\n\n                                                                                                      79", "mimetype": "text/plain", "start_char_idx": 39, "end_char_idx": 1462, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "44222823-0282-4fbb-8581-2520e51c5335": {"__data__": {"id_": "44222823-0282-4fbb-8581-2520e51c5335", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "35eba794-6046-4733-bc62-80ee184ab6ad", "node_type": "4", "metadata": {}, "hash": "5c74a704b905da084f0257433873a491e35276499047ce6e6100121225663511", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n10.4.4. Image understanding and reasoning\n      Prompt\n\n\n      Look at this sequence of three shapes. What shape should come as the fourth shape? Explain\n      your reasoning with detailed descriptions of the first shapes.\n\n\n      Model Response\n\n\n     The fourth shape should be a hexagon. The first shape is a triangle, the second shape is a\n      square, and the third shape is a pentagon. The number of sides in each shape is increasing\n      by one\n\n\nFigure 13 | Solving a multimodal reasoning problem.The model is able to recognize shapes in the\nimage, understand their properties and reason about the relationship between them to predict the\nnext object.\nSource: photo taken by an author from the Gemini team.\n\n\n                                                                                                  80", "mimetype": "text/plain", "start_char_idx": 38, "end_char_idx": 912, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d65f520a-0ffc-4d4d-947b-1a2e75776ca3": {"__data__": {"id_": "d65f520a-0ffc-4d4d-947b-1a2e75776ca3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e85ccd83-517a-49ea-825c-40e0e03a4467", "node_type": "4", "metadata": {}, "hash": "22f03324bd957c2a4f8be5b3885bc2483df7b62369b4091362e82e088bce80ca", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n10.4.5. Geometrical reasoning\n      Prompt\n                                                                      x+ 15\n      Find the height of the parallelogram given its area with 100 square units.\n\n\n      Model Response\n\n\n     The area of the parallelogram is equal to the product of the base and the height. Hence\n     100 = (\ud835\udc65 + 15)\ud835\udc65. We get \ud835\udc652 + 15\ud835\udc65 \u2212 100 = 0. The roots of this equation are \ud835\udc651 = 5 and\n      \ud835\udc652 = \u221220. The second root is negative and is not acceptable. Final Answer: The final answer\n      is 5. I hope it is correct.\n\n\nFigure 14 | Solving a geometrical reasoning task. The model shows good understanding of the task\nand is able to provide meaningful reasoning steps despite slightly unclear instructions.\nSource: Lu et al. (2021).\n\n\n10.4.6. Information seeking about objects\n      Prompt\n\n\n      Find a connection between these. Hint: think about historical events.\n\n\n      Model Response\n\n\n     The Moon is the only celestial body in which humans have played golf. In 1971, the Apollo\n     14 crew hit two golf balls on the lunar surface.\n\n\nFigure 15 | Solving a puzzle using multimodal inputs. The model recognizes the objects in the images\nand identifies a commonality that connects the two objects.\nSource: photo taken by an author from the Gemini team.\n\n\n                                                                                                      81", "mimetype": "text/plain", "start_char_idx": 38, "end_char_idx": 1481, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3bd27334-b347-4d3d-b02f-022af2fb316c": {"__data__": {"id_": "3bd27334-b347-4d3d-b02f-022af2fb316c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3c006079-78be-4b33-82b2-54d8840a5b0d", "node_type": "4", "metadata": {}, "hash": "86cca4ff9d01bc0eba9ffadec1bf5d0a3618a1f890c763f89770e386c735373f", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n10.4.7. Multimodal reasoning based on visual cues\n      Prompt\n                                     j;*--*-*,,                              -x\n                                       'Sqd\n      Do you know the precise location where this image was taken?\n\n\n      Model Response\n\n\n     This image was taken in New York City. The Empire State Building is visible in the background.\n     The street in the foreground is 8th Avenue. The cross street is West 34th Street.\n\n\nFigure 16 | Identifying the objects in the image (the Empire State Building) and recognizing what\nthose are even with small levels of visual distortion in the image. Based on the image, the model is\nalso able to correctly identify the precise location of the person taking the photo.\nSource: photo taken by an author from the Gemini team.\n\n\n                                                                                                      82", "mimetype": "text/plain", "start_char_idx": 37, "end_char_idx": 1005, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "650e515e-ff00-4151-a792-44b392b7e3e6": {"__data__": {"id_": "650e515e-ff00-4151-a792-44b392b7e3e6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9f02c365-50fd-4fd2-9802-a240d1dac0e3", "node_type": "4", "metadata": {}, "hash": "397f201d0d46228370d14b70626e9798ae5c8ad1640e351bde41706f84b2f5ec", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n 10.4.8. Multimodal humor understanding\n      Prompt\n                                                                      Game at 300\n                                                                               FPS\n                                                                                 Game a\n                                75 hz                                                    FPS\n                             Monitor\n      Can you explain this meme?\n\n\n      Model Response\n\n\n      The meme is about a person who is playing a game at 300 FPS, but their monitor is only\n      75Hz. The person is trying to catch the game, but it is too fast for their monitor to display.\n      The meme is funny because it is relatable to anyone who has ever played a game on a monitor\n      that is not fast enough.\n\n\n Figure 17 | Explanation of humor in a meme. The model is showing the ability to not only describe\nwhat is happening in the image but also what it means even though the cultural context is not\n mentioned explicitly in the image or the prompt.\n Source: Hwang and Shwartz (2023).\n\n\n                                                                                                      83", "mimetype": "text/plain", "start_char_idx": 37, "end_char_idx": 1295, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a94ae2f3-4864-45d0-a773-f8329efca4da": {"__data__": {"id_": "a94ae2f3-4864-45d0-a773-f8329efca4da", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "03a55c9d-430e-4abd-9a35-9f83424d75a6", "node_type": "4", "metadata": {}, "hash": "42ca1ddad9c96adae9a82e11e2bcd54d09ba27b9de7f1ae39dc3827395b2a3c9", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n10.5. Commonsense reasoning in a multilingual setting\n     Prompt\n                                        The Basic Chinese Family Tree\n                                     Yeve        rairai                    waipo      waigong\n                                     00\n                                                                                          9909\n                                   baba    00                                            mama\n                                    45           95                        48           #HAH\n                                     didi       mcimei        wc           gege         jiejie\n     Explain what i should call my grandparents on each side of my family based on this chart\n\n\n     Model Response\n\n\n     On your father\u2019s side, you would call your grandparents Y\u00e9ye and N \u02c7ainai. On your mother\u2019s\n     side, you would call your grandparents W\u00e0ig\u00afong and W\u00e0ip\u00f3.\n\n\nFigure 18 | Common-sense reasoning in images. The model is able to understand the relationships\nrepresented in the graphs and reason about them in a multilingual setting.\nSource: image created by an author from the Gemini team.\n\n\n                                                                                                  84", "mimetype": "text/plain", "start_char_idx": 37, "end_char_idx": 1350, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "082c1e75-6bf8-45b1-8438-44fbc27e7ce1": {"__data__": {"id_": "082c1e75-6bf8-45b1-8438-44fbc27e7ce1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "38c287fa-aea2-4fbd-80de-817f1780f432", "node_type": "4", "metadata": {}, "hash": "53f0534e40954c6e1a3353dd60b135273e07f553aebbd44bf3aa464e825a8215", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n10.5.1. Reasoning and code generation\n     Prompt\n\n\n     Create a web app called \"Opossum Search\":\n\n\n     1. Every time you make a search query, it should redirect you to a google search with the same\n     query, but the word opossum before it.\n     2. It should be visually similar to Google search,\n     3. Instead of the google logo, it should have a picture of an opossum from the internet.\n     4. It should be a single html file, no separate js or css files.\n     5. It should say \"Powered by google search\" in the footer.\n\n\n     Model Response (rendered code)\n\n\n     Here is the code for Opossum Search:\n\n\n       1 <!DOCTYPE html>\n       2 <html>\n       3 <head>\n       4     <title>Opossum Search</title>\n       5     <style>\n       6       body {\n       7         font\u2212family: Arial, sans\u2212serif;\n       8         margin: 0;\n       9         padding: 0;\n      10       }\n      11\n      12       header {\n      13         background\u2212color: #333;\n      14         color: #fff;\n      15         text\u2212align: center;\n      16         padding: 1rem 0;\n      17       }\n      18\n      19       .search\u2212bar {\n      20         display: flex;\n      21         justify\u2212content: center;\n      22         align\u2212items: center;\n      23         height: 50px;\n      24         margin\u2212top: 1rem;\n      25       }\n      26\n      27       .search\u2212input {\n      28         width: 50%;\n      29         padding: 0.5rem;\n      30         border: 1px solid #ccc;\n      31         border\u2212radius: 5px;\n      32       }\n      33\n      34       .search\u2212button {\n      35         margin\u2212left: 1rem;\n      36         padding: 0.5rem 1rem;\n      37         border: 1px solid #ccc;\n      38         border\u2212radius: 5px;\n      39         cursor: pointer;\n      40       }\n      41\n      42       footer {\n      43         background\u2212color: #333;\n      44         color: #fff;\n      45         text\u2212align: center;\n      46         padding: 1rem 0;\n      47         margin\u2212top: 1rem;\n      48       }\n      49     </style>\n      50 </head>\n      51 <body>\n      52     <header>\n      53       <h1>Opossum Search</h1>\n      54       <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a4/Didelphis_virginiana.jpg/\n      55       1200px\u2212Didelphis_virginiana.jpg\" alt=\"Opossum\" width=\"100\">\n      56     </header>\n      57\n      58     <div class=\"search\u2212bar\">\n      59       <input type=\"text\" class=\"search\u2212input\" placeholder=\"Search the web\">\n      60       <button class=\"search\u2212button\">Search</button>\n      61     </div>\n\n\n                                                                                                              85", "mimetype": "text/plain", "start_char_idx": 51, "end_char_idx": 2730, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c31f4179-130c-4102-b5d8-a57cadd92bf2": {"__data__": {"id_": "c31f4179-130c-4102-b5d8-a57cadd92bf2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "de801551-848a-415a-9a25-0167cfae22ba", "node_type": "4", "metadata": {}, "hash": "9cc9ff137d827e52f316acdcceb44efe2bf45d943548849329a5aa41822ebfb5", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n      62    <footer>\n      63      Powered by Google Search\n      64    </footer>\n      65\n      66    <script>\n      67      const searchInput = document.querySelector('.search\u2212input');\n      68      const searchButton = document.querySelector('.search\u2212button');\n      69\n      70      searchButton.addEventListener('click', () => {\n      71        const query = searchInput.value;\n      72        if (query) {\n      73         window.location.href = `https://www.google.com/search?q=opossum+${query}`;\n      74        }\n      75      });\n      76    </script>\n      77 </body>\n     This code creates a simple web app that looks similar to Google Search, but with an opossum\n     logo. When you enter a search query and click the \"Search\" button, it will redirect you to a\n     Google search with the word \"opossum\" added to the beginning of your query. The app is\n     powered by Google Search, as indicated in the footer.\n\n\n     Rendered Website\n                                                  Opossum Search\n                           Search Ihe web                                                         Seinch\n                                                      Powered by Google Search\nFigure 19 | Writing code for a website based on user request. The model follows the instructions and\nrequirements defined by the user and converts them to functioning code.\n\n\n                                                                                                          86", "mimetype": "text/plain", "start_char_idx": 46, "end_char_idx": 1582, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6037b531-077e-4b19-9ce4-361e51481ab6": {"__data__": {"id_": "6037b531-077e-4b19-9ce4-361e51481ab6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "caee1745-65b3-4cd4-b147-2b5ac82055f3", "node_type": "4", "metadata": {}, "hash": "f0cd8f4377a95438f27398a3e90147b2af81de622f86a7d5d99777ac5694b0ec", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n10.5.2. Mathematics: Calculus\n       Prompt\n\n\n       Find the derivative of sinh \ud835\udc65 + cosh \ud835\udc66 = \ud835\udc65 + \ud835\udc66.\n\n\n       Model Response (rendered latex)\n       We have that \ud835\udc51\ud835\udc65 sinh \ud835\udc65 = cosh \ud835\udc65 and \ud835\udc51\ud835\udc65 cosh \ud835\udc66 = sinh \ud835\udc66 \ud835\udc51\ud835\udc65 \ud835\udc51            \ud835\udc51                                    \ud835\udc51 \ud835\udc66. Therefore,\n\n\n                                                                                 cosh \ud835\udc65 + sinh \ud835\udc66 \ud835\udc51\ud835\udc65 = 1 + \ud835\udc51\ud835\udc65 \ud835\udc51 \ud835\udc66              \ud835\udc51 \ud835\udc66.\n       Solving for \ud835\udc51\ud835\udc65 \ud835\udc51 \ud835\udc66, we find that\n\n\n                                                                                              \ud835\udc51 \ud835\udc66  = sinh \ud835\udc66 \u2212 1 1 \u2212 cosh \ud835\udc65.\n                                                                                              \ud835\udc51\ud835\udc65\n       Final Answer: The final answer is sinh \ud835\udc66\u22121 1\u2212cosh \ud835\udc65.\n\n\nFigure 20 | Solving a calculus problem. The model is able to get a solution to a calculus problem with\nstep-by-step explanation and correctly defined LaTeX equations.\nSource: question is provided by Macmillan Learning.\n\n\n                                                                                                                                                  87", "mimetype": "text/plain", "start_char_idx": 66, "end_char_idx": 1221, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "981bcfd4-49ab-41d4-a6be-1faa18c30cb5": {"__data__": {"id_": "981bcfd4-49ab-41d4-a6be-1faa18c30cb5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "de10392b-704d-450a-b743-be592a37eba1", "node_type": "4", "metadata": {}, "hash": "fb6986e62bec8d96a324bf8a4526eed3f7bc5a908d765e0d32b640a2bee1f029", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n 10.6. Multi-step reasoning and mathematics\n     Prompt\n\n\n     In the year 2018 factory A produced goods worth 500 million forints, while factory B produced\n     goods worth 400 million forints. According to their long-term plans, factory A will increase\n     production by 5% each year, while factory B will increase production by 6% per year.\n\n\n     The above plans were published on an economy website. A debate started in the comment\n     section of the article: one user claimed that the difference between the volumes of production\n     between the two factories would get smaller and smaller in the next couple of years.\n\n\n     Calculate the missing values of the table below and show that the above claim is not true.\n\n\n     Header: | 2018 | 2019 | 2020 | 2021 |\n     Production volume of factory A (mln) | 500 | | | |\n     Production volume of factory B (mln) | 400 | | | |\n\n\n     Model Response (rendered Markdown)\n\n\n       Year           Production volume of              Production volume of               Difference\n                      factory A (mln)                   factory B (mln)                   (mln)\n       2018           500                               400                                100\n       2019           525                               424                                101\n       2020           551.25                            449.44                             101.81\n       2021           578.81                            476.41                             102.4\n     The production volume of factory A in 2019 is 500 * 1.05 = 525 million forints.\n     The production volume of factory B in 2019 is 400 * 1.06 = 424 million forints.\n     The difference in production volume in 2019 is 525 - 424 = 101 million forints.\n\n\n     The production volume of factory A in 2020 is 525 * 1.05 = 551.25 million forints.\n     The production volume of factory B in 2020 is 424 * 1.06 = 449.44 million forints.\n     The difference in production volume in 2020 is 551.25 - 449.44 = 101.81 million forints.\n\n\n     The production volume of factory A in 2021 is 551.25 * 1.05 = 578.81 million forints.\n     The production volume of factory B in 2021 is 449.44 * 1.06 = 476.41 million forints.\n     The difference in production volume in 2021 is 578.81 - 476.41 = 102.4 million forints.\n\n\n     As you can see, the difference in production volume between the two factories is actually\n     increasing, not decreasing. Therefore, the claim that the difference between the volumes of\n     production between the two factories would get smaller and smaller in the next couple of years\n     is not true.\n\n\n Figure 21 | Solving a multi-step math problem. The model is able to understand the task and generate\n a markdown table with correctly calculated values. It also explicitly follows the instructions to show\nwhere the numbers come from and answer the question given in the task.\n Source: Oktat\u00e1si Hivatal (2023, p. 20)\n\n\n                                                                                                         88", "mimetype": "text/plain", "start_char_idx": 38, "end_char_idx": 3147, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f0eee39d-7b3a-4b3e-8534-0ee899ac7a9f": {"__data__": {"id_": "f0eee39d-7b3a-4b3e-8534-0ee899ac7a9f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "00bf5ee0-e44c-41bd-8caa-b3c931a6110f", "node_type": "4", "metadata": {}, "hash": "0d1fdf6bde3c26a35e9f6c628f5510afc8943725e168dbc7a51efeace4f9555f", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n10.6.1. Complex image understanding, code generation, and instruction following\n     Prompt\n\n\n     I want you to take the function depicted in the top left subplot, multiply it by 1000, and then\n     add it to the function depicted in the bottom left subplot. Generate matplotlib code for the\n     single resulting plot.\n\n\n     Model Response (rendered code)\n\n\n      1 import numpy as np\n      2 import matplotlib.pyplot as plt\n      3\n      4 x = np.linspace(0, 10, 1000)\n      5 y1 = np.sin(x)\n      6 y2 = np.exp(x)\n      7\n      8 plt.plot(x, 1000\u2217y1 + y2)\n      9 plt.show()\n\n\n     Rendered Graph\n                                             zuduu\n                                             15000\nFigure 22 | Multimodal reasoning capabilities applied to code generation. Gemini Ultra needs to\nperform inverse graphics task to infer the code that would have generated the plots, perform additional\nmathematical transformations, and generate relevant code.\nSource: figure generated by an author from the Gemini team.\n\n\n                                                                                                         89", "mimetype": "text/plain", "start_char_idx": 42, "end_char_idx": 1228, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e1c9298f-fa30-4886-aa1b-3f28380a3b40": {"__data__": {"id_": "e1c9298f-fa30-4886-aa1b-3f28380a3b40", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "70d37a69-9e43-44d0-bc0c-b65aa55d1597", "node_type": "4", "metadata": {}, "hash": "3a9e4f170d3c2c8b4d4f7240f773b55f04db63c972f60acf54717f82e8a6f53c", "class_name": "RelatedNodeInfo"}}, "text": "Gemini: A Family of Highly Capable Multimodal Models\n\n\n10.6.2. Video understanding and reasoning\n     Prompt (video)\n\n\n     How could this person improve their technique?\n\n\n     Model Response\n\n\n     This soccer player needs to work on their ball-striking mechanics. Their foot is not making\n     good contact with the ball, which is causing the ball to fly up and away from the goal. They\n     also need to work on their body positioning. They should be leaning into the shot with their\n     non-kicking leg in front of their kicking leg, and they should be following through with their\n     kicking leg.\n\n\nFigure 23 | Video understanding and reasoning over the situation presented in the video. Here, we\nprovide a video as input to the model together with a text prompt (images are provided here only\nfor visualization purposes). The model is able to analyze what happened in the video and provide\nrecommendations on how the actions in the video could have been better.\nVideo source: \"Football/Soccer Penalty Miss\"\nhttps://www.youtube.com/watch?v=VmWxjmJ3mvs\n\n\n                                                                                                     90", "mimetype": "text/plain", "start_char_idx": 38, "end_char_idx": 1204, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"84aea1d7-6bd5-4064-b0cc-942124651f8b": {"node_ids": ["570acf52-f613-43ff-9c87-4995e44f1080"], "metadata": {}}, "df73c18a-5e55-4389-8ee2-092b394ce21c": {"node_ids": ["309dbfa8-2e7e-4def-ad8d-068a9addfe8e"], "metadata": {}}, "b03751a5-6dfb-4237-abd6-4c667ff2e263": {"node_ids": ["4fe68737-08af-473c-a056-570e8f5ab130"], "metadata": {}}, "9b1889e5-5909-4777-9934-4aa5ec509f20": {"node_ids": ["e730ac37-60ab-4fcb-969a-88fdea1ecc26"], "metadata": {}}, "fdcb044a-a0d6-44f9-ba93-3977f323d9ab": {"node_ids": ["6db95581-861d-4056-b4f7-c8f49b69d52d"], "metadata": {}}, "9bee60bc-35e5-4c90-b5a8-57d9d4aca5f3": {"node_ids": ["2a803ccc-4401-4854-9e90-e1a119a6eb12"], "metadata": {}}, "4a05cb45-6040-4edb-9318-4603662c1c60": {"node_ids": ["bac5362d-6307-41c5-a3b9-decd1c5ee303"], "metadata": {}}, "d833b0a9-8ae5-4878-b40b-a4e8b0c0b453": {"node_ids": ["63aee76d-6e26-4d2a-8da2-9760ea1dcdd2", "d1db085d-77d7-4f83-b8ee-49e222bce7ab"], "metadata": {}}, "e90003a3-12ee-4960-a6ba-7ac28d985723": {"node_ids": ["2c84f064-6027-4e6e-9881-0785be5a9a46"], "metadata": {}}, "d4c1b94e-ab41-49f3-98b3-e897a2fe5e7e": {"node_ids": ["a1f940f1-eb60-4f06-843b-54c7d83e74e0"], "metadata": {}}, "39cf8f8f-9a62-464d-ab5a-30cb5e2b1278": {"node_ids": ["03aaedea-0a1f-4f4e-8172-1d8dc5a02ccd"], "metadata": {}}, "62c30508-e83c-4515-8b72-318a4dd46462": {"node_ids": ["f873c89c-4916-4dd1-8ba9-3654590879f1"], "metadata": {}}, "c7afd9e9-bac8-4364-a1ac-d41ea937d1ef": {"node_ids": ["5cf9fa4c-7138-45a6-b7e7-84e1ffb51b81", "51f23561-6cbf-4136-a5ab-ebb6def37f12"], "metadata": {}}, "8f733488-a603-4b16-b723-79ad0fc46e31": {"node_ids": ["4d3aafd5-7da3-4f8e-8cf7-3f2e285df1c0"], "metadata": {}}, "45bcd4fe-053a-47c1-9f0d-2941cc1ed1ee": {"node_ids": ["411a7d35-f8b9-4583-bda4-44b8ec65d919"], "metadata": {}}, "352cad43-0b83-4ed8-9282-ebbc11bbc4c7": {"node_ids": ["1ad9d390-c05d-4e89-a0ac-bb23edf436e8"], "metadata": {}}, "4823fafb-2ba4-46fe-b9e7-2a15e2c105b0": {"node_ids": ["de2b091a-dfbf-40e1-98e6-6ef9d19c0347"], "metadata": {}}, "e9a9fce0-6155-400f-bd73-56e598877979": {"node_ids": ["7f796ab5-2ae2-403c-bfdc-902eea3d8a2e", "921e8c01-57ef-4177-b5c1-2bd74cfec5f7"], "metadata": {}}, "3f0f4da4-2713-4e37-87ed-e33e0e4833e1": {"node_ids": ["098960e3-7be1-49b3-bbdc-d35fc235dab6"], "metadata": {}}, "106c69f6-ae52-4334-8ab9-25ad00a16dc2": {"node_ids": ["a138febe-7833-4497-ac14-8f4c57ddcb39"], "metadata": {}}, "9f2545d5-3381-4c78-a106-98dd040c1486": {"node_ids": ["b1aae730-5f59-44fe-a6c9-f82352815b86"], "metadata": {}}, "b327cfdb-6b4a-49e0-a2ae-4ffd81223440": {"node_ids": ["25ccd597-5eda-4b02-924e-64849c904608"], "metadata": {}}, "3dd12f4e-ece9-4854-b69b-466626102bc8": {"node_ids": ["c68c958f-98ad-4966-ab8c-93254ade2348"], "metadata": {}}, "62099bd2-c144-412d-8323-bb904f56599a": {"node_ids": ["29184fc1-6eaf-4c2f-90ba-9e367d9ad59c"], "metadata": {}}, "a4081485-ced5-4b70-b7e7-0675100ed292": {"node_ids": ["03a2f46a-2e46-457b-9457-a3b295989524"], "metadata": {}}, "6125c96c-8808-408a-a67f-9a7752c3e1ef": {"node_ids": ["04c88911-8078-4b90-94bd-bf35ae642d1a"], "metadata": {}}, "d9f478bb-b5b1-4e8b-96f4-d20d35c0b236": {"node_ids": ["54432497-33dd-4873-a315-75aed0c97d13"], "metadata": {}}, "dba508a2-037d-4361-928c-bfdc52236b9a": {"node_ids": ["f7b54486-7719-430d-b4c9-e24e77962385"], "metadata": {}}, "09eff6ca-526b-42bc-b459-45c4319a25b2": {"node_ids": ["84981b46-73d9-4513-8fa2-64bce783822b"], "metadata": {}}, "7fb10a90-3e1b-4fea-89d0-aabccd5cf061": {"node_ids": ["366c86ed-ef48-485a-b948-b4b206012c57"], "metadata": {}}, "e4d7113b-8774-4cc5-990c-5116c969a337": {"node_ids": ["550c1249-4c70-4830-ba9a-2f1da3737e84"], "metadata": {}}, "f4de7cc9-8e1a-4220-8e5b-73cdcb7907d0": {"node_ids": ["d0ada9ef-b6d2-45f9-90ae-76e948c3fef7"], "metadata": {}}, "c9c6e28a-b9b7-48fd-aac9-65d762f9582b": {"node_ids": ["51ddd985-26a3-4ff8-b312-853de7442a3f"], "metadata": {}}, "be00defc-486e-4a7e-b939-ba0a7260ae40": {"node_ids": ["93d1d435-3423-400f-b6ca-fb7324e86865"], "metadata": {}}, "e97a1479-4a28-4387-82dd-53cabf54a572": {"node_ids": ["0d13ecd4-ca73-4696-8b68-cd3e6d3fb628"], "metadata": {}}, "76f88cd0-81cf-4733-83ce-06950a39afa6": {"node_ids": ["5a65f5e3-f861-4a7a-a545-9d739b702854"], "metadata": {}}, "0a122a7a-8a06-43b7-b2e3-4144352a46cc": {"node_ids": ["087d44da-2279-4973-912a-2b08072390b5"], "metadata": {}}, "0645aecb-2e3b-4259-bdea-3e86ee8a0598": {"node_ids": ["b98c4297-9a99-454b-8d4c-641d269e74ad"], "metadata": {}}, "892148dc-bef2-46a1-8944-f6b9b2f15476": {"node_ids": ["89b26998-bc35-4b10-ae15-96bfac9cc127"], "metadata": {}}, "e999d1de-7633-4302-8d3f-fa024ff5303a": {"node_ids": ["a73e37a0-e3cf-44bc-afe8-ae9201e4b1a1"], "metadata": {}}, "8c50530d-a192-4ca8-bfd5-39f90ec8290e": {"node_ids": ["05524d37-4964-4874-963e-4088b7947cac", "fcbacea3-ea3d-43bd-a1c5-d1d4bc025833"], "metadata": {}}, "1584bd7d-6226-4860-b88c-fa92e7999503": {"node_ids": ["67cdf4d5-5515-4af0-8a77-589a7d96d136", "9790adf1-a6a9-43c6-b029-3db0345f9fd5"], "metadata": {}}, "17b8b7e8-b4db-47c8-b382-a97f2da48307": {"node_ids": ["359de1ba-4855-436f-878a-df62338cfef9", "36e3f164-2b89-49f7-9a43-389c22785cb9"], "metadata": {}}, "2cbb2433-e70e-4084-a6ce-e6fdb2c9ca58": {"node_ids": ["c9752251-f8ce-4cc7-84db-ae993320a81c", "4317f965-4425-4e5f-92b2-cc55efc937aa"], "metadata": {}}, "56bc2258-3ef6-4e4b-b4fe-0f0b37dcfdae": {"node_ids": ["7dd1f7f4-139d-421d-9edd-de3524bb9b9d", "6c1d30ac-30c0-4dea-977f-8d5f5051de8d"], "metadata": {}}, "82eb7b56-7cd0-44bf-a3a7-e1a38f467db7": {"node_ids": ["f506544e-70c1-46c3-9a82-acdcaa5da3ce", "ecf07567-1e4b-40f1-9b8e-66100226f134"], "metadata": {}}, "3544fa19-551a-41e6-9479-69e416d8ebc1": {"node_ids": ["aadfece9-7f39-4def-9f5e-51b6e7ad1be0"], "metadata": {}}, "05a59412-b1c2-4acf-9728-a356c2b971c5": {"node_ids": ["4a7b9174-14c6-490a-9d95-54e06cfba31b", "6ca7408f-c9e8-4919-b063-0e0b195af7bc"], "metadata": {}}, "06447da5-94c4-40c7-8584-d3e878cc616e": {"node_ids": ["6a4d6fe3-c297-4919-a594-dfd1f91b3ada"], "metadata": {}}, "e825ea7f-2b2b-48e2-8c69-1ff8ebd857fd": {"node_ids": ["c435317e-db5d-45ae-a2bd-f48754c18ab8"], "metadata": {}}, "8965efd6-0ce7-41d7-ab1f-bcf99044a19d": {"node_ids": ["f78c2f86-2359-493e-b79c-fda67608761a", "d7047934-d8ef-4098-9f77-2e7bee215506"], "metadata": {}}, "93fd352c-f131-4c0a-a03d-aa226cfb9be2": {"node_ids": ["736cf978-aed2-44d2-a152-de25128edb81", "9c14eb31-cbcb-48be-81ff-8d589439fcfc"], "metadata": {}}, "05546230-f685-4b61-8d05-2fe707fa11fc": {"node_ids": ["3c386aef-17f8-44db-a91a-6742888c15cc", "084fe3f6-e43a-4430-ab90-8857329fb5d3"], "metadata": {}}, "d2c867d0-670f-44b0-b575-3f5775922c5f": {"node_ids": ["ec440fe4-b9d1-4de4-a388-6e869a16c3ca"], "metadata": {}}, "4c3ee72e-52e4-4a8f-9824-ff1de684dabb": {"node_ids": ["cf122f49-1b3a-4676-a5d7-9fcd21249c3f", "0f43b777-9c6e-4418-b653-0d99f7370d00"], "metadata": {}}, "cc45db44-ddb5-426c-ae93-f57c0d26dc33": {"node_ids": ["668216f9-d2da-4085-bdfb-08e22144d2cb"], "metadata": {}}, "b76cdc16-3237-41ca-9c9f-5ab898e7a077": {"node_ids": ["aa160a77-35c4-4b13-a3fe-81f1ef180d31"], "metadata": {}}, "a4d13d7e-0fe6-4cc5-95ec-e8b2624c2795": {"node_ids": ["4ba1b6ed-197c-4db9-9a03-01a50752e5fd"], "metadata": {}}, "3e885305-29ee-4615-a697-a4ba2fcf901e": {"node_ids": ["48975aa1-0e2f-4caf-ab25-66db3af56e57"], "metadata": {}}, "ecc7b314-f186-44d3-bc98-04696c2287a8": {"node_ids": ["1ca4e726-6ff9-46af-90cf-67a02b950318"], "metadata": {}}, "4526244d-5186-4ed2-8a95-23cd2b9ba597": {"node_ids": ["8721c083-5851-4b4f-8d8b-a67babef97fe"], "metadata": {}}, "f3510e33-2cc2-4a97-844a-bc18429c00ed": {"node_ids": ["1f48f8cf-c1c9-4894-a4f2-9d08c2d7fb28"], "metadata": {}}, "328d4855-ff4b-412d-8c7c-19dface1a83c": {"node_ids": ["b3a75b14-d987-451a-a83c-94de4160180b"], "metadata": {}}, "4a7b1260-6fda-4491-9b8f-f80504c9c6a5": {"node_ids": ["63ade1e8-a44f-4a97-9261-020a5452163f"], "metadata": {}}, "7d09033f-a40d-48c9-acf0-d2f1b1a2f0e7": {"node_ids": ["0f76c549-8d9e-4bdd-8582-ebbcc6c5e081"], "metadata": {}}, "fe00072c-38e6-421b-b227-1e1e369df698": {"node_ids": ["7e2f039d-a900-4ff6-af89-118a66c56df8"], "metadata": {}}, "a94626ed-131f-4915-a415-69a37e82d0bd": {"node_ids": ["99186704-d59b-46e2-969a-04f73bd20ccf"], "metadata": {}}, "6246923a-2475-4964-aee1-f26ef4127963": {"node_ids": ["508d15f5-f4b6-4ab1-be16-fd2458bc0ad0"], "metadata": {}}, "904ce7da-b0b8-4868-94d1-fde93db79fa0": {"node_ids": ["b32fba8b-4628-455b-a018-5970a5de131e"], "metadata": {}}, "eb2353d6-6068-4020-a3b8-39bec5277a38": {"node_ids": ["3fbfed0c-2872-4652-8e15-8aa2022ebe8b", "b60a3a1a-444c-469f-8275-d9a83bc291ec", "450ee741-1551-4144-916a-6dc3336cd4f0"], "metadata": {}}, "91e47bab-e29c-442a-8ea2-6c8a04a5b3e0": {"node_ids": ["19eaad7e-4adb-4906-aa3d-ca9e06e0624a"], "metadata": {}}, "6e61ea38-6e41-404d-a29b-e29f32c1a28c": {"node_ids": ["1def6a21-fb81-4d1b-be77-52031860a154"], "metadata": {}}, "0445e872-d5a5-44f7-adb6-a9b485adb1c5": {"node_ids": ["5e6d7c60-4ae6-47f8-9d8e-6851ae109a65"], "metadata": {}}, "32990677-c8e5-47f3-8b69-e2c1e5c81d12": {"node_ids": ["1bea6dea-7672-4b13-9450-4f0307132515"], "metadata": {}}, "db062d8f-37ef-475c-81b7-c2b03c8bfbd9": {"node_ids": ["d9bb9521-66e5-4319-a7ed-c9662e4ec9ff", "147576a4-20a0-4732-98d0-a3911b13934e", "370dc4b6-b257-4c73-b0ea-31287d744a1a"], "metadata": {}}, "400d7fed-e6c7-483b-81fa-02aca53ef457": {"node_ids": ["3af8814e-fd29-4947-ab07-75fb2a1c4361"], "metadata": {}}, "c3840f64-daa1-4d91-a3d4-2ed3598cd8e1": {"node_ids": ["e6700d6e-eee8-45d0-befb-da13d3e1b45e"], "metadata": {}}, "b3921a53-3544-4f8a-8879-850762fee430": {"node_ids": ["ddd4d91a-8aac-4787-b444-299cb1ccd4c8"], "metadata": {}}, "225c8537-a293-405a-92d6-25cd615e1d8c": {"node_ids": ["ced501ab-d2ce-45ed-b5f0-509788456ed3"], "metadata": {}}, "35eba794-6046-4733-bc62-80ee184ab6ad": {"node_ids": ["44222823-0282-4fbb-8581-2520e51c5335"], "metadata": {}}, "e85ccd83-517a-49ea-825c-40e0e03a4467": {"node_ids": ["d65f520a-0ffc-4d4d-947b-1a2e75776ca3"], "metadata": {}}, "3c006079-78be-4b33-82b2-54d8840a5b0d": {"node_ids": ["3bd27334-b347-4d3d-b02f-022af2fb316c"], "metadata": {}}, "9f02c365-50fd-4fd2-9802-a240d1dac0e3": {"node_ids": ["650e515e-ff00-4151-a792-44b392b7e3e6"], "metadata": {}}, "03a55c9d-430e-4abd-9a35-9f83424d75a6": {"node_ids": ["a94ae2f3-4864-45d0-a773-f8329efca4da"], "metadata": {}}, "38c287fa-aea2-4fbd-80de-817f1780f432": {"node_ids": ["082c1e75-6bf8-45b1-8438-44fbc27e7ce1"], "metadata": {}}, "de801551-848a-415a-9a25-0167cfae22ba": {"node_ids": ["c31f4179-130c-4102-b5d8-a57cadd92bf2"], "metadata": {}}, "caee1745-65b3-4cd4-b147-2b5ac82055f3": {"node_ids": ["6037b531-077e-4b19-9ce4-361e51481ab6"], "metadata": {}}, "de10392b-704d-450a-b743-be592a37eba1": {"node_ids": ["981bcfd4-49ab-41d4-a6be-1faa18c30cb5"], "metadata": {}}, "00bf5ee0-e44c-41bd-8caa-b3c931a6110f": {"node_ids": ["f0eee39d-7b3a-4b3e-8534-0ee899ac7a9f"], "metadata": {}}, "70d37a69-9e43-44d0-bc0c-b65aa55d1597": {"node_ids": ["e1c9298f-fa30-4886-aa1b-3f28380a3b40"], "metadata": {}}}}